{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"The Internals of PySpark (Apache Spark 3.4.0)","text":"<p>Welcome to The Internals of PySpark online book! \ud83e\udd19</p> <p>I'm Jacek Laskowski, an IT freelancer specializing in Apache Spark (incl. Spark SQL and Spark Structured Streaming), Delta Lake, Databricks, and Apache Kafka (incl. Kafka Streams and ksqlDB) with brief forays into a wider data engineering space (e.g., Trino, Dask and dbt, mostly during Warsaw Data Engineering meetups).</p> <p>I'm very excited to have you here and hope you will enjoy exploring the internals of PySpark as much as I have.</p> <p>Flannery O'Connor</p> <p>I write to discover what I know.</p> <p>\"The Internals Of\" series</p> <p>I'm also writing other online books in the \"The Internals Of\" series. Please visit \"The Internals Of\" Online Books home page.</p> <p>Expect text and code snippets from a variety of public sources. Attribution follows.</p> <p>Now, let's take a deep dive into PySpark \ud83d\udd25</p> <p>Last update: 2023-04-22</p>"},{"location":"MonitorThread/","title":"MonitorThread","text":"<p><code>MonitorThread</code> is...FIXME</p>"},{"location":"Py4JServer/","title":"Py4JServer","text":"<p><code>Py4JServer</code> is a gateway server between Python and Java Virtual Machine (JVM) using Py4J.</p> <p><code>Py4JServer</code> is a wrapper for a py4j Server.</p>"},{"location":"Py4JServer/#creating-instance","title":"Creating Instance","text":"<p><code>Py4JServer</code> takes the following to be created:</p> <ul> <li> <code>SparkConf</code> (Spark Core) <p><code>Py4JServer</code> is created\u00a0when:</p> <ul> <li>PythonGatewayServer command-line application is started</li> <li>PythonRunner command-line application is started</li> </ul>"},{"location":"Py4JServer/#py4j-server","title":"py4j Server <p><code>Py4JServer</code> creates a <code>ClientServer</code> (py4j) or <code>GatewayServer</code> (py4j) based on PYSPARK_PIN_THREAD environment variable.</p>","text":""},{"location":"Py4JServer/#connection-secret","title":"Connection Secret <pre><code>secret: String\n</code></pre> <p><code>Py4JServer</code> creates a connection secret for a secure communication.</p>","text":""},{"location":"Py4JServer/#start","title":"start <pre><code>start(): Unit\n</code></pre> <p><code>start</code> requests the py4j Server to start.</p>","text":""},{"location":"Py4JServer/#getlisteningport","title":"getListeningPort <pre><code>getListeningPort: Int\n</code></pre> <p><code>getListeningPort</code> requests the py4j Server for the listening port.</p>","text":""},{"location":"PythonAccumulatorV2/","title":"PythonAccumulatorV2","text":"<p><code>PythonAccumulatorV2</code> is...FIXME</p>"},{"location":"PythonBroadcast/","title":"PythonBroadcast","text":"<p><code>PythonBroadcast</code> is...FIXME</p>"},{"location":"PythonForeachWriter/","title":"PythonForeachWriter","text":"<p><code>PythonForeachWriter</code> is...FIXME</p>"},{"location":"PythonFunction/","title":"PythonFunction","text":"<p><code>PythonFunction</code> is a metadata of a Python function to be executed in PythonRunner.</p>"},{"location":"PythonFunction/#creating-instance","title":"Creating Instance","text":"<p><code>PythonFunction</code> takes the following to be created:</p> <ul> <li> Command (<code>Array[Byte]</code>) <li> Environment Variables (<code>Map[String, String]</code>) <li> Python Includes (<code>List[String]</code>) <li> Python Executable <li> Python Version <li> Broadcast Variables with PythonBroadcast]s <li> PythonAccumulatorV2 <p><code>PythonFunction</code> is created when...FIXME</p>"},{"location":"PythonGatewayServer/","title":"PythonGatewayServer","text":"<p><code>PythonGatewayServer</code> is a command-line application (process) that starts a Py4JServer on an ephemeral port.</p> <p><code>PythonGatewayServer</code> is the Python runner for pyspark shell (Spark Core).</p>"},{"location":"PythonGatewayServer/#main","title":"main <p><code>main</code> creates a Py4JServer and requests it to start.</p> <p><code>main</code> requests the <code>Py4JServer</code> for the listening port (boundPort) and prints out the following DEBUG message to the logs:</p> <pre><code>Started PythonGatewayServer on port [boundPort]\n</code></pre> <p> <code>main</code> uses _PYSPARK_DRIVER_CONN_INFO_PATH environment variable for the path of a connection info file (for the associated python process) with the listening port and the secret. <p><code>main</code> pauses (blocks) until the Python driver finishes (by reading from the system input that blocks until input data is available, the end of the stream is detected, or an exception is thrown).</p> <p>In the end, once the Python driver finishes, <code>main</code> prints out the following DEBUG message to the logs:</p> <pre><code>Exiting due to broken pipe from Python driver\n</code></pre> <p><code>main</code> prints out the following ERROR message to the logs and exists when the listening port is <code>-1</code>:</p> <pre><code>[server] failed to bind; exiting\n</code></pre>","text":""},{"location":"PythonGatewayServer/#_pyspark_driver_conn_info_path","title":"_PYSPARK_DRIVER_CONN_INFO_PATH <p><code>PythonGatewayServer</code> uses <code>_PYSPARK_DRIVER_CONN_INFO_PATH</code> environment variable for the path of a connection info file for communication between this and the Python processes.</p> <p><code>_PYSPARK_DRIVER_CONN_INFO_PATH</code> is configured when java_gateway.py module is requested to launch_gateway.</p>","text":""},{"location":"PythonGatewayServer/#logging","title":"Logging <p>Enable <code>ALL</code> logging level for <code>org.apache.spark.api.python.PythonGatewayServer</code> logger to see what happens inside.</p> <p>Add the following line to <code>conf/log4j2.properties</code>:</p> <pre><code>logger.PythonGatewayServer.name = org.apache.spark.api.python.PythonGatewayServer\nlogger.PythonGatewayServer.level = all\n</code></pre> <p>Refer to Logging.</p>","text":""},{"location":"PythonRDD/","title":"PythonRDD","text":"<p><code>PythonRDD</code> is an <code>RDD</code> (<code>RDD[Array[Byte]]</code>) that uses PythonRunner (to compute a partition).</p>"},{"location":"PythonRDD/#creating-instance","title":"Creating Instance","text":"<p><code>PythonRDD</code> takes the following to be created:</p> <ul> <li> Parent <code>RDD</code> <li> PythonFunction <li> <code>preservePartitoning</code> flag <li> <code>isFromBarrier</code> flag (default: <code>false</code>) <p><code>PythonRDD</code> is created when...FIXME</p>"},{"location":"PythonRDD/#runjob","title":"runJob <pre><code>runJob(\n  sc: SparkContext,\n  rdd: JavaRDD[Array[Byte]],\n  partitions: JArrayList[Int]): Array[Any]\n</code></pre> <p><code>runJob</code>...FIXME</p>","text":""},{"location":"PythonRDD/#collectandserve","title":"collectAndServe <pre><code>collectAndServe[T](\n  rdd: RDD[T]): Array[Any]\n</code></pre> <p><code>collectAndServe</code>...FIXME</p>","text":""},{"location":"PythonRDD/#collectandservewithjobgroup","title":"collectAndServeWithJobGroup <pre><code>collectAndServeWithJobGroup[T](\n  rdd: RDD[T],\n  groupId: String,\n  description: String,\n  interruptOnCancel: Boolean): Array[Any]\n</code></pre> <p><code>collectAndServeWithJobGroup</code>...FIXME</p>","text":""},{"location":"PythonRDD/#serveiterator-utility","title":"serveIterator Utility <pre><code>serveIterator(\n  items: Iterator[_],\n  threadName: String): Array[Any]\n</code></pre> <p><code>serveIterator</code> serveToStream with a writer function that...FIXME</p> <p><code>serveIterator</code>\u00a0is used when:</p> <ul> <li><code>PythonRDD</code> utility is used to runJob, collectAndServe and collectAndServeWithJobGroup</li> <li><code>Dataset</code> is requested to <code>collectToPython</code>, <code>tailToPython</code>, <code>getRowsToPython</code></li> </ul>","text":""},{"location":"PythonRDD/#servetostream-utility","title":"serveToStream Utility <pre><code>serveToStream(\n  threadName: String)(\n  writeFunc: OutputStream =&gt; Unit): Array[Any]\n</code></pre> <p><code>serveToStream</code> serveToStream with the authHelper and the input arguments.</p> <p><code>serveToStream</code>\u00a0is used when:</p> <ul> <li><code>PythonRDD</code> utility is used to serveIterator</li> <li><code>Dataset</code> is requested to <code>collectAsArrowToPython</code></li> </ul>","text":""},{"location":"PythonRDD/#socketauthhelper","title":"SocketAuthHelper <p><code>PythonRDD</code> uses a SocketAuthHelper.</p>","text":""},{"location":"PythonRunner/","title":"PythonRunner","text":"<p><code>PythonRunner</code> is a command-line application to launch Python applications.</p> <p><code>PythonRunner</code> is used by spark-submit.</p> <p><code>PythonRunner</code> executes a configured python executable as a subprocess and then has it connect back to the JVM to access system properties, etc.</p>"},{"location":"PythonRunner/#arguments","title":"Arguments","text":"<p><code>PythonRunner</code> requires the following command-line arguments:</p> <ol> <li>Main python file (<code>pythonFile</code>)</li> <li>Extra python files (<code>pyFiles</code>)</li> <li>Application arguments</li> </ol>"},{"location":"PythonRunner/#main","title":"main <p><code>main</code> takes the arguments from command line.</p> <p> <code>main</code> determines what python executable to use based on (in that order): <ol> <li>spark.pyspark.driver.python configuration property</li> <li>spark.pyspark.python configuration property</li> <li><code>PYSPARK_DRIVER_PYTHON</code> environment variable</li> <li><code>PYSPARK_PYTHON</code> environment variable</li> <li><code>python3</code></li> </ol> <p><code>main</code> creates a Py4JServer that is started on a daemon py4j-gateway-init thread.</p> <p><code>main</code> waits until the gateway server has started.</p> <p><code>main</code> launches a Python process using the python executable and the following environment variables.</p>    Environment Variable Value     <code>PYTHONPATH</code>    <code>PYTHONUNBUFFERED</code> YES   PYSPARK_GATEWAY_PORT getListeningPort   PYSPARK_GATEWAY_SECRET secret   <code>PYSPARK_PYTHON</code> spark.pyspark.python if defined   <code>PYTHONHASHSEED</code> <code>PYTHONHASHSEED</code> env var if defined   <code>OMP_NUM_THREADS</code> <code>spark.driver.cores</code> unless defined    <p><code>main</code> waits for the Python process to finish and requests the <code>Py4JServer</code> to shutdown.</p>","text":""},{"location":"PythonRunner/#demo","title":"Demo <pre><code>./bin/spark-class org.apache.spark.deploy.PythonRunner\n</code></pre>","text":""},{"location":"PythonWorkerFactory/","title":"PythonWorkerFactory","text":""},{"location":"PythonWorkerFactory/#creating-instance","title":"Creating Instance","text":"<p><code>PythonWorkerFactory</code> takes the following to be created:</p> <ul> <li> Python Executable <li> Environment Variables (<code>Map[String, String]</code>) <p><code>PythonWorkerFactory</code> is created when <code>SparkEnv</code> is requested to <code>createPythonWorker</code> (when <code>BasePythonRunner</code> is requested to compute a partition).</p>"},{"location":"PythonWorkerFactory/#usedaemon-flag","title":"useDaemon Flag <p><code>PythonWorkerFactory</code> uses <code>useDaemon</code> internal flag that is the value of spark.python.use.daemon configuration property to decide whether to use lighter daemon or non-daemon workers.</p> <p><code>useDaemon</code> flag is used when <code>PythonWorkerFactory</code> requested to create, stop or release a worker and stop a daemon module.</p>","text":""},{"location":"PythonWorkerFactory/#python-daemon-module","title":"Python Daemon Module <p><code>PythonWorkerFactory</code> uses spark.python.daemon.module configuration property to define the Python Daemon Module.</p> <p>The Python Daemon Module is used when <code>PythonWorkerFactory</code> is requested to create and start a daemon module.</p>","text":""},{"location":"PythonWorkerFactory/#python-worker-module","title":"Python Worker Module <p><code>PythonWorkerFactory</code> uses spark.python.worker.module configuration property to specify the Python Worker Module.</p> <p>The Python Worker Module is used when <code>PythonWorkerFactory</code> is requested to create and start a worker.</p>","text":""},{"location":"PythonWorkerFactory/#creating-python-worker","title":"Creating Python Worker <pre><code>create(): Socket\n</code></pre> <p><code>create</code>...FIXME</p> <p><code>create</code> is used when <code>SparkEnv</code> is requested to <code>createPythonWorker</code>.</p>","text":""},{"location":"PythonWorkerFactory/#creating-daemon-worker","title":"Creating Daemon Worker <pre><code>createThroughDaemon(): Socket\n</code></pre> <p><code>createThroughDaemon</code>...FIXME</p> <p><code>createThroughDaemon</code> is used when <code>PythonWorkerFactory</code> is requested to create a Python worker (with useDaemon flag enabled).</p>","text":""},{"location":"PythonWorkerFactory/#starting-python-daemon-process","title":"Starting Python Daemon Process <pre><code>startDaemon(): Unit\n</code></pre> <p><code>startDaemon</code>...FIXME</p>","text":""},{"location":"PythonWorkerFactory/#creating-simple-non-daemon-worker","title":"Creating Simple Non-Daemon Worker <pre><code>createSimpleWorker(): Socket\n</code></pre> <p><code>createSimpleWorker</code>...FIXME</p> <p><code>createSimpleWorker</code> is used when <code>PythonWorkerFactory</code> is requested to create a Python worker (with useDaemon flag disabled).</p>","text":""},{"location":"PythonWorkerFactory/#logging","title":"Logging <p>Enable <code>ALL</code> logging level for <code>org.apache.spark.api.python.PythonWorkerFactory</code> logger to see what happens inside.</p> <p>Add the following line to <code>conf/log4j2.properties</code>:</p> <pre><code>logger.PythonWorkerFactory.name = org.apache.spark.api.python.PythonWorkerFactory\nlogger.PythonWorkerFactory.level = all\n</code></pre> <p>Refer to Logging.</p>","text":""},{"location":"Setup/","title":"PySpark Setup","text":""},{"location":"Setup/#install-ipython","title":"Install IPython","text":"<p>Follow the steps as described in the official documentation of IPython.</p> <pre><code>pip install ipython\n</code></pre>"},{"location":"Setup/#start-pyspark","title":"Start PySpark","text":"<pre><code>export PYSPARK_DRIVER_PYTHON=ipython\n</code></pre> <p>For Java 11, use <code>-Dio.netty.tryReflectionSetAccessible=true</code> (see Downloading in the official documentation of Apache Spark).</p> <pre><code>./bin/pyspark --driver-java-options=-Dio.netty.tryReflectionSetAccessible=true\n</code></pre> <pre><code>Python 3.9.1 (default, Feb  3 2021, 07:38:02)\nType 'copyright', 'credits' or 'license' for more information\nIPython 7.20.0 -- An enhanced Interactive Python. Type '?' for help.\nSetting default log level to \"WARN\".\nTo adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\nWelcome to\n      ____              __\n     / __/__  ___ _____/ /__\n    _\\ \\/ _ \\/ _ `/ __/  '_/\n   /__ / .__/\\_,_/_/ /_/\\_\\   version 3.1.1\n      /_/\n\nUsing Python version 3.9.1 (default, Feb  3 2021 07:38:02)\nSpark context Web UI available at http://192.168.68.101:4040\nSpark context available as 'sc' (master = local[*], app id = local-1613571272142).\nSparkSession available as 'spark'.\n\nIn [1]:\n</code></pre> <pre><code>In [1]: spark.version\nOut[1]: '3.1.1'\n</code></pre>"},{"location":"SocketAuthHelper/","title":"SocketAuthHelper","text":"<p><code>SocketAuthHelper</code> is...FIXME</p>"},{"location":"SocketAuthServer/","title":"SocketAuthServer","text":""},{"location":"SocketAuthServer/#servetostream-utility","title":"serveToStream Utility <pre><code>serveToStream(\n  threadName: String,\n  authHelper: SocketAuthHelper)(writeFunc: OutputStream =&gt; Unit): Array[Any]\n</code></pre> <p><code>serveToStream</code>...FIXME</p> <p><code>serveToStream</code>\u00a0is used when:</p> <ul> <li>FIXME</li> </ul>","text":""},{"location":"SocketFuncServer/","title":"SocketFuncServer","text":"<p><code>SocketFuncServer</code> is...FIXME</p>"},{"location":"SparkConf/","title":"SparkConf","text":"<p><code>SparkConf</code> is a Python class.</p>"},{"location":"SparkConf/#creating-instance","title":"Creating Instance","text":"<p><code>SparkConf</code> takes the following to be created:</p> <ul> <li> <code>loadDefaults</code> flag (default: <code>True</code>) <li> <code>JVMView</code> (py4j) <li> JConf (default: <code>None</code>) <p>While being created, <code>SparkConf</code> uses the JVMView (of the SparkContext) unless the <code>_jconf</code> and <code>_jvm</code> are given.</p>"},{"location":"SparkConf/#demo","title":"Demo","text":"<pre><code>from pyspark import SparkConf\n</code></pre>"},{"location":"SparkContext/","title":"SparkContext","text":""},{"location":"SparkContext/#creating-instance","title":"Creating Instance","text":"<p><code>SparkContext</code> takes the following to be created:</p> <ul> <li> Master URL (default: <code>None</code>) <li> Application Name (default: <code>None</code>) <li> Spark Home (default: <code>None</code>) <li> Py Files (default: <code>None</code>) <li> Environment (default: <code>None</code>) <li> Batch Size (default: <code>0</code>) <li> <code>PickleSerializer</code> <li> <code>SparkConf</code> (default: <code>None</code>) <li> Gateway (default: <code>None</code>) <li> Corresponding <code>SparkContext</code> on JVM (default: <code>None</code>) <li> <code>BasicProfiler</code> <p>While being created, <code>SparkContext</code> _ensure_initialized (with the gateway and the conf) followed by _do_init.</p>"},{"location":"SparkContext/#demo","title":"Demo","text":"<pre><code>from pyspark import SparkContext\n</code></pre>"},{"location":"SparkContext/#javagateway","title":"JavaGateway <p><code>SparkContext</code> defines <code>_gateway</code> property for a <code>JavaGateway</code> that is given or launched when _ensure_initialized.</p>","text":""},{"location":"SparkContext/#jvmview","title":"JVMView <p><code>SparkContext</code> defines <code>_jvm</code> property for a <code>JVMView</code> (py4j) to access to the Java Virtual Machine of the JavaGateway.</p>","text":""},{"location":"SparkContext/#_ensure_initialized","title":"_ensure_initialized <pre><code>_ensure_initialized(\n  cls, instance=None, gateway=None, conf=None)\n</code></pre> <p><code>_ensure_initialized</code> is a <code>@classmethod</code>.</p> <p><code>_ensure_initialized</code> takes the given gateway or launch_gateway.</p> <p><code>_ensure_initialized</code>...FIXME</p> <p><code>_ensure_initialized</code>\u00a0is used when:</p> <ul> <li><code>SparkContext</code> is created and <code>setSystemProperty</code></li> <li>shell.py is launched</li> </ul>","text":""},{"location":"SparkContext/#_do_init","title":"_do_init <pre><code>_do_init(\n  self, master, appName, sparkHome,\n  pyFiles, environment, batchSize, serializer,\n  conf, jsc, profiler_cls)\n</code></pre> <p><code>_do_init</code>...FIXME</p>","text":""},{"location":"building-from-sources/","title":"Building from Sources","text":"<pre><code>$ java -version\nopenjdk version \"11.0.10\" 2021-01-19\nOpenJDK Runtime Environment AdoptOpenJDK (build 11.0.10+9)\nOpenJDK 64-Bit Server VM AdoptOpenJDK (build 11.0.10+9, mixed mode)\n</code></pre> <pre><code>./build/mvn \\\n  -Pyarn,kubernetes,hive,hive-thriftserver,scala-2.12 \\\n  -DskipTests \\\n  clean install\n</code></pre>"},{"location":"building-from-sources/#building-pyspark-related-operators","title":"Building PySpark-Related Operators","text":"<pre><code>./build/mvn -DskipTests -pl :spark-sql_2.12 clean install\n</code></pre> <pre><code>cp sql/core/target/spark-sql_2.12-3.1.1.jar assembly/target/scala-2.12/jars/\n</code></pre>"},{"location":"configuration-properties/","title":"Configuration Properties","text":""},{"location":"configuration-properties/#sparkpysparkdriverpython","title":"spark.pyspark.driver.python <p>Default: (undefined)</p>","text":""},{"location":"configuration-properties/#sparkpysparkpython","title":"spark.pyspark.python <p>Default: (undefined)</p>","text":""},{"location":"configuration-properties/#sparkpythonusedaemon","title":"spark.python.use.daemon <p>Because forking processes from Java is expensive, we prefer to launch a single Python daemon, <code>pyspark/daemon.py</code> (by default) and tell it to fork new workers for our tasks. This daemon currently only works on UNIX-based systems now because it uses signals for child management, so we can also fall back to launching workers, <code>pyspark/worker.py</code> (by default) directly.</p> <p>Default: <code>true</code> (always disabled on Windows)</p> <p>Used when PythonWorkerFactory is created</p>","text":""},{"location":"configuration-properties/#sparkpythondaemonmodule","title":"spark.python.daemon.module <p>Default: <code>pyspark.daemon</code></p> <p>Used when PythonWorkerFactory is created</p>","text":""},{"location":"configuration-properties/#sparkpythonworkermodule","title":"spark.python.worker.module <p>Default: (undefined)</p> <p>Used when PythonWorkerFactory is created</p>","text":""},{"location":"configuration-properties/#spark.sql","title":"spark.sql","text":""},{"location":"configuration-properties/#spark.sql.execution.arrow.pyspark.enabled","title":"execution.arrow.pyspark.enabled <p>spark.sql.execution.arrow.pyspark.enabled</p> <p>Enables Arrow Optimization</p> <p>Default: <code>false</code></p>","text":""},{"location":"environment-variables/","title":"Environment Variables","text":"<p>PySpark uses environment variables to configure execution environment.</p>"},{"location":"environment-variables/#pyspark_gateway_port","title":"PYSPARK_GATEWAY_PORT","text":""},{"location":"environment-variables/#pyspark_gateway_secret","title":"PYSPARK_GATEWAY_SECRET","text":""},{"location":"environment-variables/#pyspark_pin_thread","title":"PYSPARK_PIN_THREAD <p>Enables pinned thread mode to synchronize PVM threads with JVM threads based on Py4J's ClientServer (<code>true</code>) or GatewayServer (<code>false</code>)</p> <p>Default: <code>false</code></p> <p>Used when:</p> <ul> <li>launch_gateway is executed</li> <li>Py4JServer is created (and initializes the server)</li> </ul>","text":""},{"location":"logging/","title":"Logging","text":"<p>Delta Lake uses the same logging infrastructure as Apache Spark.</p>"},{"location":"overview/","title":"PySpark","text":"<p>PySpark is the Python frontend for Apache Spark.</p>"},{"location":"overview/#shellpy","title":"shell.py","text":"<p>pyspark shell defines PYTHONSTARTUP environment variable to execute shell.py before the first prompt is displayed in Python interactive mode.</p>"},{"location":"overview/#py4j","title":"Py4J","text":"<p>java_gateway uses Py4J - A Bridge between Python and Java:</p> <p>Py4J enables Python programs running in a Python interpreter to dynamically access Java objects in a Java Virtual Machine. Methods are called as if the Java objects resided in the Python interpreter and Java collections can be accessed through standard Python collection methods. Py4J also enables Java programs to call back Python objects.</p>"},{"location":"overview/#pysparksql-package","title":"pyspark.sql Package","text":"<p><code>pyspark.sql</code> is a Python package for Spark SQL.</p> <pre><code>from pyspark.sql import *\n</code></pre> <p>Tip</p> <p>Learn more about Modules and Packages in Python in The Python Tutorial.</p>"},{"location":"overview/#__init__py","title":"__init__.py","text":"<p>The <code>__init__.py</code> files are required to make Python treat directories containing the file as packages.</p> <p>Per 6.4.1. Importing * From a Package:</p> <p>The import statement uses the following convention: if a package's <code>__init__.py</code> code defines a list named <code>__all__</code>, it is taken to be the list of module names that should be imported when <code>from package import *</code> is encountered.</p> <p>Per Public and Internal Interfaces in PEP 8 -- Style Guide for Python Code:</p> <p>To better support introspection, modules should explicitly declare the names in their public API using the <code>__all__</code> attribute.</p> <p>From <code>python/pyspark/sql/__init__.py</code>:</p> <pre><code>__all__ = [\n    'SparkSession', 'SQLContext', 'HiveContext', 'UDFRegistration',\n    'DataFrame', 'GroupedData', 'Column', 'Catalog', 'Row',\n    'DataFrameNaFunctions', 'DataFrameStatFunctions', 'Window', 'WindowSpec',\n    'DataFrameReader', 'DataFrameWriter', 'PandasCogroupedOps'\n]\n</code></pre>"},{"location":"overview/#pandas","title":"pandas","text":"<p>The minimum version of Pandas is <code>0.23.2</code> (and PandasConversionMixin asserts that).</p> <pre><code>import pandas as pd\n</code></pre>"},{"location":"overview/#pyarrow","title":"pyarrow","text":"<p>The minimum version of PyArrow is <code>1.0.0</code> (and PandasConversionMixin asserts that).</p> <pre><code>import pyarrow\n</code></pre>"},{"location":"overview/#python-mixins","title":"Python Mixins","text":"<p>From 8.7. Class definitions:</p> <p>classdef    ::=  [decorators] \"class\" classname [inheritance] \":\" suite</p> <p>The inheritance list usually gives a list of base classes</p> <p>PySpark uses mixins:</p> <ul> <li>PandasConversionMixin</li> <li>PandasMapOpsMixin</li> <li>SparkConversionMixin</li> </ul>"},{"location":"overview/#pandas-user-defined-functions","title":"Pandas User-Defined Functions","text":"<p>Pandas User-Defined Functions (Vectorized User-Defined Functions) are user-defined functions that are executed using Apache Arrow to transfer data and Pandas to work with the data, which allows vectorized operations.</p> <p>Pandas UDFs are defined using pandas_udf function as a decorator (using <code>@pandas_udf(returnType, functionType)</code> annotation) or to wrap the function, and no additional configuration.</p> <p>A Pandas UDF behaves as a regular PySpark function API in general.</p> <p>The minimum versions supported:</p> <ul> <li>pandas 0.23.2</li> <li>pyarrow 1.0.0</li> </ul> <p>As of Spark 3.0 with Python 3.6+, using Python type hints to specify type hints for the pandas UDF is encouraged (instead of specifying pandas UDF type via <code>functionType</code> argument).</p> <p>The type hint should use <code>pandas.Series</code> in most cases (except <code>pandas.DataFrame</code>).</p>"},{"location":"pyspark/","title":"PySpark Shell","text":"<p><code>pyspark</code> is a Spark shell for Python.</p>"},{"location":"arrow-optimization/","title":"Arrow Optimization","text":"<p>Arrow Optimization is an optimization that uses Apache Arrow for columnar data transfers in the following:</p> <ul> <li>pyspark.sql.DataFrame.toPandas</li> <li>pyspark.sql.SparkSession.createDataFrame (when called with a Pandas <code>DataFrame</code> or a NumPy <code>ndarray</code>)</li> </ul> <p>The following data types are unsupported: <code>ArrayType</code> of <code>TimestampType</code>.</p>"},{"location":"connect/","title":"Spark Connect","text":"<p>PySpark supports remote connection to Spark clusters using Spark Connect (Spark SQL).</p> <pre><code>$ ./bin/pyspark --help\nUsage: ./bin/pyspark [options]\n\nOptions:\n Spark Connect only:\n   --remote CONNECT_URL       URL to connect to the server for Spark Connect, e.g.,\n                              sc://host:port. --master and --deploy-mode cannot be set\n                              together with this option. This option is experimental, and\n                              might change between minor releases.\n ...\n</code></pre> <p>Spark Connect for Python requires the following Python libraries:</p> Module Version pandas 1.0.5 pyarrow 1.0.0 grpc 1.48.1 <pre><code>// switching to an conda environment with the libraries\n$ conda activate pyspark\n\n$ ./bin/pyspark --remote sc://localhost\nPython 3.10.10 (main, Mar 21 2023, 13:41:39) [Clang 14.0.6 ] on darwin\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\nWelcome to\n      ____              __\n     / __/__  ___ _____/ /__\n    _\\ \\/ _ \\/ _ `/ __/  '_/\n   /__ / .__/\\_,_/_/ /_/\\_\\   version 3.4.0\n      /_/\n\nUsing Python version 3.10.10 (main, Mar 21 2023 13:41:39)\nClient connected to the Spark Connect server at localhost\nSparkSession available as 'spark'.\n\n&gt;&gt;&gt; spark.client\n&lt;pyspark.sql.connect.client.SparkConnectClient object at 0x7fed8867ab90&gt;\n</code></pre>"},{"location":"connect/#is_remote","title":"is_remote","text":"<pre><code># from pyspark.sql.utils import is_remote\nis_remote() -&gt; bool\n</code></pre> <p><code>is_remote</code> is <code>True</code> when <code>SPARK_REMOTE</code> environment variable is defined (in <code>os.environ</code>).</p>"},{"location":"demo/","title":"Demos","text":"<ol> <li>Executing PySpark Applications Using spark-submit</li> <li>Running PySpark Application on minikube</li> </ol>"},{"location":"demo/executing-pyspark-applications-using-spark-submit/","title":"Demo: Executing PySpark Applications Using spark-submit","text":"<p>PySpark applications are executed using <code>spark-submit</code> (Spark Core) command-line application.</p> <pre><code>spark-submit 1.py extra args\n</code></pre> <p>For a PySpark application, <code>spark-submit</code> uses PythonRunner and launches an extra python process:</p> <pre><code>ps -o pid,ppid,command | grep python | grep -v grep\n</code></pre> <pre><code>org.apache.spark.deploy.SparkSubmit 1.py extra args\n</code></pre> <pre><code>Python /usr/local/bin/ipython 1.py extra args\n</code></pre>"},{"location":"demo/executing-pyspark-applications-using-spark-submit/#spark_print_launch_command-environment-variable","title":"SPARK_PRINT_LAUNCH_COMMAND Environment Variable","text":"<p>Use <code>SPARK_PRINT_LAUNCH_COMMAND</code> environment variable to have the complete Spark command printed out to the standard output (cf. spark-submit shell script).</p> <pre><code>SPARK_PRINT_LAUNCH_COMMAND=1 spark-submit 1.py extra args\n</code></pre>"},{"location":"demo/executing-pyspark-applications-using-spark-submit/#verbose-option","title":"verbose Option","text":"<p>Use <code>--verbose</code> option for verbose debugging output.</p> <pre><code>Parsed arguments:\n  ...\n  pyFiles                 null\n  ...\n  primaryResource         file:/Users/jacek/dev/sandbox/python-sandbox/1.py\n  name                    1.py\n  childArgs               [extra args]\n...\nMain class:\norg.apache.spark.deploy.PythonRunner\nArguments:\nfile:/Users/jacek/dev/sandbox/python-sandbox/1.py\nnull\nextra\nargs\nSpark config:\n(spark.app.name,1.py)\n(spark.master,local[*])\n(spark.submit.pyFiles,)\n(spark.submit.deployMode,client)\n</code></pre>"},{"location":"demo/running-pyspark-application-on-minikube/","title":"Demo: Running PySpark Application on minikube","text":"<p>This demo shows how to run a PySpark application on Kubernetes (using minikube).</p> <p>Tip</p> <p>This is a follow-up demo to Demo: Running Spark Application on minikube in the The Internals of Spark on Kubernetes.</p>"},{"location":"features/","title":"Features","text":""},{"location":"ml/","title":"PySpark MLlib","text":"<p>PySpark MLlib is a Python module to work with Spark MLlib for <code>DataFrame</code>-based machine learning pipelines.</p> <pre><code>from pyspark.ml import *\n</code></pre>"},{"location":"ml/Distributor/","title":"Distributor","text":"<p><code>Distributor</code> is the parent (abstract) class of TorchDistributor.</p>"},{"location":"ml/Distributor/#creating-instance","title":"Creating Instance","text":"<p><code>Distributor</code> takes the following to be created:</p> <ul> <li> Number of processes (default: <code>1</code>) <li> <code>local_mode</code> flag (default: <code>True</code>) <li> <code>use_gpu</code> flag (default: <code>True</code>) <p>Abstract Class</p> <p><code>Distributor</code> is not supposed to be created directly.</p>"},{"location":"ml/Distributor/#_get_num_tasks","title":"_get_num_tasks","text":"<pre><code>_get_num_tasks(\n  self) -&gt; int\n</code></pre> <p><code>_get_num_tasks</code>...FIXME</p>"},{"location":"ml/Distributor/#get_gpus_owned","title":"get_gpus_owned","text":"<pre><code>get_gpus_owned(\n  context: Union[SparkContext, BarrierTaskContext]) -&gt; List[str]\n</code></pre> <p><code>get_gpus_owned</code>...FIXME</p> <p><code>get_gpus_owned</code> is used when:</p> <ul> <li><code>TorchDistributor</code> is requested to _run_local_training and _get_spark_task_function</li> </ul>"},{"location":"ml/TorchDistributor/","title":"TorchDistributor","text":"<p><code>TorchDistributor</code> is a Distributor.</p> <pre><code>from pyspark.ml.torch.distributor import TorchDistributor\n\ndistributor = TorchDistributor(\n    num_processes=1,\n    local_mode=False,\n    use_gpu=False)\n</code></pre> <pre><code># Use a path to a training script\n# and variable-length kwargs\ndistributor.run(\n    \"train.py\",\n    \"--learning-rate=1e-3\",\n    \"--batch-size=64\",\n    \"--my-key=my-value\")\n\n# Started local training with 1 processes\n# NOTE: Redirects are currently not supported in Windows or MacOs.\n# Finished local training with 1 processes\n</code></pre> <pre><code># Use a Callable (function)\n# The number of positional arguments is the number of kwargs\ndef train(a, b, c):\n    print(f\"Got a={a}, b={b}, c={c}\")\n    return 'success'\n\ndistributor.run(\n    train,\n    \"--learning-rate=1e-3\",\n    \"--batch-size=64\",\n    \"--my-key=my-value\")\n\n# Started distributed training with 1 executor proceses\n# NOTE: Redirects are currently not supported in Windows or MacOs.    (0 + 1) / 1]\n# NOTE: Redirects are currently not supported in Windows or MacOs.\n# Got a=--learning-rate=1e-3, b=--batch-size=64, c=--my-key=my-value\n# Got a=--learning-rate=1e-3, b=--batch-size=64, c=--my-key=my-value\n# Finished distributed training with 1 executor proceses\n# 'success'\n</code></pre>"},{"location":"ml/TorchDistributor/#run","title":"Running Distributed Training","text":"<pre><code>run(\n    self,\n    train_object: Union[Callable, str],\n    *args: Any) -&gt; Optional[Any]\n</code></pre> <p><code>run</code> determines whether to run a PyTorch function or a script (based on the given <code>train_object</code>). For a script, <code>run</code> uses _run_training_on_pytorch_file. Otherwise, <code>run</code> uses _run_training_on_pytorch_function.</p> <p>In the end, <code>run</code> runs the training. In local mode, <code>run</code> runs local training. Otherwise, <code>run</code> runs distributed training.</p>"},{"location":"ml/TorchDistributor/#_run_local_training","title":"Local Training","text":"<pre><code>_run_local_training(\n    self,\n    framework_wrapper_fn: Callable,\n    train_object: Union[Callable, str],\n    *args: Any,\n) -&gt; Optional[Any]\n</code></pre> <p><code>_run_local_training</code> looks up <code>CUDA_VISIBLE_DEVICES</code> among the environment variables.</p> <p>With use_gpu, <code>_run_local_training</code>...FIXME</p> <p><code>_run_local_training</code> prints out the following INFO message to the logs:</p> <pre><code>Started local training with [num_processes] processes\n</code></pre> <p><code>_run_local_training</code> executes the given <code>framework_wrapper_fn</code> function (with the input_params, the given <code>train_object</code> and the <code>args</code>).</p> <p>In the end, <code>_run_local_training</code> prints out the following INFO message to the logs:</p> <pre><code>Finished local training with [num_processes] processes\n</code></pre>"},{"location":"ml/TorchDistributor/#_run_distributed_training","title":"Distributed Training","text":"<pre><code>_run_distributed_training(\n    self,\n    framework_wrapper_fn: Callable,\n    train_object: Union[Callable, str],\n    *args: Any,\n) -&gt; Optional[Any]\n</code></pre> <p><code>_run_distributed_training</code>...FIXME</p>"},{"location":"pandas-on-spark/","title":"pandas API on Spark","text":"<p>pandas API on Spark (pyspark.pandas package) has been added to PySpark to execute pandas code on Spark clusters with no changes (except the import).</p> <p>There are two related PySpark packages with pandas support:</p> <ul> <li>pyspark.pandas</li> <li>pyspark.sql.pandas</li> </ul> <p>Spark Structured Streaming</p> <p>pandas API on Spark does not support Spark Structured Streaming (streaming queries).</p>"},{"location":"pandas-on-spark/#modules","title":"Modules","text":"<p>pandas API on Spark requires that the following modules to be installed:</p> Module Version pandas 1.0.5 PyArrow 1.0.0"},{"location":"pandas-on-spark/#PYARROW_IGNORE_TIMEZONE","title":"PYARROW_IGNORE_TIMEZONE","text":"<p>For PyArrow 2.0.0 and above, pandas API on Spark requires <code>PYARROW_IGNORE_TIMEZONE</code> environment variable to be set to <code>1</code> (on the driver and executors).</p>"},{"location":"pandas-on-spark/#PYSPARK_PANDAS_USAGE_LOGGER","title":"PYSPARK_PANDAS_USAGE_LOGGER <p>pandas API on Spark uses <code>PYSPARK_PANDAS_USAGE_LOGGER</code> (formerly <code>KOALAS_USAGE_LOGGER</code>) environment variable for a usage logger.</p>","text":""},{"location":"pandas-on-spark/#demo","title":"Demo <pre><code># The following would be required if we used pandas\n# import pandas as pd\n\n# but we don't need it anymore \ud83d\ude0a\n\n# The only change is supposed to be this extra `pyspark` prefix\n# in the name of the package\n\nimport pyspark.pandas as pd\n</code></pre> Python   <pre><code>pd.read_csv(\"people.csv\")\n</code></pre>    <pre><code>   id  name\n0   0  zero\n1   1   one\n2   2   two\n</code></pre>","text":""},{"location":"pyspark/java_gateway/","title":"java_gateway.py","text":"<p><code>java_gateway</code> is a Python module that allows launching a gateway process to establish communication channel to Py4JServer.</p>"},{"location":"pyspark/java_gateway/#launch_gateway","title":"launch_gateway <pre><code>launch_gateway(\n  conf=None,\n  popen_kwargs=None)\n</code></pre> <p><code>launch_gateway</code> reads PYSPARK_GATEWAY_PORT and PYSPARK_GATEWAY_SECRET environment variables if defined and assumes that the child Java gateway process has already been started (e.g. PythonGatewayServer).</p> <p> <p>Otherwise, <code>launch_gateway</code> builds the command to start <code>spark-submit</code>:</p> <ol> <li>Finds <code>SPARK_HOME</code> with <code>./bin/spark-submit</code></li> <li>Appends all the configuration properties (from the input <code>conf</code>) using <code>--conf</code></li> <li>Appends <code>PYSPARK_SUBMIT_ARGS</code> environment variable if defined or assumes <code>pyspark-shell</code></li> </ol> <p><code>launch_gateway</code> sets up <code>_PYSPARK_DRIVER_CONN_INFO_PATH</code> environment variable to point at an unique temporary file.</p> <p><code>launch_gateway</code> configures a pipe to stdin for the corresponding Java gateway process to use to monitor the Python process.</p> <p><code>launch_gateway</code> starts <code>bin/spark-submit</code> command and waits for a connection info file to be created at <code>_PYSPARK_DRIVER_CONN_INFO_PATH</code>. <code>launch_gateway</code> reads the port and the secret from the file once available.</p> <p><code>launch_gateway</code> connects to the gateway using py4j's <code>ClientServer</code> or <code>JavaGateway</code> based on PYSPARK_PIN_THREAD environment variable.</p> <p><code>launch_gateway</code> imports Spark packages and classes (using py4j):</p> <ul> <li><code>org.apache.spark.SparkConf</code></li> <li><code>org.apache.spark.api.java.*</code></li> <li><code>org.apache.spark.api.python.*</code></li> <li><code>org.apache.spark.ml.python.*</code></li> <li><code>org.apache.spark.mllib.api.python.*</code></li> <li><code>org.apache.spark.resource.*</code></li> <li><code>org.apache.spark.sql.*</code></li> <li><code>org.apache.spark.sql.api.python.*</code></li> <li><code>org.apache.spark.sql.hive.*</code></li> <li><code>scala.Tuple2</code></li> </ul> <p><code>launch_gateway</code>\u00a0is used when:</p> <ul> <li><code>SparkContext</code> is requested to _ensure_initialized</li> </ul>","text":""},{"location":"pyspark/shell/","title":"shell.py","text":"<p><code>shell.py</code> script is the interactive shell of PySpark.</p> <p><code>shell.py</code> defines the following variables:</p> <ul> <li><code>sc</code> being pyspark.SparkContext</li> <li><code>spark</code> being pyspark.sql.session.SparkSession</li> <li><code>sql</code> being SparkSession.sql</li> <li><code>sqlContext</code> and <code>sqlCtx</code> for compatibility</li> </ul>"},{"location":"pyspark/pandas/","title":"pyspark.pandas Package","text":"<p>When imported (that triggers <code>__init__.py</code>), <code>pyspark.pandas</code> does monkey-patching of <code>pandas.DataFrame</code> and <code>pandas.Series</code> classes (using __class_getitem__ dunder method).</p> Pandas PySpark pandas.DataFrame <code>pyspark.pandas.frame.DataFrame</code> pandas.Series <code>pyspark.pandas.series.Series</code>"},{"location":"pyspark/pandas/DataFrame/","title":"DataFrame","text":"<p><code>DataFrame</code> is a Frame with an InternalFrame.</p> <p><code>DataFrame</code> is a <code>Generic[T]</code> (Python).</p>"},{"location":"pyspark/pandas/DataFrame/#creating-instance","title":"Creating Instance","text":"<p><code>DataFrame</code> takes the following to be created:</p> <ul> <li> data (optional) <li> index (optional) <li> columns (optional) <li> dtype (optional) <li> copy (optional)"},{"location":"pyspark/pandas/DataFrame/#_internal_frame","title":"_internal_frame","text":"<p><code>DataFrame</code> is given or creates an InternalFrame when created.</p> <pre><code>object.__setattr__(self, \"_internal_frame\", internal)\n</code></pre>"},{"location":"pyspark/pandas/DataFrame/#_internal","title":"InternalFrame","text":"Frame <pre><code>@property\ndef _internal(\n    self) -&gt; InternalFrame\n</code></pre> <p><code>_internal</code> is part of the Frame abstraction.</p> <p><code>_internal</code> returns the _internal_frame (that is expected to be of type InternalFrame).</p>"},{"location":"pyspark/pandas/InternalFrame/","title":"InternalFrame","text":"<p><code>InternalFrame</code> is the underlying managed Spark DataFrame of pyspark.pandas.DataFrame.</p>"},{"location":"pyspark/pandas/InternalFrame/#creating-instance","title":"Creating Instance","text":"<p><code>InternalFrame</code> takes the following to be created:</p> <ul> <li>Spark DataFrame</li> <li> <code>index_spark_columns</code> (optional) <li> <code>index_names</code> (optional) <li> <code>index_fields</code> (optional) <li> <code>column_labels</code> (optional) <li> <code>data_spark_columns</code> (optional) <li> <code>data_fields</code> (optional) <li> <code>column_label_names</code> (optional)"},{"location":"pyspark/pandas/InternalFrame/#spark_frame","title":"Spark DataFrame","text":"<p><code>InternalFrame</code> is given a Spark DataFrame when created.</p>"},{"location":"pyspark/pandas/InternalFrame/#_sdf","title":"Managed Spark DataFrame","text":"<p><code>_sdf</code> is the underlying managed Spark DataFrame.</p> <p><code>_sdf</code> is the Spark DataFrame with attach_default_index and __natural_order__ columns selected.</p>"},{"location":"pyspark/pandas/InternalFrame/#SPARK_DEFAULT_INDEX_NAME","title":"Default Index Column Name","text":"<p><code>InternalFrame</code> uses the following as the name of the default index column:</p> <pre><code>__index_level_0__\n</code></pre>"},{"location":"pyspark/pandas/InternalFrame/#SPARK_INDEX_NAME_PATTERN","title":"Index Column Pattern","text":"<p><code>InternalFrame</code> defines a regular pattern to match the index columns.</p> <pre><code>__index_level_[0-9]+__\n</code></pre> <p>It is invalid to name columns in the Spark DataFrame to match the index column pattern. Index columns must not be in the columns of the Spark DataFrame.</p>"},{"location":"pyspark/pandas/InternalFrame/#to_internal_spark_frame","title":"to_internal_spark_frame","text":"<pre><code>@lazy_property\ndef to_internal_spark_frame(\n    self) -&gt; SparkDataFrame\n</code></pre> <p><code>to_internal_spark_frame</code> returns the spark_frame with the index_spark_columns followed by the data_spark_columns.</p>"},{"location":"pyspark/pandas/InternalFrame/#spark_frame","title":"spark_frame","text":"<pre><code>from pyspark.sql import DataFrame as SparkDataFrame\n\n@property\ndef spark_frame(\n    self) -&gt; SparkDataFrame\n</code></pre> <p><code>spark_frame</code> returns the underlying managed Spark DataFrame.</p>"},{"location":"pyspark/pandas/InternalFrame/#demo","title":"Demo","text":"<pre><code>from pyspark import pandas as ps\n\npsdf = ps.DataFrame({\n    'A': [1, 2, 3, 4],\n    'B': [5, 6, 7, 8],\n    'C': [9, 10, 11, 12],\n    'D': [13, 14, 15, 16],\n    'E': [17, 18, 19, 20]}, columns = ['A', 'B', 'C', 'D', 'E'])\n\npsdf._internal\n# &lt;pyspark.pandas.internal.InternalFrame object at 0x7f7ff024f820&gt;\n\npsdf._internal.spark_frame\n# DataFrame[__index_level_0__: bigint, A: bigint, B: bigint, C: bigint, D: bigint, E: bigint, __natural_order__: bigint]\n\npsdf._internal.spark_frame.show()\n# +-----------------+---+---+---+---+---+-----------------+\n# |__index_level_0__|  A|  B|  C|  D|  E|__natural_order__|\n# +-----------------+---+---+---+---+---+-----------------+\n# |                0|  1|  5|  9| 13| 17|      17179869184|\n# |                1|  2|  6| 10| 14| 18|      42949672960|\n# |                2|  3|  7| 11| 15| 19|      68719476736|\n# |                3|  4|  8| 12| 16| 20|      94489280512|\n# +-----------------+---+---+---+---+---+-----------------+\n\npsdf._internal.to_internal_spark_frame.show()\n# +-----------------+---+---+---+---+---+\n# |__index_level_0__|  A|  B|  C|  D|  E|\n# +-----------------+---+---+---+---+---+\n# |                0|  1|  5|  9| 13| 17|\n# |                1|  2|  6| 10| 14| 18|\n# |                2|  3|  7| 11| 15| 19|\n# |                3|  4|  8| 12| 16| 20|\n# +-----------------+---+---+---+---+---+\n</code></pre>"},{"location":"pyspark/pandas/generic/","title":"pyspark.pandas.generic Package","text":"<p><code>pyspark.pandas.generic</code> package is...FIXME</p>"},{"location":"pyspark/pandas/generic/Frame/","title":"Frame","text":"<p><code>Frame</code> is an abstraction of frames that behave like pandas.DataFrame and pandas.Series.</p> <pre><code>class Frame(object, metaclass=ABCMeta)\n</code></pre>"},{"location":"pyspark/pandas/generic/Frame/#contract","title":"Contract","text":""},{"location":"pyspark/pandas/generic/Frame/#__getitem","title":"__getitem__ <pre><code>@abstractmethod\ndef __getitem__(\n  self,\n  key: Any) -&gt; Any\n</code></pre> <pre><code>class hello():\n  def __getitem__(self, key):\n    print(f\"__getitem__({key})\")\n\nh = hello()\n\n&gt;&gt;&gt; h[4]\n__getitem__(4)\n</code></pre>","text":""},{"location":"pyspark/pandas/generic/Frame/#_internal","title":"_internal <pre><code>@property\n@abstractmethod\ndef _internal(\n  self) -&gt; InternalFrame\n</code></pre>","text":""},{"location":"pyspark/pandas/generic/Frame/#implementations","title":"Implementations","text":"<ul> <li>DataFrame</li> <li><code>Series</code></li> </ul>"},{"location":"pyspark/sql/functions/","title":"functions.py","text":"<p><code>functions.py</code> module belongs to <code>pyspark.sql</code> package.</p>"},{"location":"pyspark/sql/functions/#udf","title":"udf <pre><code>udf(f=None, returnType=StringType())\n</code></pre> <p><code>udf</code>...FIXME</p>","text":""},{"location":"pyspark/sql/udf/","title":"udf.py","text":"<p><code>udf.py</code> module belongs to <code>pyspark.sql</code> package.</p>"},{"location":"pyspark/sql/udf/#public-objects","title":"Public Objects <p><code>udf</code> module defines UDFRegistration as the public object.</p>","text":""},{"location":"pyspark/sql/udf/#_create_udf","title":"_create_udf <pre><code>_create_udf(f, returnType, evalType)\n</code></pre> <p><code>_create_udf</code> creates an UserDefinedFunction (with the name of the object to be the name of function <code>f</code>).</p> <p><code>_create_udf</code>\u00a0is used when:</p> <ul> <li>pandas_udf function is used</li> <li>udf function is used</li> </ul>","text":""},{"location":"pyspark/sql/pandas/","title":"pyspark.sql.pandas Package","text":"<p><code>pyspark.sql.pandas</code> package is...FIXME</p>"},{"location":"pyspark/sql/pandas/functions/","title":"functions.py","text":"<p><code>functions.py</code> defines pandas_udf for pandas user defined functions.</p> <p><code>functions.py</code> is part of <code>pyspark.sql.pandas</code> package.</p> <pre><code>from pyspark.sql.functions import pandas_udf\n</code></pre>"},{"location":"pyspark/sql/pandas/functions/#pandas_udf","title":"pandas_udf <pre><code>pandas_udf(\n  f=None,\n  returnType=None,\n  functionType=None)\n</code></pre> <p><code>pandas_udf</code> creates a pandas user-defined functions.</p> <p><code>pandas_udf</code> _create_pandas_udf (possibly creating a partial function with <code>functools.partial</code> (Python) when used as a decorator).</p>","text":""},{"location":"pyspark/sql/pandas/functions/#pandas_udf_decorator","title":"Decorator","text":"<p><code>pandas_udf</code> can and usually is used as a Python decorator with two positional arguments for the return and function types.</p> <pre><code>@pandas_udf(returnType, functionType)\n</code></pre>"},{"location":"pyspark/sql/pandas/functions/#pandas_udf_returnType","title":"returnType","text":"<p><code>returnType</code> can be one of the following:</p> <ul> <li><code>pyspark.sql.types.DataType</code></li> <li>A DDL-formatted type string</li> </ul>"},{"location":"pyspark/sql/pandas/functions/#pandas_udf_functionType","title":"functionType","text":"<p><code>functionType</code> must be one the values from <code>PandasUDFType</code>:</p> <ul> <li><code>PythonEvalType.SQL_SCALAR_PANDAS_UDF</code></li> <li><code>PythonEvalType.SQL_SCALAR_PANDAS_ITER_UDF</code></li> <li><code>PythonEvalType.SQL_GROUPED_MAP_PANDAS_UDF</code></li> <li><code>PythonEvalType.SQL_GROUPED_AGG_PANDAS_UDF</code></li> <li><code>PythonEvalType.SQL_MAP_PANDAS_ITER_UDF</code></li> <li><code>PythonEvalType.SQL_COGROUPED_MAP_PANDAS_UDF</code></li> </ul>"},{"location":"pyspark/sql/pandas/functions/#_create_pandas_udf","title":"_create_pandas_udf","text":"<pre><code>_create_pandas_udf(\n  f,\n  returnType,\n  evalType)\n</code></pre> <p><code>_create_pandas_udf</code>...FIXME</p>"},{"location":"runners/ArrowPythonRunner/","title":"ArrowPythonRunner","text":"<p><code>ArrowPythonRunner</code> is a pyspark runner with <code>PythonArrowOutput</code> for...FIXME</p>"},{"location":"runners/ArrowPythonRunner/#creating-instance","title":"Creating Instance","text":"<p><code>ArrowPythonRunner</code> takes the following to be created:</p> <ul> <li> <code>Seq[ChainedPythonFunctions]</code> <li> Eval Type <li> Argument Offsets (<code>Array[Array[Int]]</code>) <li> <code>Schema</code> (Spark SQL) <li> TimeZone ID <li> Configuration (<code>Map[String, String]</code>) <p><code>ArrowPythonRunner</code> is created when AggregateInPandasExec, <code>ArrowEvalPythonExec</code>, <code>FlatMapGroupsInPandasExec</code>, <code>MapInPandasExec</code>, <code>WindowInPandasExec</code> physical operators are executed.</p>"},{"location":"runners/BasePythonRunner/","title":"BasePythonRunner","text":"<p><code>BasePythonRunner</code> is an abstraction of Python Runners.</p>"},{"location":"runners/BasePythonRunner/#contract","title":"Contract","text":""},{"location":"runners/BasePythonRunner/#newreaderiterator","title":"newReaderIterator <pre><code>newReaderIterator(\n  stream: DataInputStream,\n  writerThread: WriterThread,\n  startTime: Long,\n  env: SparkEnv,\n  worker: Socket,\n  releasedOrClosed: AtomicBoolean,\n  context: TaskContext): Iterator[OUT]\n</code></pre> <p>Used when <code>BasePythonRunner</code> is requested to compute</p>","text":""},{"location":"runners/BasePythonRunner/#newwriterthread","title":"newWriterThread <pre><code>newWriterThread(\n  env: SparkEnv,\n  worker: Socket,\n  inputIterator: Iterator[IN],\n  partitionIndex: Int,\n  context: TaskContext): WriterThread\n</code></pre> <p>Used when <code>BasePythonRunner</code> is requested to compute</p>","text":""},{"location":"runners/BasePythonRunner/#implementations","title":"Implementations","text":"<ul> <li>ArrowPythonRunner</li> <li>CoGroupedArrowPythonRunner</li> <li>PythonRunner</li> <li>PythonUDFRunner</li> </ul>"},{"location":"runners/BasePythonRunner/#scala-definition","title":"Scala Definition","text":"<p><code>BasePythonRunner</code> is a type constructor in Scala (generic class in Java) with the following definition:</p> <pre><code>abstract class BasePythonRunner[IN, OUT](...) {\n// ...\n}\n</code></pre> <p><code>BasePythonRunner</code> uses <code>IN</code> and <code>OUT</code> as the name of the types for the input and output values.</p>"},{"location":"runners/BasePythonRunner/#creating-instance","title":"Creating Instance","text":"<p><code>BasePythonRunner</code> takes the following to be created:</p> <ul> <li> <code>ChainedPythonFunctions</code> <li> Eval Type <li> Argument Offsets <p><code>BasePythonRunner</code> requires that the number of ChainedPythonFunctions and Argument Offsets are the same.</p> <p>Abstract Class</p> <p><code>BasePythonRunner</code> is an abstract class and cannot be created directly. It is created indirectly for the concrete BasePythonRunners.</p>"},{"location":"runners/BasePythonRunner/#computing-result","title":"Computing Result <pre><code>compute(\n  inputIterator: Iterator[IN],\n  partitionIndex: Int,\n  context: TaskContext): Iterator[OUT]\n</code></pre> <p><code>compute</code> makes sure that <code>spark.executorEnv.OMP_NUM_THREADS</code> configuration option is set or defaults to <code>spark.executor.cores</code> property.</p> <p><code>compute</code> defines the following environment variables:</p> <ul> <li><code>SPARK_LOCAL_DIRS</code> to be the local directories of the local <code>DiskBlockManager</code></li> <li><code>SPARK_BUFFER_SIZE</code> to be the value of <code>spark.buffer.size</code> configuration property (default: <code>65536</code>)</li> </ul> <p><code>compute</code> can optionally define environment variables:</p> <ul> <li><code>SPARK_REUSE_WORKER</code> to be <code>1</code> based on <code>spark.python.worker.reuse</code> configuration property (default: <code>true</code>)</li> <li><code>PYSPARK_EXECUTOR_MEMORY_MB</code> to be the value of <code>spark.executor.pyspark.memory</code> configuration property if defined</li> </ul> <p><code>compute</code> requests the executor's <code>SparkEnv</code> to <code>createPythonWorker</code> (for a <code>pythonExec</code> and the environment variables) that requests PythonWorkerFactory to create a Python worker (and give a <code>java.net.Socket</code>).</p>  <p>FIXME</p> <p>Describe <code>pythonExec</code>.</p>  <p><code>compute</code> [newWriterThread] with the Python worker and the input arguments.</p> <p><code>compute</code> creates and starts a MonitorThread to watch the Python worker.</p> <p><code>compute</code> creates a new reader iterator to read lines from the Python worker's stdout.</p> <p><code>compute</code> is used when:</p> <ul> <li><code>PythonRDD</code> is requested to <code>compute</code></li> <li>AggregateInPandasExec, <code>ArrowEvalPythonExec</code>, <code>BatchEvalPythonExec</code>, <code>FlatMapCoGroupsInPandasExec</code>, <code>FlatMapGroupsInPandasExec</code> <code>MapInPandasExec</code>, <code>WindowInPandasExec</code> physical operators are executed</li> </ul>","text":""},{"location":"runners/PythonRunner/","title":"PythonRunner","text":"<p><code>PythonRunner</code> is a concrete BasePythonRunner.</p>"},{"location":"runners/PythonRunner/#creating-instance","title":"Creating Instance","text":"<p><code>PythonRunner</code> takes the following to be created:</p> <ul> <li> <code>Seq[ChainedPythonFunctions]</code> <p><code>PythonRunner</code> is created (indirectly using apply factory method) when:</p> <ul> <li><code>PythonRDD</code> is requested to compute a partition</li> <li><code>PythonForeachWriter</code> is requested for a PythonRunner</li> </ul>"},{"location":"runners/PythonRunner/#creating-pythonrunner","title":"Creating PythonRunner <pre><code>apply(\n  func: PythonFunction): PythonRunner\n</code></pre> <p><code>apply</code> simply creates a PythonRunner for the PythonFunction.</p> <p><code>apply</code> is used when:</p> <ul> <li><code>PythonRDD</code> is requested to compute a partition</li> <li><code>PythonForeachWriter</code> is requested for a PythonRunner</li> </ul>","text":""},{"location":"runners/PythonUDFRunner/","title":"PythonUDFRunner","text":"<p><code>PythonUDFRunner</code> is...FIXME</p>"},{"location":"sql/","title":"PySpark SQL","text":"<p>PySpark SQL is a Python module to work with Spark SQL.</p> <pre><code>from pyspark.sql import *\n</code></pre>"},{"location":"sql/AggregateInPandasExec/","title":"AggregateInPandasExec Physical Operator","text":"<p><code>AggregateInPandasExec</code> is a unary physical operator (Spark SQL).</p>"},{"location":"sql/AggregateInPandasExec/#creating-instance","title":"Creating Instance","text":"<p><code>AggregateInPandasExec</code> takes the following to be created:</p> <ul> <li> Grouping Expressions (Spark SQL) (<code>Seq[NamedExpression]</code>) <li> PythonUDFs <li> Result Named Expressions (Spark SQL) (<code>Seq[NamedExpression]</code>) <li> Child Physical Operator (Spark SQL) <p><code>AggregateInPandasExec</code> is created when <code>Aggregation</code> execution planning strategy (Spark SQL) is executed for <code>Aggregate</code> logical operators (Spark SQL) with PythonUDF aggregate expressions only.</p>"},{"location":"sql/AggregateInPandasExec/#executing-operator","title":"Executing Operator <pre><code>doExecute(): RDD[InternalRow]\n</code></pre> <p><code>doExecute</code> uses ArrowPythonRunner (one per partition) to execute PythonUDFs.</p> <p><code>doExecute</code> is part of the <code>SparkPlan</code> (Spark SQL) abstraction.</p>","text":""},{"location":"sql/DataFrame/","title":"DataFrame","text":"<p><code>DataFrame</code> is a Python class with PandasMapOpsMixin and PandasConversionMixin mixins.</p> <p><code>DataFrame</code> lives in <code>pyspark.sql.dataframe</code> module (together with <code>DataFrameNaFunctions</code> and <code>DataFrameStatFunctions</code>).</p> <pre><code>from pyspark.sql.dataframe import DataFrame\n</code></pre>"},{"location":"sql/DataFrame/#creating-instance","title":"Creating Instance","text":"<p><code>DataFrame</code> takes the following to be created:</p> <ul> <li> jdf <li> SQLContext"},{"location":"sql/DataFrame/#groupby","title":"groupBy <pre><code>groupBy(self, *cols)\n</code></pre> <p><code>groupBy</code> requests the _jdf to <code>groupBy</code> and creates a GroupedData with it.</p>","text":""},{"location":"sql/FlatMapGroupsInPandas/","title":"FlatMapGroupsInPandas Logical Operator","text":"<p><code>FlatMapGroupsInPandas</code> is a unary logical operator (Spark SQL).</p> <p><code>FlatMapGroupsInPandas</code> is planned as a FlatMapGroupsInPandasExec physical operator.</p>"},{"location":"sql/FlatMapGroupsInPandas/#creating-instance","title":"Creating Instance","text":"<p><code>FlatMapGroupsInPandas</code> takes the following to be created:</p> <ul> <li> Grouping Attributes (Spark SQL) <li> Function Expression (Spark SQL) <li> Output Attributes (Spark SQL) <li> Child Logical Operator (Spark SQL) <p><code>FlatMapGroupsInPandas</code> is created\u00a0when:</p> <ul> <li><code>RelationalGroupedDataset</code> is requested to flatMapGroupsInPandas (with a PythonUDF)</li> </ul>"},{"location":"sql/FlatMapGroupsInPandasExec/","title":"FlatMapGroupsInPandasExec Physical Operator","text":"<p><code>FlatMapGroupsInPandasExec</code> is a unary physical operator (Spark SQL) to execute a PythonUDF.</p> <p><code>FlatMapGroupsInPandasExec</code> represents a FlatMapGroupsInPandas logical operator at execution time.</p>"},{"location":"sql/FlatMapGroupsInPandasExec/#creating-instance","title":"Creating Instance","text":"<p><code>FlatMapGroupsInPandasExec</code> takes the following to be created:</p> <ul> <li> Grouping Attributes (Spark SQL) <li> Function Expression (Spark SQL) <li> Output Attributes (Spark SQL) <li> Child Physical Operator (Spark SQL) <p><code>FlatMapGroupsInPandasExec</code> is created\u00a0when:</p> <ul> <li><code>BasicOperators</code> (Spark SQL) execution planning strategy is executed (on a logical query plan with FlatMapGroupsInPandas logical operators)</li> </ul>"},{"location":"sql/GroupedData/","title":"GroupedData","text":"<p><code>GroupedData</code> is created by DataFrame.groupBy.</p> <p><code>GroupedData</code> is a Python class with PandasGroupedOpsMixin mixin.</p> <p><code>GroupedData</code> is defined in <code>pyspark.sql.group</code> module.</p> <pre><code>from pyspark.sql.group import GroupedData\n</code></pre>"},{"location":"sql/GroupedData/#creating-instance","title":"Creating Instance","text":"<p><code>GroupedData</code> takes the following to be created:</p> <ul> <li> RelationalGroupedDataset <li> DataFrame <p><code>GroupedData</code> is created\u00a0when:</p> <ul> <li>FIXME</li> </ul>"},{"location":"sql/PandasCogroupedOps/","title":"PandasCogroupedOps","text":"<p><code>PandasCogroupedOps</code> is a logical grouping created by GroupedData.cogroup.</p> <pre><code>from pyspark.sql.pandas.group_ops import PandasCogroupedOps\n</code></pre> <p><code>PandasCogroupedOps</code> is included in <code>__all__</code> of <code>pyspark.sql</code> module (via <code>__init__.py</code>).</p>"},{"location":"sql/PandasCogroupedOps/#creating-instance","title":"Creating Instance","text":"<p><code>PandasCogroupedOps</code> takes the following to be created:</p> <ul> <li> GroupedData <li> GroupedData <p><code>PandasCogroupedOps</code> is created\u00a0when:</p> <ul> <li><code>PandasGroupedOpsMixin</code> is requested to cogroup</li> </ul>"},{"location":"sql/PandasCogroupedOps/#applyinpandas","title":"applyInPandas <pre><code>applyInPandas(self, func, schema)\n</code></pre> <p><code>applyInPandas</code> creates a DataFrame with the result of flatMapCoGroupsInPandas with a pandas user defined function of <code>SQL_COGROUPED_MAP_PANDAS_UDF</code> type.</p>  <p><code>applyInPandas</code> creates a pandas user defined function for the given <code>func</code> and the return type by the given <code>schema</code>. The pandas UDF is of <code>SQL_COGROUPED_MAP_PANDAS_UDF</code> type.</p> <p><code>applyInPandas</code> applies the pandas UDF on all the columns of the two GroupedDatas (that creates a <code>Column</code> expression).</p> <p><code>applyInPandas</code> requests the GroupedData for the associated RelationalGroupedDataset that is in turn requested to flatMapCoGroupsInPandas.</p>","text":""},{"location":"sql/PandasConversionMixin/","title":"PandasConversionMixin","text":"<p><code>PandasConversionMixin</code> is a Python mixin of DataFrame to convert to Pandas (pandas.DataFrame).</p>"},{"location":"sql/PandasConversionMixin/#toPandas","title":"toPandas","text":"<pre><code>toPandas(self)\n</code></pre> <p><code>toPandas</code> can only be used with DataFrame.</p> <p>With Arrow optimization enabled, <code>toPandas</code> to_arrow_schema.</p> <p>pyarrow</p> <p>Arrow Optimization uses <code>pyarrow</code> module.</p> <p><code>toPandas</code> renames the columns to be of <code>col_[index]</code> format and _collect_as_arrow (with <code>split_batches</code> based on <code>arrowPySparkSelfDestructEnabled</code> configuration property).</p> <p><code>toPandas</code> creates a <code>pyarrow.Table</code> (from the <code>RecordBatch</code>es) and converts the table to a pandas-compatible NumPy array or <code>DataFrame</code>. <code>toPandas</code> renames the columns back to the initial column names.</p> <p>Note</p> <p>Column order is assumed.</p> <p>With Arrow optimization disabled, <code>toPandas</code> collects the records (<code>DataFrame.collect</code>) and creates a <code>pandas.DataFrame</code> (with some type munging).</p>"},{"location":"sql/PandasGroupedOpsMixin/","title":"PandasGroupedOpsMixin","text":"<p><code>PandasGroupedOpsMixin</code> is a Python mixin for GroupedData class.</p>"},{"location":"sql/PandasGroupedOpsMixin/#applyinpandas","title":"applyInPandas <pre><code>applyInPandas(self, func, schema)\n</code></pre> <p><code>applyInPandas</code> creates a <code>DataFrame</code> with flatMapGroupsInPandas.</p>","text":""},{"location":"sql/PandasGroupedOpsMixin/#example","title":"Example <pre><code>df1 = spark.createDataFrame(\n    [(20000101, 1, 1.0), (20000101, 2, 2.0), (20000102, 1, 3.0), (20000102, 2, 4.0)],\n    (\"time\", \"id\", \"v1\"))\ndf2 = spark.createDataFrame(\n    [(20000101, 1, \"x\"), (20000101, 2, \"y\")],\n    (\"time\", \"id\", \"v2\"))\n</code></pre> <pre><code>import pandas as pd\ndef asof_join(k, l, r):\n  if k == (1,):\n    return pd.merge_asof(l, r, on=\"time\", by=\"id\")\n  else:\n    return pd.DataFrame(columns=['time', 'id', 'v1', 'v2'])\n</code></pre> <pre><code>gd1 = df1.groupby(\"id\")\ngd2 = df2.groupby(\"id\")\n\ngd1.cogroup(gd2).applyInPandas(\n  asof_join, \"time int, id int, v1 double, v2 string\").show()\n</code></pre>","text":""},{"location":"sql/PandasGroupedOpsMixin/#cogroup","title":"cogroup <pre><code>cogroup(self, other)\n</code></pre> <p><code>cogroup</code> creates a PandasCogroupedOps.</p>","text":""},{"location":"sql/PandasMapOpsMixin/","title":"PandasMapOpsMixin","text":"<p><code>PandasMapOpsMixin</code> is a Python mixin for DataFrame class.</p>"},{"location":"sql/PythonUDF/","title":"PythonUDF","text":"<p><code>PythonUDF</code> is a Catalyst expression (Spark SQL).</p>"},{"location":"sql/PythonUDF/#creating-instance","title":"Creating Instance","text":"<p><code>PythonUDF</code> takes the following to be created:</p> <ul> <li> Name <li> PythonFunction <li> <code>DataType</code> (Spark SQL) <li> Children Catalyst Expressions (Spark SQL) <li> Python Eval Type <li> <code>udfDeterministic</code> flag <li> Result ID (<code>ExprId</code>) <p><code>PythonUDF</code> is created\u00a0when:</p> <ul> <li><code>UserDefinedPythonFunction</code> is requested to builder</li> </ul>"},{"location":"sql/PythonUDF/#unevaluable","title":"Unevaluable","text":"<p><code>PythonUDF</code> is an <code>Unevaluable</code> expression (Spark SQL).</p>"},{"location":"sql/PythonUDF/#nonsqlexpression","title":"NonSQLExpression","text":"<p><code>PythonUDF</code> is a <code>NonSQLExpression</code> expression (Spark SQL).</p>"},{"location":"sql/PythonUDF/#userdefinedexpression","title":"UserDefinedExpression","text":"<p><code>PythonUDF</code> is a <code>UserDefinedExpression</code> expression (Spark SQL).</p>"},{"location":"sql/RelationalGroupedDataset/","title":"RelationalGroupedDataset","text":"<p><code>RelationalGroupedDataset</code> is a result of executing high-level grouping operators.</p> <p>This is a stub</p> <p>This page is a stub to describe PySpark-related methods only. Learn more about RelationalGroupedDataset in The Internals of Spark SQL.</p>"},{"location":"sql/RelationalGroupedDataset/#flatmapcogroupsinpandas","title":"flatMapCoGroupsInPandas <pre><code>flatMapCoGroupsInPandas(\n  r: RelationalGroupedDataset,\n  expr: PythonUDF): DataFrame\n</code></pre> <p><code>flatMapCoGroupsInPandas</code>...FIXME</p> <p><code>flatMapCoGroupsInPandas</code>\u00a0is used when:</p> <ul> <li><code>PandasCogroupedOps</code> is requested to applyInPandas</li> </ul>","text":""},{"location":"sql/RelationalGroupedDataset/#flatmapgroupsinpandas","title":"flatMapGroupsInPandas <pre><code>flatMapGroupsInPandas(\n  expr: PythonUDF): DataFrame\n</code></pre> <p><code>flatMapGroupsInPandas</code> creates a <code>DataFrame</code> with a FlatMapGroupsInPandas logical operator.</p> <p><code>flatMapGroupsInPandas</code> asserts that the input PythonUDF is a grouped map udf (the evalType is <code>PythonEvalType.SQL_GROUPED_MAP_PANDAS_UDF</code>).</p> <p><code>flatMapGroupsInPandas</code> asserts that the return type of the input PythonUDF is <code>StructType</code>.</p> <p><code>flatMapGroupsInPandas</code>\u00a0is used when:</p> <ul> <li><code>PandasGroupedOpsMixin</code> is requested to applyInPandas</li> </ul>","text":""},{"location":"sql/SQLContext/","title":"SQLContext","text":"<p><code>SQLContext</code> is...FIXME</p>"},{"location":"sql/SparkConversionMixin/","title":"SparkConversionMixin","text":"<p><code>SparkConversionMixin</code> is a Python mixin for SparkSession class.</p>"},{"location":"sql/SparkSession/","title":"SparkSession","text":""},{"location":"sql/SparkSession/#_create_shell_session","title":"_create_shell_session <pre><code>_create_shell_session()\n</code></pre> <p><code>_create_shell_session</code> is a <code>@staticmethod</code>.</p> <p><code>_create_shell_session</code>...FIXME</p> <p><code>_create_shell_session</code>\u00a0is used when:</p> <ul> <li>FIXME</li> </ul>","text":""},{"location":"sql/SparkSession/#sql","title":"sql <pre><code>sql(\n  self, sqlQuery)\n</code></pre> <p><code>sql</code> creates a <code>DataFrame</code> with the <code>sqlQuery</code> query executed.</p>","text":""},{"location":"sql/UDFRegistration/","title":"UDFRegistration","text":"<p><code>UDFRegistration</code> is available as <code>spark.udf</code>.</p>"},{"location":"sql/UDFRegistration/#register","title":"register","text":"<pre><code>register(\n  self,\n  name: str,\n  f: Union[Callable[..., Any], \"UserDefinedFunctionLike\"],\n  returnType: Optional[Union[pyspark.sql.types.DataType, str]] = None,\n) -&gt; \"UserDefinedFunctionLike\"\n</code></pre> <p><code>register</code> registers a Python function (incl. lambda function) or a user-defined function as a SQL function (under the given <code>name</code>).</p> Function <code>f</code> Description A Python function <ul><li>Includes lambda (unnamed) functions<li><code>Callable[..., Any]</code><li>The return type is <code>StringType</code> when not specified<li>Always <code>PythonEvalType.SQL_BATCHED_UDF</code> <code>pyspark.sql.functions.udf</code> <ul><li>row-at-a-time<li><code>UserDefinedFunctionLike</code> <code>pyspark.sql.functions.pandas_udf</code> <ul><li>vectorized<li><code>UserDefinedFunctionLike</code> <p><code>evalType</code> of the a user-defined function can be one of the following:</p> <ul> <li><code>PythonEvalType.SQL_BATCHED_UDF</code></li> <li><code>PythonEvalType.SQL_SCALAR_PANDAS_UDF</code></li> <li><code>PythonEvalType.SQL_SCALAR_PANDAS_ITER_UDF</code></li> <li><code>PythonEvalType.SQL_GROUPED_AGG_PANDAS_UDF</code></li> </ul> <p><code>register</code> _create_udf and requests the <code>_jsparkSession</code> for the <code>UDFRegistration</code> (Spark SQL) to <code>registerPython</code> (Spark SQL).</p> <pre><code>from pyspark.sql.functions import call_udf, col\nfrom pyspark.sql.types import IntegerType, StringType\n\nrows = [(1, \"a\"),(2, \"b\"), (3, \"c\")]\ncolumns = [\"id\", \"name\"]\ndf = spark.createDataFrame(rows, columns)\n\nspark.udf.register(\"intX2\", lambda i: i * 2, IntegerType())\ndf.select(call_udf(\"intX2\", \"id\")).show()\n</code></pre>"},{"location":"sql/UserDefinedFunction/","title":"UserDefinedFunction","text":"<p><code>UserDefinedFunction</code> is...FIXME</p>"},{"location":"sql/UserDefinedPythonFunction/","title":"UserDefinedPythonFunction","text":""},{"location":"sql/UserDefinedPythonFunction/#creating-instance","title":"Creating Instance","text":"<p><code>UserDefinedPythonFunction</code> takes the following to be created:</p> <ul> <li> Name <li> <code>PythonFunction</code> <li> <code>DataType</code> (Spark SQL) <li> Python Eval Type <li> <code>udfDeterministic</code> flag"},{"location":"sql/UserDefinedPythonFunction/#creating-pythonudf","title":"Creating PythonUDF <pre><code>builder(\n  e: Seq[Expression]): Expression\n</code></pre> <p><code>builder</code> creates a PythonUDF for the arguments and the given children expressions.</p> <p><code>builder</code>\u00a0is used when:</p> <ul> <li><code>UDFRegistration</code> is requested to register a Python UDF (Spark SQL)</li> <li><code>UserDefinedPythonFunction</code> is requested to apply</li> </ul>","text":""},{"location":"sql/UserDefinedPythonFunction/#applying-pythonudf","title":"Applying PythonUDF <pre><code>apply(\n  exprs: Column*): Column\n</code></pre> <p><code>apply</code> creates a PythonUDF with the input <code>Column</code> (Spark SQL) expressions and creates a new <code>Column</code>.</p> <p><code>apply</code>\u00a0is used when:</p> <ul> <li><code>UDFRegistration</code> is requested to register a Python UDF (Spark SQL)</li> <li><code>UserDefinedPythonFunction</code> is requested to apply</li> </ul>","text":""}]}