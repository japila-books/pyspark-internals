{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"The Internals of PySpark (Apache Spark 3.1.1-rc2) \u00b6 Welcome to The Internals of PySpark online book! \ud83e\udd19 I'm Jacek Laskowski , an IT freelancer specializing in Apache Spark , Delta Lake and Apache Kafka (with brief forays into a wider data engineering space, e.g. Trino and ksqlDB , mostly during Warsaw Data Engineering meetups). I'm very excited to have you here and hope you will enjoy exploring the internals of Spark SQL as much as I have. Flannery O'Connor I write to discover what I know. \"The Internals Of\" series I'm also writing other online books in the \"The Internals Of\" series. Please visit \"The Internals Of\" Online Books home page. Expect text and code snippets from a variety of public sources. Attribution follows. Now, let me introduce you to PySpark \ud83d\udd25","title":"Welcome"},{"location":"#the-internals-of-pyspark-apache-spark-311-rc2","text":"Welcome to The Internals of PySpark online book! \ud83e\udd19 I'm Jacek Laskowski , an IT freelancer specializing in Apache Spark , Delta Lake and Apache Kafka (with brief forays into a wider data engineering space, e.g. Trino and ksqlDB , mostly during Warsaw Data Engineering meetups). I'm very excited to have you here and hope you will enjoy exploring the internals of Spark SQL as much as I have. Flannery O'Connor I write to discover what I know. \"The Internals Of\" series I'm also writing other online books in the \"The Internals Of\" series. Please visit \"The Internals Of\" Online Books home page. Expect text and code snippets from a variety of public sources. Attribution follows. Now, let me introduce you to PySpark \ud83d\udd25","title":"The Internals of PySpark (Apache Spark 3.1.1-rc2)"},{"location":"AggregateInPandasExec/","text":"AggregateInPandasExec Physical Operator \u00b6 AggregateInPandasExec is a unary physical operator ( Spark SQL ). Creating Instance \u00b6 AggregateInPandasExec takes the following to be created: Grouping Expressions ( Spark SQL ) ( Seq[NamedExpression] ) PythonUDF s Result Named Expressions ( Spark SQL ) ( Seq[NamedExpression] ) Child Physical Operator ( Spark SQL ) AggregateInPandasExec is created when Aggregation execution planning strategy ( Spark SQL ) is executed for Aggregate logical operators ( Spark SQL ) with PythonUDF aggregate expressions only. Executing Operator \u00b6 doExecute () : RDD [ InternalRow ] doExecute uses ArrowPythonRunner (one per partition) to execute PythonUDFs . doExecute is part of the SparkPlan ( Spark SQL ) abstraction.","title":"AggregateInPandasExec"},{"location":"AggregateInPandasExec/#aggregateinpandasexec-physical-operator","text":"AggregateInPandasExec is a unary physical operator ( Spark SQL ).","title":"AggregateInPandasExec Physical Operator"},{"location":"AggregateInPandasExec/#creating-instance","text":"AggregateInPandasExec takes the following to be created: Grouping Expressions ( Spark SQL ) ( Seq[NamedExpression] ) PythonUDF s Result Named Expressions ( Spark SQL ) ( Seq[NamedExpression] ) Child Physical Operator ( Spark SQL ) AggregateInPandasExec is created when Aggregation execution planning strategy ( Spark SQL ) is executed for Aggregate logical operators ( Spark SQL ) with PythonUDF aggregate expressions only.","title":"Creating Instance"},{"location":"AggregateInPandasExec/#executing-operator","text":"doExecute () : RDD [ InternalRow ] doExecute uses ArrowPythonRunner (one per partition) to execute PythonUDFs . doExecute is part of the SparkPlan ( Spark SQL ) abstraction.","title":" Executing Operator"},{"location":"ArrowPythonRunner/","text":"ArrowPythonRunner \u00b6 ArrowPythonRunner is a pyspark runner with PythonArrowOutput for...FIXME Creating Instance \u00b6 ArrowPythonRunner takes the following to be created: Seq[ChainedPythonFunctions] Eval Type Argument Offsets ( Array[Array[Int]] ) Schema ( Spark SQL ) TimeZone ID Configuration ( Map[String, String] ) ArrowPythonRunner is created when AggregateInPandasExec , ArrowEvalPythonExec , FlatMapGroupsInPandasExec , MapInPandasExec , WindowInPandasExec physical operators are executed.","title":"ArrowPythonRunner"},{"location":"ArrowPythonRunner/#arrowpythonrunner","text":"ArrowPythonRunner is a pyspark runner with PythonArrowOutput for...FIXME","title":"ArrowPythonRunner"},{"location":"ArrowPythonRunner/#creating-instance","text":"ArrowPythonRunner takes the following to be created: Seq[ChainedPythonFunctions] Eval Type Argument Offsets ( Array[Array[Int]] ) Schema ( Spark SQL ) TimeZone ID Configuration ( Map[String, String] ) ArrowPythonRunner is created when AggregateInPandasExec , ArrowEvalPythonExec , FlatMapGroupsInPandasExec , MapInPandasExec , WindowInPandasExec physical operators are executed.","title":"Creating Instance"},{"location":"BasePythonRunner/","text":"BasePythonRunner \u00b6 BasePythonRunner is an abstraction of Python Runners . Contract \u00b6 newReaderIterator \u00b6 newReaderIterator ( stream : DataInputStream , writerThread : WriterThread , startTime : Long , env : SparkEnv , worker : Socket , releasedOrClosed : AtomicBoolean , context : TaskContext ) : Iterator [ OUT ] Used when BasePythonRunner is requested to compute newWriterThread \u00b6 newWriterThread ( env : SparkEnv , worker : Socket , inputIterator : Iterator [ IN ], partitionIndex : Int , context : TaskContext ) : WriterThread Used when BasePythonRunner is requested to compute Implementations \u00b6 ArrowPythonRunner CoGroupedArrowPythonRunner PythonRunner PythonUDFRunner Scala Definition \u00b6 BasePythonRunner is a type constructor in Scala ( generic class in Java) with the following definition: abstract class BasePythonRunner [ IN , OUT ](...) { // ... } BasePythonRunner uses IN and OUT as the name of the types for the input and output values. Creating Instance \u00b6 BasePythonRunner takes the following to be created: ChainedPythonFunctions Eval Type Argument Offsets BasePythonRunner requires that the number of ChainedPythonFunctions and Argument Offsets are the same. Abstract Class BasePythonRunner is an abstract class and cannot be created directly. It is created indirectly for the concrete BasePythonRunners . Computing Result \u00b6 compute ( inputIterator : Iterator [ IN ], partitionIndex : Int , context : TaskContext ) : Iterator [ OUT ] compute makes sure that spark.executorEnv.OMP_NUM_THREADS configuration option is set or defaults to spark.executor.cores property. compute defines the following environment variables: SPARK_LOCAL_DIRS to be the local directories of the local DiskBlockManager SPARK_BUFFER_SIZE to be the value of spark.buffer.size configuration property (default: 65536 ) compute can optionally define environment variables: SPARK_REUSE_WORKER to be 1 based on spark.python.worker.reuse configuration property (default: true ) PYSPARK_EXECUTOR_MEMORY_MB to be the value of spark.executor.pyspark.memory configuration property if defined compute requests the executor's SparkEnv to createPythonWorker (for a pythonExec and the environment variables) that requests PythonWorkerFactory to create a Python worker (and give a java.net.Socket ). FIXME Describe pythonExec . compute [newWriterThread] with the Python worker and the input arguments. compute creates and starts a MonitorThread to watch the Python worker. compute creates a new reader iterator to read lines from the Python worker's stdout. compute is used when: PythonRDD is requested to compute AggregateInPandasExec , ArrowEvalPythonExec , BatchEvalPythonExec , FlatMapCoGroupsInPandasExec , FlatMapGroupsInPandasExec MapInPandasExec , WindowInPandasExec physical operators are executed","title":"BasePythonRunner"},{"location":"BasePythonRunner/#basepythonrunner","text":"BasePythonRunner is an abstraction of Python Runners .","title":"BasePythonRunner"},{"location":"BasePythonRunner/#contract","text":"","title":"Contract"},{"location":"BasePythonRunner/#newreaderiterator","text":"newReaderIterator ( stream : DataInputStream , writerThread : WriterThread , startTime : Long , env : SparkEnv , worker : Socket , releasedOrClosed : AtomicBoolean , context : TaskContext ) : Iterator [ OUT ] Used when BasePythonRunner is requested to compute","title":" newReaderIterator"},{"location":"BasePythonRunner/#newwriterthread","text":"newWriterThread ( env : SparkEnv , worker : Socket , inputIterator : Iterator [ IN ], partitionIndex : Int , context : TaskContext ) : WriterThread Used when BasePythonRunner is requested to compute","title":" newWriterThread"},{"location":"BasePythonRunner/#implementations","text":"ArrowPythonRunner CoGroupedArrowPythonRunner PythonRunner PythonUDFRunner","title":"Implementations"},{"location":"BasePythonRunner/#scala-definition","text":"BasePythonRunner is a type constructor in Scala ( generic class in Java) with the following definition: abstract class BasePythonRunner [ IN , OUT ](...) { // ... } BasePythonRunner uses IN and OUT as the name of the types for the input and output values.","title":"Scala Definition"},{"location":"BasePythonRunner/#creating-instance","text":"BasePythonRunner takes the following to be created: ChainedPythonFunctions Eval Type Argument Offsets BasePythonRunner requires that the number of ChainedPythonFunctions and Argument Offsets are the same. Abstract Class BasePythonRunner is an abstract class and cannot be created directly. It is created indirectly for the concrete BasePythonRunners .","title":"Creating Instance"},{"location":"BasePythonRunner/#computing-result","text":"compute ( inputIterator : Iterator [ IN ], partitionIndex : Int , context : TaskContext ) : Iterator [ OUT ] compute makes sure that spark.executorEnv.OMP_NUM_THREADS configuration option is set or defaults to spark.executor.cores property. compute defines the following environment variables: SPARK_LOCAL_DIRS to be the local directories of the local DiskBlockManager SPARK_BUFFER_SIZE to be the value of spark.buffer.size configuration property (default: 65536 ) compute can optionally define environment variables: SPARK_REUSE_WORKER to be 1 based on spark.python.worker.reuse configuration property (default: true ) PYSPARK_EXECUTOR_MEMORY_MB to be the value of spark.executor.pyspark.memory configuration property if defined compute requests the executor's SparkEnv to createPythonWorker (for a pythonExec and the environment variables) that requests PythonWorkerFactory to create a Python worker (and give a java.net.Socket ). FIXME Describe pythonExec . compute [newWriterThread] with the Python worker and the input arguments. compute creates and starts a MonitorThread to watch the Python worker. compute creates a new reader iterator to read lines from the Python worker's stdout. compute is used when: PythonRDD is requested to compute AggregateInPandasExec , ArrowEvalPythonExec , BatchEvalPythonExec , FlatMapCoGroupsInPandasExec , FlatMapGroupsInPandasExec MapInPandasExec , WindowInPandasExec physical operators are executed","title":" Computing Result"},{"location":"MonitorThread/","text":"MonitorThread \u00b6 MonitorThread is...FIXME","title":"MonitorThread"},{"location":"MonitorThread/#monitorthread","text":"MonitorThread is...FIXME","title":"MonitorThread"},{"location":"PandasConversionMixin/","text":"PandasConversionMixin \u00b6 PandasConversionMixin is a Python mixin for DataFrame class. toPandas \u00b6 toPandas ( self ) toPandas can only be used with DataFrame (from pyspark.sql.dataframe ). toPandas ...FIXME","title":"PandasConversionMixin"},{"location":"PandasConversionMixin/#pandasconversionmixin","text":"PandasConversionMixin is a Python mixin for DataFrame class.","title":"PandasConversionMixin"},{"location":"PandasConversionMixin/#topandas","text":"toPandas ( self ) toPandas can only be used with DataFrame (from pyspark.sql.dataframe ). toPandas ...FIXME","title":" toPandas"},{"location":"PandasMapOpsMixin/","text":"PandasMapOpsMixin \u00b6 PandasMapOpsMixin is a Python mixin for DataFrame class.","title":"PandasMapOpsMixin"},{"location":"PandasMapOpsMixin/#pandasmapopsmixin","text":"PandasMapOpsMixin is a Python mixin for DataFrame class.","title":"PandasMapOpsMixin"},{"location":"Py4JServer/","text":"Py4JServer \u00b6 Connection Secret \u00b6 secret : String Py4JServer creates a connection secret to establish a secured communication to...FIXME start \u00b6 start () : Unit start ...FIXME start is used when: PythonGatewayServer is launched PythonRunner is launched getListeningPort \u00b6 getListeningPort : Int getListeningPort ...FIXME getListeningPort is used when: PythonGatewayServer is launched PythonRunner is launched","title":"Py4JServer"},{"location":"Py4JServer/#py4jserver","text":"","title":"Py4JServer"},{"location":"Py4JServer/#connection-secret","text":"secret : String Py4JServer creates a connection secret to establish a secured communication to...FIXME","title":" Connection Secret"},{"location":"Py4JServer/#start","text":"start () : Unit start ...FIXME start is used when: PythonGatewayServer is launched PythonRunner is launched","title":" start"},{"location":"Py4JServer/#getlisteningport","text":"getListeningPort : Int getListeningPort ...FIXME getListeningPort is used when: PythonGatewayServer is launched PythonRunner is launched","title":" getListeningPort"},{"location":"PythonAccumulatorV2/","text":"PythonAccumulatorV2 \u00b6 PythonAccumulatorV2 is...FIXME","title":"PythonAccumulatorV2"},{"location":"PythonAccumulatorV2/#pythonaccumulatorv2","text":"PythonAccumulatorV2 is...FIXME","title":"PythonAccumulatorV2"},{"location":"PythonBroadcast/","text":"PythonBroadcast \u00b6 PythonBroadcast is...FIXME","title":"PythonBroadcast"},{"location":"PythonBroadcast/#pythonbroadcast","text":"PythonBroadcast is...FIXME","title":"PythonBroadcast"},{"location":"PythonForeachWriter/","text":"PythonForeachWriter \u00b6 PythonForeachWriter is...FIXME","title":"PythonForeachWriter"},{"location":"PythonForeachWriter/#pythonforeachwriter","text":"PythonForeachWriter is...FIXME","title":"PythonForeachWriter"},{"location":"PythonFunction/","text":"PythonFunction \u00b6 PythonFunction is a metadata of a Python function to be executed in PythonRunner . Creating Instance \u00b6 PythonFunction takes the following to be created: Command ( Array[Byte] ) Environment Variables ( Map[String, String] ) Python Includes ( List[String] ) Python Executable Python Version Broadcast Variables with PythonBroadcast ]s PythonAccumulatorV2 PythonFunction is created when...FIXME","title":"PythonFunction"},{"location":"PythonFunction/#pythonfunction","text":"PythonFunction is a metadata of a Python function to be executed in PythonRunner .","title":"PythonFunction"},{"location":"PythonFunction/#creating-instance","text":"PythonFunction takes the following to be created: Command ( Array[Byte] ) Environment Variables ( Map[String, String] ) Python Includes ( List[String] ) Python Executable Python Version Broadcast Variables with PythonBroadcast ]s PythonAccumulatorV2 PythonFunction is created when...FIXME","title":"Creating Instance"},{"location":"PythonGatewayServer/","text":"PythonGatewayServer \u00b6 PythonGatewayServer is a standalone application ( process ) that starts a Py4JServer on an ephemeral port. PythonGatewayServer is the Python runner for pyspark shell ( Apache Spark ). main \u00b6 main ( args : Array [ String ]) : Unit main creates a Py4JServer and requests it to start . main requests the Py4JServer for the listening port ( boundPort ) and prints out the following DEBUG message to the logs: Started PythonGatewayServer on port [boundPort] main uses _PYSPARK_DRIVER_CONN_INFO_PATH environment variable for the path of a connection info file (for the associated python process) with the listening port and the secret . main pauses ( blocks ) until the Python driver finishes (by reading from the system input that blocks until input data is available, the end of the stream is detected, or an exception is thrown). In the end, once the Python driver finishes, main prints out the following DEBUG message to the logs: Exiting due to broken pipe from Python driver main prints out the following ERROR message to the logs and exists when the listening port is -1 : [server] failed to bind; exiting _PYSPARK_DRIVER_CONN_INFO_PATH \u00b6 PythonGatewayServer uses _PYSPARK_DRIVER_CONN_INFO_PATH environment variable for the path of a connection info file for communication between this and the Python processes. _PYSPARK_DRIVER_CONN_INFO_PATH is configured when java_gateway.py module is requested to launch_gateway . Logging \u00b6 Enable ALL logging level for org.apache.spark.api.python.PythonGatewayServer logger to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.api.python.PythonGatewayServer=ALL Refer to Logging .","title":"PythonGatewayServer"},{"location":"PythonGatewayServer/#pythongatewayserver","text":"PythonGatewayServer is a standalone application ( process ) that starts a Py4JServer on an ephemeral port. PythonGatewayServer is the Python runner for pyspark shell ( Apache Spark ).","title":"PythonGatewayServer"},{"location":"PythonGatewayServer/#main","text":"main ( args : Array [ String ]) : Unit main creates a Py4JServer and requests it to start . main requests the Py4JServer for the listening port ( boundPort ) and prints out the following DEBUG message to the logs: Started PythonGatewayServer on port [boundPort] main uses _PYSPARK_DRIVER_CONN_INFO_PATH environment variable for the path of a connection info file (for the associated python process) with the listening port and the secret . main pauses ( blocks ) until the Python driver finishes (by reading from the system input that blocks until input data is available, the end of the stream is detected, or an exception is thrown). In the end, once the Python driver finishes, main prints out the following DEBUG message to the logs: Exiting due to broken pipe from Python driver main prints out the following ERROR message to the logs and exists when the listening port is -1 : [server] failed to bind; exiting","title":" main"},{"location":"PythonGatewayServer/#_pyspark_driver_conn_info_path","text":"PythonGatewayServer uses _PYSPARK_DRIVER_CONN_INFO_PATH environment variable for the path of a connection info file for communication between this and the Python processes. _PYSPARK_DRIVER_CONN_INFO_PATH is configured when java_gateway.py module is requested to launch_gateway .","title":" _PYSPARK_DRIVER_CONN_INFO_PATH"},{"location":"PythonGatewayServer/#logging","text":"Enable ALL logging level for org.apache.spark.api.python.PythonGatewayServer logger to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.api.python.PythonGatewayServer=ALL Refer to Logging .","title":"Logging"},{"location":"PythonRDD/","text":"PythonRDD \u00b6 PythonRDD is an RDD ( RDD[Array[Byte]] ) that uses PythonRunner (to compute a partition ). Creating Instance \u00b6 PythonRDD takes the following to be created: Parent RDD PythonFunction preservePartitoning flag isFromBarrier flag (default: false ) PythonRDD is created when...FIXME runJob \u00b6 runJob ( sc : SparkContext , rdd : JavaRDD [ Array [ Byte ]], partitions : JArrayList [ Int ]) : Array [ Any ] runJob ...FIXME collectAndServe \u00b6 collectAndServe [ T ]( rdd : RDD [ T ]) : Array [ Any ] collectAndServe ...FIXME collectAndServeWithJobGroup \u00b6 collectAndServeWithJobGroup [ T ]( rdd : RDD [ T ], groupId : String , description : String , interruptOnCancel : Boolean ) : Array [ Any ] collectAndServeWithJobGroup ...FIXME serveIterator Utility \u00b6 serveIterator ( items : Iterator [ _ ], threadName : String ) : Array [ Any ] serveIterator serveToStream with a writer function that...FIXME serveIterator is used when: PythonRDD utility is used to runJob , collectAndServe and collectAndServeWithJobGroup Dataset is requested to collectToPython , tailToPython , getRowsToPython serveToStream Utility \u00b6 serveToStream ( threadName : String )( writeFunc : OutputStream => Unit ) : Array [ Any ] serveToStream serveToStream with the authHelper and the input arguments. serveToStream is used when: PythonRDD utility is used to serveIterator Dataset is requested to collectAsArrowToPython SocketAuthHelper \u00b6 PythonRDD uses a SocketAuthHelper .","title":"PythonRDD"},{"location":"PythonRDD/#pythonrdd","text":"PythonRDD is an RDD ( RDD[Array[Byte]] ) that uses PythonRunner (to compute a partition ).","title":"PythonRDD"},{"location":"PythonRDD/#creating-instance","text":"PythonRDD takes the following to be created: Parent RDD PythonFunction preservePartitoning flag isFromBarrier flag (default: false ) PythonRDD is created when...FIXME","title":"Creating Instance"},{"location":"PythonRDD/#runjob","text":"runJob ( sc : SparkContext , rdd : JavaRDD [ Array [ Byte ]], partitions : JArrayList [ Int ]) : Array [ Any ] runJob ...FIXME","title":" runJob"},{"location":"PythonRDD/#collectandserve","text":"collectAndServe [ T ]( rdd : RDD [ T ]) : Array [ Any ] collectAndServe ...FIXME","title":" collectAndServe"},{"location":"PythonRDD/#collectandservewithjobgroup","text":"collectAndServeWithJobGroup [ T ]( rdd : RDD [ T ], groupId : String , description : String , interruptOnCancel : Boolean ) : Array [ Any ] collectAndServeWithJobGroup ...FIXME","title":" collectAndServeWithJobGroup"},{"location":"PythonRDD/#serveiterator-utility","text":"serveIterator ( items : Iterator [ _ ], threadName : String ) : Array [ Any ] serveIterator serveToStream with a writer function that...FIXME serveIterator is used when: PythonRDD utility is used to runJob , collectAndServe and collectAndServeWithJobGroup Dataset is requested to collectToPython , tailToPython , getRowsToPython","title":" serveIterator Utility"},{"location":"PythonRDD/#servetostream-utility","text":"serveToStream ( threadName : String )( writeFunc : OutputStream => Unit ) : Array [ Any ] serveToStream serveToStream with the authHelper and the input arguments. serveToStream is used when: PythonRDD utility is used to serveIterator Dataset is requested to collectAsArrowToPython","title":" serveToStream Utility"},{"location":"PythonRDD/#socketauthhelper","text":"PythonRDD uses a SocketAuthHelper .","title":" SocketAuthHelper"},{"location":"PythonRunner/","text":"PythonRunner \u00b6 PythonRunner is a concrete BasePythonRunner . Creating Instance \u00b6 PythonRunner takes the following to be created: Seq[ChainedPythonFunctions] PythonRunner is created (indirectly using apply factory method) when: PythonRDD is requested to compute a partition PythonForeachWriter is requested for a PythonRunner Creating PythonRunner \u00b6 apply ( func : PythonFunction ) : PythonRunner apply simply creates a PythonRunner for the PythonFunction . apply is used when: PythonRDD is requested to compute a partition PythonForeachWriter is requested for a PythonRunner","title":"PythonRunner"},{"location":"PythonRunner/#pythonrunner","text":"PythonRunner is a concrete BasePythonRunner .","title":"PythonRunner"},{"location":"PythonRunner/#creating-instance","text":"PythonRunner takes the following to be created: Seq[ChainedPythonFunctions] PythonRunner is created (indirectly using apply factory method) when: PythonRDD is requested to compute a partition PythonForeachWriter is requested for a PythonRunner","title":"Creating Instance"},{"location":"PythonRunner/#creating-pythonrunner","text":"apply ( func : PythonFunction ) : PythonRunner apply simply creates a PythonRunner for the PythonFunction . apply is used when: PythonRDD is requested to compute a partition PythonForeachWriter is requested for a PythonRunner","title":" Creating PythonRunner"},{"location":"PythonUDF/","text":"PythonUDF \u00b6 PythonUDF is a Catalyst expression ( Spark SQL ). Creating Instance \u00b6 PythonUDF takes the following to be created: Name PythonFunction DataType ( Spark SQL ) Children Catalyst Expressions ( Spark SQL ) Python Eval Type udfDeterministic flag Result ID ( ExprId ) PythonUDF is created when: UserDefinedPythonFunction is requested to builder Unevaluable \u00b6 PythonUDF is an Unevaluable expression ( Spark SQL ). NonSQLExpression \u00b6 PythonUDF is a NonSQLExpression expression ( Spark SQL ). UserDefinedExpression \u00b6 PythonUDF is a UserDefinedExpression expression ( Spark SQL ).","title":"PythonUDF"},{"location":"PythonUDF/#pythonudf","text":"PythonUDF is a Catalyst expression ( Spark SQL ).","title":"PythonUDF"},{"location":"PythonUDF/#creating-instance","text":"PythonUDF takes the following to be created: Name PythonFunction DataType ( Spark SQL ) Children Catalyst Expressions ( Spark SQL ) Python Eval Type udfDeterministic flag Result ID ( ExprId ) PythonUDF is created when: UserDefinedPythonFunction is requested to builder","title":"Creating Instance"},{"location":"PythonUDF/#unevaluable","text":"PythonUDF is an Unevaluable expression ( Spark SQL ).","title":"Unevaluable"},{"location":"PythonUDF/#nonsqlexpression","text":"PythonUDF is a NonSQLExpression expression ( Spark SQL ).","title":"NonSQLExpression"},{"location":"PythonUDF/#userdefinedexpression","text":"PythonUDF is a UserDefinedExpression expression ( Spark SQL ).","title":"UserDefinedExpression"},{"location":"PythonUDFRunner/","text":"PythonUDFRunner \u00b6 PythonUDFRunner is...FIXME","title":"PythonUDFRunner"},{"location":"PythonUDFRunner/#pythonudfrunner","text":"PythonUDFRunner is...FIXME","title":"PythonUDFRunner"},{"location":"PythonWorkerFactory/","text":"PythonWorkerFactory \u00b6 Creating Instance \u00b6 PythonWorkerFactory takes the following to be created: Python Executable Environment Variables ( Map[String, String] ) PythonWorkerFactory is created when SparkEnv is requested to createPythonWorker (when BasePythonRunner is requested to compute a partition ). useDaemon Flag \u00b6 PythonWorkerFactory uses useDaemon internal flag that is the value of spark.python.use.daemon configuration property to decide whether to use lighter daemon or non-daemon workers. useDaemon flag is used when PythonWorkerFactory requested to create , stop or release a worker and stop a daemon module . Python Daemon Module \u00b6 PythonWorkerFactory uses spark.python.daemon.module configuration property to define the Python Daemon Module . The Python Daemon Module is used when PythonWorkerFactory is requested to create and start a daemon module . Python Worker Module \u00b6 PythonWorkerFactory uses spark.python.worker.module configuration property to specify the Python Worker Module . The Python Worker Module is used when PythonWorkerFactory is requested to create and start a worker . Creating Python Worker \u00b6 create () : Socket create ...FIXME create is used when SparkEnv is requested to createPythonWorker . Creating Daemon Worker \u00b6 createThroughDaemon () : Socket createThroughDaemon ...FIXME createThroughDaemon is used when PythonWorkerFactory is requested to create a Python worker (with useDaemon flag enabled). Starting Python Daemon Process \u00b6 startDaemon () : Unit startDaemon ...FIXME Creating Simple Non-Daemon Worker \u00b6 createSimpleWorker () : Socket createSimpleWorker ...FIXME createSimpleWorker is used when PythonWorkerFactory is requested to create a Python worker (with useDaemon flag disabled). Logging \u00b6 Enable ALL logging level for org.apache.spark.api.python.PythonWorkerFactory logger to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.api.python.PythonWorkerFactory=ALL Refer to Logging .","title":"PythonWorkerFactory"},{"location":"PythonWorkerFactory/#pythonworkerfactory","text":"","title":"PythonWorkerFactory"},{"location":"PythonWorkerFactory/#creating-instance","text":"PythonWorkerFactory takes the following to be created: Python Executable Environment Variables ( Map[String, String] ) PythonWorkerFactory is created when SparkEnv is requested to createPythonWorker (when BasePythonRunner is requested to compute a partition ).","title":"Creating Instance"},{"location":"PythonWorkerFactory/#usedaemon-flag","text":"PythonWorkerFactory uses useDaemon internal flag that is the value of spark.python.use.daemon configuration property to decide whether to use lighter daemon or non-daemon workers. useDaemon flag is used when PythonWorkerFactory requested to create , stop or release a worker and stop a daemon module .","title":" useDaemon Flag"},{"location":"PythonWorkerFactory/#python-daemon-module","text":"PythonWorkerFactory uses spark.python.daemon.module configuration property to define the Python Daemon Module . The Python Daemon Module is used when PythonWorkerFactory is requested to create and start a daemon module .","title":" Python Daemon Module"},{"location":"PythonWorkerFactory/#python-worker-module","text":"PythonWorkerFactory uses spark.python.worker.module configuration property to specify the Python Worker Module . The Python Worker Module is used when PythonWorkerFactory is requested to create and start a worker .","title":" Python Worker Module"},{"location":"PythonWorkerFactory/#creating-python-worker","text":"create () : Socket create ...FIXME create is used when SparkEnv is requested to createPythonWorker .","title":" Creating Python Worker"},{"location":"PythonWorkerFactory/#creating-daemon-worker","text":"createThroughDaemon () : Socket createThroughDaemon ...FIXME createThroughDaemon is used when PythonWorkerFactory is requested to create a Python worker (with useDaemon flag enabled).","title":" Creating Daemon Worker"},{"location":"PythonWorkerFactory/#starting-python-daemon-process","text":"startDaemon () : Unit startDaemon ...FIXME","title":" Starting Python Daemon Process"},{"location":"PythonWorkerFactory/#creating-simple-non-daemon-worker","text":"createSimpleWorker () : Socket createSimpleWorker ...FIXME createSimpleWorker is used when PythonWorkerFactory is requested to create a Python worker (with useDaemon flag disabled).","title":" Creating Simple Non-Daemon Worker"},{"location":"PythonWorkerFactory/#logging","text":"Enable ALL logging level for org.apache.spark.api.python.PythonWorkerFactory logger to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.api.python.PythonWorkerFactory=ALL Refer to Logging .","title":"Logging"},{"location":"Setup/","text":"PySpark Setup \u00b6 Install IPython \u00b6 Follow the steps as described in the official documentation of IPython. pip install ipython Start PySpark \u00b6 export PYSPARK_DRIVER_PYTHON = ipython ./bin/pyspark Python 3.9.1 (default, Feb 3 2021, 07:38:02) Type 'copyright', 'credits' or 'license' for more information IPython 7.20.0 -- An enhanced Interactive Python. Type '?' for help. Setting default log level to \"WARN\". To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel). Welcome to ____ __ / __/__ ___ _____/ /__ _\\ \\/ _ \\/ _ `/ __/ '_/ /__ / .__/\\_,_/_/ /_/\\_\\ version 3.1.1 /_/ Using Python version 3.9.1 (default, Feb 3 2021 07:38:02) Spark context Web UI available at http://192.168.68.101:4040 Spark context available as 'sc' (master = local[*], app id = local-1613571272142). SparkSession available as 'spark'. In [1]: In [1]: spark.version Out[1]: '3.1.1'","title":"Setup"},{"location":"Setup/#pyspark-setup","text":"","title":"PySpark Setup"},{"location":"Setup/#install-ipython","text":"Follow the steps as described in the official documentation of IPython. pip install ipython","title":"Install IPython"},{"location":"Setup/#start-pyspark","text":"export PYSPARK_DRIVER_PYTHON = ipython ./bin/pyspark Python 3.9.1 (default, Feb 3 2021, 07:38:02) Type 'copyright', 'credits' or 'license' for more information IPython 7.20.0 -- An enhanced Interactive Python. Type '?' for help. Setting default log level to \"WARN\". To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel). Welcome to ____ __ / __/__ ___ _____/ /__ _\\ \\/ _ \\/ _ `/ __/ '_/ /__ / .__/\\_,_/_/ /_/\\_\\ version 3.1.1 /_/ Using Python version 3.9.1 (default, Feb 3 2021 07:38:02) Spark context Web UI available at http://192.168.68.101:4040 Spark context available as 'sc' (master = local[*], app id = local-1613571272142). SparkSession available as 'spark'. In [1]: In [1]: spark.version Out[1]: '3.1.1'","title":"Start PySpark"},{"location":"SocketAuthHelper/","text":"SocketAuthHelper \u00b6 SocketAuthHelper is...FIXME","title":"SocketAuthHelper"},{"location":"SocketAuthHelper/#socketauthhelper","text":"SocketAuthHelper is...FIXME","title":"SocketAuthHelper"},{"location":"SocketAuthServer/","text":"SocketAuthServer \u00b6 serveToStream Utility \u00b6 serveToStream ( threadName : String , authHelper : SocketAuthHelper )( writeFunc : OutputStream => Unit ) : Array [ Any ] serveToStream ...FIXME serveToStream is used when: FIXME","title":"SocketAuthServer"},{"location":"SocketAuthServer/#socketauthserver","text":"","title":"SocketAuthServer"},{"location":"SocketAuthServer/#servetostream-utility","text":"serveToStream ( threadName : String , authHelper : SocketAuthHelper )( writeFunc : OutputStream => Unit ) : Array [ Any ] serveToStream ...FIXME serveToStream is used when: FIXME","title":" serveToStream Utility"},{"location":"SocketFuncServer/","text":"SocketFuncServer \u00b6 SocketFuncServer is...FIXME","title":"SocketFuncServer"},{"location":"SocketFuncServer/#socketfuncserver","text":"SocketFuncServer is...FIXME","title":"SocketFuncServer"},{"location":"SparkContext/","text":"SparkContext \u00b6 SparkContext is a Python class. JavaGateway \u00b6 SparkContext defines _gateway property for a JavaGateway that is given or launched when _ensure_initialized . JVMView \u00b6 SparkContext defines _jvm property for a JVMView ( py4j ) to access to the Java Virtual Machine of the JavaGateway . _ensure_initialized \u00b6 _ensure_initialized ( cls , instance = None , gateway = None , conf = None ) _ensure_initialized is a @classmethod . _ensure_initialized ...FIXME _ensure_initialized is used when: SparkContext is __init__ and setSystemProperty pyspark/shell.py is launched","title":"SparkContext"},{"location":"SparkContext/#sparkcontext","text":"SparkContext is a Python class.","title":"SparkContext"},{"location":"SparkContext/#javagateway","text":"SparkContext defines _gateway property for a JavaGateway that is given or launched when _ensure_initialized .","title":" JavaGateway"},{"location":"SparkContext/#jvmview","text":"SparkContext defines _jvm property for a JVMView ( py4j ) to access to the Java Virtual Machine of the JavaGateway .","title":" JVMView"},{"location":"SparkContext/#_ensure_initialized","text":"_ensure_initialized ( cls , instance = None , gateway = None , conf = None ) _ensure_initialized is a @classmethod . _ensure_initialized ...FIXME _ensure_initialized is used when: SparkContext is __init__ and setSystemProperty pyspark/shell.py is launched","title":" _ensure_initialized"},{"location":"SparkConversionMixin/","text":"SparkConversionMixin \u00b6 SparkConversionMixin is a Python mixin for SparkSession class.","title":"SparkConversionMixin"},{"location":"SparkConversionMixin/#sparkconversionmixin","text":"SparkConversionMixin is a Python mixin for SparkSession class.","title":"SparkConversionMixin"},{"location":"SparkSession/","text":"SparkSession \u00b6 _create_shell_session \u00b6 _create_shell_session () _create_shell_session is a @staticmethod . _create_shell_session ...FIXME _create_shell_session is used when: FIXME","title":"SparkSession"},{"location":"SparkSession/#sparksession","text":"","title":"SparkSession"},{"location":"SparkSession/#_create_shell_session","text":"_create_shell_session () _create_shell_session is a @staticmethod . _create_shell_session ...FIXME _create_shell_session is used when: FIXME","title":" _create_shell_session"},{"location":"UserDefinedPythonFunction/","text":"UserDefinedPythonFunction \u00b6 Creating Instance \u00b6 UserDefinedPythonFunction takes the following to be created: Name PythonFunction DataType ( Spark SQL ) Python Eval Type udfDeterministic flag Creating PythonUDF \u00b6 builder ( e : Seq [ Expression ]) : Expression builder creates a PythonUDF for the arguments and the given children expressions. builder is used when: UDFRegistration is requested to register a Python UDF ( Spark SQL ) UserDefinedPythonFunction is requested to apply Applying PythonUDF \u00b6 apply ( exprs : Column* ) : Column apply creates a PythonUDF with the input Column ( Spark SQL ) expressions and creates a new Column . apply is used when: UDFRegistration is requested to register a Python UDF ( Spark SQL ) UserDefinedPythonFunction is requested to apply","title":"UserDefinedPythonFunction"},{"location":"UserDefinedPythonFunction/#userdefinedpythonfunction","text":"","title":"UserDefinedPythonFunction"},{"location":"UserDefinedPythonFunction/#creating-instance","text":"UserDefinedPythonFunction takes the following to be created: Name PythonFunction DataType ( Spark SQL ) Python Eval Type udfDeterministic flag","title":"Creating Instance"},{"location":"UserDefinedPythonFunction/#creating-pythonudf","text":"builder ( e : Seq [ Expression ]) : Expression builder creates a PythonUDF for the arguments and the given children expressions. builder is used when: UDFRegistration is requested to register a Python UDF ( Spark SQL ) UserDefinedPythonFunction is requested to apply","title":" Creating PythonUDF"},{"location":"UserDefinedPythonFunction/#applying-pythonudf","text":"apply ( exprs : Column* ) : Column apply creates a PythonUDF with the input Column ( Spark SQL ) expressions and creates a new Column . apply is used when: UDFRegistration is requested to register a Python UDF ( Spark SQL ) UserDefinedPythonFunction is requested to apply","title":" Applying PythonUDF"},{"location":"configuration-properties/","text":"Configuration Properties \u00b6 spark.python.use.daemon \u00b6 Because forking processes from Java is expensive, we prefer to launch a single Python daemon, pyspark/daemon.py (by default) and tell it to fork new workers for our tasks. This daemon currently only works on UNIX-based systems now because it uses signals for child management, so we can also fall back to launching workers, pyspark/worker.py (by default) directly. Default: true (always disabled on Windows) Used when PythonWorkerFactory is created spark.python.daemon.module \u00b6 Default: pyspark.daemon Used when PythonWorkerFactory is created spark.python.worker.module \u00b6 Default: (undefined) Used when PythonWorkerFactory is created","title":"Configuration Properties"},{"location":"configuration-properties/#configuration-properties","text":"","title":"Configuration Properties"},{"location":"configuration-properties/#sparkpythonusedaemon","text":"Because forking processes from Java is expensive, we prefer to launch a single Python daemon, pyspark/daemon.py (by default) and tell it to fork new workers for our tasks. This daemon currently only works on UNIX-based systems now because it uses signals for child management, so we can also fall back to launching workers, pyspark/worker.py (by default) directly. Default: true (always disabled on Windows) Used when PythonWorkerFactory is created","title":" spark.python.use.daemon"},{"location":"configuration-properties/#sparkpythondaemonmodule","text":"Default: pyspark.daemon Used when PythonWorkerFactory is created","title":" spark.python.daemon.module"},{"location":"configuration-properties/#sparkpythonworkermodule","text":"Default: (undefined) Used when PythonWorkerFactory is created","title":" spark.python.worker.module"},{"location":"java_gateway/","text":"java_gateway.py \u00b6 java_gateway Python module allows for launching a gateway process to establish communication channel between this and Py4JServer . launch_gateway \u00b6 launch_gateway ( conf = None , popen_kwargs = None ) launch_gateway reads PYSPARK_GATEWAY_PORT and PYSPARK_GATEWAY_SECRET environment variables if defined and assumes that the child Java gateway process has already been started (e.g. PythonGatewayServer ). Otherwise, launch_gateway builds the command to start spark-submit : Finds SPARK_HOME with ./bin/spark-submit Appends all the configuration properties (from the input conf ) using --conf Appends PYSPARK_SUBMIT_ARGS environment variable if defined or assumes pyspark-shell launch_gateway sets up _PYSPARK_DRIVER_CONN_INFO_PATH environment variable to point at an unique temporary file. launch_gateway configures a pipe to stdin for the corresponding Java gateway process to use to monitor the Python process. launch_gateway starts bin/spark-submit command and waits for a connection info file to be created at _PYSPARK_DRIVER_CONN_INFO_PATH . launch_gateway reads the port and the secret from the file once available. launch_gateway connects to the gateway using py4j's ClientServer or JavaGateway based on PYSPARK_PIN_THREAD environment variable (default: false ). PYSPARK_PIN_THREAD Gateway true ClientServer false JavaGateway launch_gateway imports Spark packages and classes (using py4j): org.apache.spark.SparkConf org.apache.spark.api.java.* org.apache.spark.api.python.* org.apache.spark.ml.python.* org.apache.spark.mllib.api.python.* org.apache.spark.resource.* org.apache.spark.sql.* org.apache.spark.sql.api.python.* org.apache.spark.sql.hive.* scala.Tuple2 launch_gateway is used when: SparkContext is requested to _ensure_initialized","title":"java_gateway.py"},{"location":"java_gateway/#java_gatewaypy","text":"java_gateway Python module allows for launching a gateway process to establish communication channel between this and Py4JServer .","title":"java_gateway.py"},{"location":"java_gateway/#launch_gateway","text":"launch_gateway ( conf = None , popen_kwargs = None ) launch_gateway reads PYSPARK_GATEWAY_PORT and PYSPARK_GATEWAY_SECRET environment variables if defined and assumes that the child Java gateway process has already been started (e.g. PythonGatewayServer ). Otherwise, launch_gateway builds the command to start spark-submit : Finds SPARK_HOME with ./bin/spark-submit Appends all the configuration properties (from the input conf ) using --conf Appends PYSPARK_SUBMIT_ARGS environment variable if defined or assumes pyspark-shell launch_gateway sets up _PYSPARK_DRIVER_CONN_INFO_PATH environment variable to point at an unique temporary file. launch_gateway configures a pipe to stdin for the corresponding Java gateway process to use to monitor the Python process. launch_gateway starts bin/spark-submit command and waits for a connection info file to be created at _PYSPARK_DRIVER_CONN_INFO_PATH . launch_gateway reads the port and the secret from the file once available. launch_gateway connects to the gateway using py4j's ClientServer or JavaGateway based on PYSPARK_PIN_THREAD environment variable (default: false ). PYSPARK_PIN_THREAD Gateway true ClientServer false JavaGateway launch_gateway imports Spark packages and classes (using py4j): org.apache.spark.SparkConf org.apache.spark.api.java.* org.apache.spark.api.python.* org.apache.spark.ml.python.* org.apache.spark.mllib.api.python.* org.apache.spark.resource.* org.apache.spark.sql.* org.apache.spark.sql.api.python.* org.apache.spark.sql.hive.* scala.Tuple2 launch_gateway is used when: SparkContext is requested to _ensure_initialized","title":" launch_gateway"},{"location":"overview/","text":"PySpark \u00b6 PySpark is the Python API for Apache Spark. shell.py \u00b6 pyspark shell defines PYTHONSTARTUP environment variable to execute shell.py before the first prompt is displayed in Python interactive mode. Py4J \u00b6 java_gateway uses Py4J - A Bridge between Python and Java : Py4J enables Python programs running in a Python interpreter to dynamically access Java objects in a Java Virtual Machine. Methods are called as if the Java objects resided in the Python interpreter and Java collections can be accessed through standard Python collection methods. Py4J also enables Java programs to call back Python objects. pyspark.sql Package \u00b6 pyspark.sql is a Python package for Spark SQL. from pyspark.sql import * Tip Learn more about Modules and Packages in Python in The Python Tutorial . __init__.py \u00b6 The __init__.py files are required to make Python treat directories containing the file as packages. Per 6.4.1. Importing * From a Package : The import statement uses the following convention: if a package's __init__.py code defines a list named __all__ , it is taken to be the list of module names that should be imported when from package import * is encountered. Per Public and Internal Interfaces in PEP 8 -- Style Guide for Python Code : To better support introspection, modules should explicitly declare the names in their public API using the __all__ attribute. From python/pyspark/sql/__init__.py : __all__ = [ 'SparkSession' , 'SQLContext' , 'HiveContext' , 'UDFRegistration' , 'DataFrame' , 'GroupedData' , 'Column' , 'Catalog' , 'Row' , 'DataFrameNaFunctions' , 'DataFrameStatFunctions' , 'Window' , 'WindowSpec' , 'DataFrameReader' , 'DataFrameWriter' , 'PandasCogroupedOps' ] pandas \u00b6 The minimum version of Pandas is 0.23.2 (and PandasConversionMixin asserts that). import pandas as pd pyarrow \u00b6 The minimum version of PyArrow is 1.0.0 (and PandasConversionMixin asserts that). import pyarrow Python Mixins \u00b6 From 8.7. Class definitions : classdef ::= [decorators] \"class\" classname [inheritance] \":\" suite The inheritance list usually gives a list of base classes PySpark uses mixins: PandasConversionMixin PandasMapOpsMixin SparkConversionMixin","title":"Overview"},{"location":"overview/#pyspark","text":"PySpark is the Python API for Apache Spark.","title":"PySpark"},{"location":"overview/#shellpy","text":"pyspark shell defines PYTHONSTARTUP environment variable to execute shell.py before the first prompt is displayed in Python interactive mode.","title":"shell.py"},{"location":"overview/#py4j","text":"java_gateway uses Py4J - A Bridge between Python and Java : Py4J enables Python programs running in a Python interpreter to dynamically access Java objects in a Java Virtual Machine. Methods are called as if the Java objects resided in the Python interpreter and Java collections can be accessed through standard Python collection methods. Py4J also enables Java programs to call back Python objects.","title":"Py4J"},{"location":"overview/#pysparksql-package","text":"pyspark.sql is a Python package for Spark SQL. from pyspark.sql import * Tip Learn more about Modules and Packages in Python in The Python Tutorial .","title":"pyspark.sql Package"},{"location":"overview/#__init__py","text":"The __init__.py files are required to make Python treat directories containing the file as packages. Per 6.4.1. Importing * From a Package : The import statement uses the following convention: if a package's __init__.py code defines a list named __all__ , it is taken to be the list of module names that should be imported when from package import * is encountered. Per Public and Internal Interfaces in PEP 8 -- Style Guide for Python Code : To better support introspection, modules should explicitly declare the names in their public API using the __all__ attribute. From python/pyspark/sql/__init__.py : __all__ = [ 'SparkSession' , 'SQLContext' , 'HiveContext' , 'UDFRegistration' , 'DataFrame' , 'GroupedData' , 'Column' , 'Catalog' , 'Row' , 'DataFrameNaFunctions' , 'DataFrameStatFunctions' , 'Window' , 'WindowSpec' , 'DataFrameReader' , 'DataFrameWriter' , 'PandasCogroupedOps' ]","title":"__init__.py"},{"location":"overview/#pandas","text":"The minimum version of Pandas is 0.23.2 (and PandasConversionMixin asserts that). import pandas as pd","title":"pandas"},{"location":"overview/#pyarrow","text":"The minimum version of PyArrow is 1.0.0 (and PandasConversionMixin asserts that). import pyarrow","title":"pyarrow"},{"location":"overview/#python-mixins","text":"From 8.7. Class definitions : classdef ::= [decorators] \"class\" classname [inheritance] \":\" suite The inheritance list usually gives a list of base classes PySpark uses mixins: PandasConversionMixin PandasMapOpsMixin SparkConversionMixin","title":"Python Mixins"},{"location":"shell/","text":"shell.py \u00b6 shell.py script is the interactive shell of PySpark. shell.py defines the following variables: sc spark sql sqlContext and sqlCtx for compatibility","title":"shell.py"},{"location":"shell/#shellpy","text":"shell.py script is the interactive shell of PySpark. shell.py defines the following variables: sc spark sql sqlContext and sqlCtx for compatibility","title":"shell.py"},{"location":"spark-logging/","text":"Logging \u00b6 Spark uses log4j for logging. Note Learn more on Spark Logging in The Internals of Apache Spark online book.","title":"Logging"},{"location":"spark-logging/#logging","text":"Spark uses log4j for logging. Note Learn more on Spark Logging in The Internals of Apache Spark online book.","title":"Logging"},{"location":"dataframe/toPandas/","text":"toPandas \u00b6 toPandas is...FIXME","title":"toPandas"},{"location":"dataframe/toPandas/#topandas","text":"toPandas is...FIXME","title":"toPandas"}]}