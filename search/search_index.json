{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"The Internals of PySpark (Apache Spark 3.4.0)","text":"<p>Welcome to The Internals of PySpark online book! \ud83e\udd19</p> <p>I'm Jacek Laskowski, an IT freelancer specializing in Apache Spark (incl. Spark SQL and Spark Structured Streaming), Delta Lake, Databricks, and Apache Kafka (incl. Kafka Streams and ksqlDB) with brief forays into a wider data engineering space (e.g., Trino, Dask and dbt, mostly during Warsaw Data Engineering meetups).</p> <p>I'm very excited to have you here and hope you will enjoy exploring the internals of PySpark as much as I have.</p> <p>Flannery O'Connor</p> <p>I write to discover what I know.</p> <p>\"The Internals Of\" series</p> <p>I'm also writing other online books in the \"The Internals Of\" series. Please visit \"The Internals Of\" Online Books home page.</p> <p>Expect text and code snippets from a variety of public sources. Attribution follows.</p> <p>Now, let's take a deep dive into PySpark \ud83d\udd25</p> <p>Last update: 2023-05-03</p>"},{"location":"MonitorThread/","title":"MonitorThread","text":"<p><code>MonitorThread</code> is...FIXME</p>"},{"location":"Py4JServer/","title":"Py4JServer","text":"<p><code>Py4JServer</code> is a gateway server between Python and Java Virtual Machine (JVM) using Py4J.</p> <p><code>Py4JServer</code> is a wrapper for a py4j Server.</p>"},{"location":"Py4JServer/#creating-instance","title":"Creating Instance","text":"<p><code>Py4JServer</code> takes the following to be created:</p> <ul> <li> <code>SparkConf</code> (Spark Core) <p><code>Py4JServer</code> is created\u00a0when:</p> <ul> <li>PythonGatewayServer command-line application is started</li> <li>PythonRunner command-line application is started</li> </ul>"},{"location":"Py4JServer/#py4j-server","title":"py4j Server <p><code>Py4JServer</code> creates a <code>ClientServer</code> (py4j) or <code>GatewayServer</code> (py4j) based on PYSPARK_PIN_THREAD environment variable.</p>","text":""},{"location":"Py4JServer/#connection-secret","title":"Connection Secret <pre><code>secret: String\n</code></pre> <p><code>Py4JServer</code> creates a connection secret for a secure communication.</p>","text":""},{"location":"Py4JServer/#start","title":"start <pre><code>start(): Unit\n</code></pre> <p><code>start</code> requests the py4j Server to start.</p>","text":""},{"location":"Py4JServer/#getlisteningport","title":"getListeningPort <pre><code>getListeningPort: Int\n</code></pre> <p><code>getListeningPort</code> requests the py4j Server for the listening port.</p>","text":""},{"location":"PythonAccumulatorV2/","title":"PythonAccumulatorV2","text":"<p><code>PythonAccumulatorV2</code> is...FIXME</p>"},{"location":"PythonBroadcast/","title":"PythonBroadcast","text":"<p><code>PythonBroadcast</code> is...FIXME</p>"},{"location":"PythonForeachWriter/","title":"PythonForeachWriter","text":"<p><code>PythonForeachWriter</code> is...FIXME</p>"},{"location":"PythonFunction/","title":"PythonFunction","text":"<p><code>PythonFunction</code> is an abstraction of metadata of a Python function (to be executed in a BasePythonRunner).</p>"},{"location":"PythonFunction/#contract-subset","title":"Contract (Subset)","text":""},{"location":"PythonFunction/#accumulator","title":"accumulator","text":"<pre><code>accumulator: PythonAccumulatorV2\n</code></pre> <p>PythonAccumulatorV2</p> <p>Used when:</p> <ul> <li><code>BasePythonRunner</code> is created</li> </ul>"},{"location":"PythonFunction/#broadcastVars","title":"broadcastVars","text":"<pre><code>broadcastVars: JList[Broadcast[PythonBroadcast]]\n</code></pre> <p>A collection of broadcast variables (Spark Core) with a PythonBroadcast</p> <p>Used when:</p> <ul> <li><code>WriterThread</code> is created</li> </ul>"},{"location":"PythonFunction/#command","title":"command","text":"<pre><code>command: Seq[Byte]\n</code></pre> <p>Used when:</p> <ul> <li><code>PythonRunner</code> is requested to newWriterThread</li> <li><code>UDFRegistration</code> is requested to register a Python UDF (for logging purposes only)</li> <li><code>PythonUDFRunner</code> is requested to writeUDFs</li> </ul>"},{"location":"PythonFunction/#implementations","title":"Implementations","text":"<ul> <li>SimplePythonFunction</li> </ul>"},{"location":"PythonGatewayServer/","title":"PythonGatewayServer","text":"<p><code>PythonGatewayServer</code> is a command-line application (process) that starts a Py4JServer on an ephemeral port.</p> <p><code>PythonGatewayServer</code> is the Python runner for pyspark shell (Spark Core).</p>"},{"location":"PythonGatewayServer/#main","title":"main <p><code>main</code> creates a Py4JServer and requests it to start.</p> <p><code>main</code> requests the <code>Py4JServer</code> for the listening port (boundPort) and prints out the following DEBUG message to the logs:</p> <pre><code>Started PythonGatewayServer on port [boundPort]\n</code></pre> <p> <code>main</code> uses _PYSPARK_DRIVER_CONN_INFO_PATH environment variable for the path of a connection info file (for the associated python process) with the listening port and the secret. <p><code>main</code> pauses (blocks) until the Python driver finishes (by reading from the system input that blocks until input data is available, the end of the stream is detected, or an exception is thrown).</p> <p>In the end, once the Python driver finishes, <code>main</code> prints out the following DEBUG message to the logs:</p> <pre><code>Exiting due to broken pipe from Python driver\n</code></pre> <p><code>main</code> prints out the following ERROR message to the logs and exists when the listening port is <code>-1</code>:</p> <pre><code>[server] failed to bind; exiting\n</code></pre>","text":""},{"location":"PythonGatewayServer/#_pyspark_driver_conn_info_path","title":"_PYSPARK_DRIVER_CONN_INFO_PATH <p><code>PythonGatewayServer</code> uses <code>_PYSPARK_DRIVER_CONN_INFO_PATH</code> environment variable for the path of a connection info file for communication between this and the Python processes.</p> <p><code>_PYSPARK_DRIVER_CONN_INFO_PATH</code> is configured when java_gateway.py module is requested to launch_gateway.</p>","text":""},{"location":"PythonGatewayServer/#logging","title":"Logging <p>Enable <code>ALL</code> logging level for <code>org.apache.spark.api.python.PythonGatewayServer</code> logger to see what happens inside.</p> <p>Add the following line to <code>conf/log4j2.properties</code>:</p> <pre><code>logger.PythonGatewayServer.name = org.apache.spark.api.python.PythonGatewayServer\nlogger.PythonGatewayServer.level = all\n</code></pre> <p>Refer to Logging.</p>","text":""},{"location":"PythonRDD/","title":"PythonRDD","text":"<p><code>PythonRDD</code> is an <code>RDD</code> (<code>RDD[Array[Byte]]</code>) that uses PythonRunner (to compute a partition).</p>"},{"location":"PythonRDD/#creating-instance","title":"Creating Instance","text":"<p><code>PythonRDD</code> takes the following to be created:</p> <ul> <li> Parent <code>RDD</code> <li> PythonFunction <li> <code>preservePartitoning</code> flag <li> <code>isFromBarrier</code> flag (default: <code>false</code>) <p><code>PythonRDD</code> is created when...FIXME</p>"},{"location":"PythonRDD/#runjob","title":"runJob <pre><code>runJob(\n  sc: SparkContext,\n  rdd: JavaRDD[Array[Byte]],\n  partitions: JArrayList[Int]): Array[Any]\n</code></pre> <p><code>runJob</code>...FIXME</p>","text":""},{"location":"PythonRDD/#collectandserve","title":"collectAndServe <pre><code>collectAndServe[T](\n  rdd: RDD[T]): Array[Any]\n</code></pre> <p><code>collectAndServe</code>...FIXME</p>","text":""},{"location":"PythonRDD/#collectandservewithjobgroup","title":"collectAndServeWithJobGroup <pre><code>collectAndServeWithJobGroup[T](\n  rdd: RDD[T],\n  groupId: String,\n  description: String,\n  interruptOnCancel: Boolean): Array[Any]\n</code></pre> <p><code>collectAndServeWithJobGroup</code>...FIXME</p>","text":""},{"location":"PythonRDD/#serveiterator-utility","title":"serveIterator Utility <pre><code>serveIterator(\n  items: Iterator[_],\n  threadName: String): Array[Any]\n</code></pre> <p><code>serveIterator</code> serveToStream with a writer function that...FIXME</p> <p><code>serveIterator</code>\u00a0is used when:</p> <ul> <li><code>PythonRDD</code> utility is used to runJob, collectAndServe and collectAndServeWithJobGroup</li> <li><code>Dataset</code> is requested to <code>collectToPython</code>, <code>tailToPython</code>, <code>getRowsToPython</code></li> </ul>","text":""},{"location":"PythonRDD/#servetostream-utility","title":"serveToStream Utility <pre><code>serveToStream(\n  threadName: String)(\n  writeFunc: OutputStream =&gt; Unit): Array[Any]\n</code></pre> <p><code>serveToStream</code> serveToStream with the authHelper and the input arguments.</p> <p><code>serveToStream</code>\u00a0is used when:</p> <ul> <li><code>PythonRDD</code> utility is used to serveIterator</li> <li><code>Dataset</code> is requested to <code>collectAsArrowToPython</code></li> </ul>","text":""},{"location":"PythonRDD/#socketauthhelper","title":"SocketAuthHelper <p><code>PythonRDD</code> uses a SocketAuthHelper.</p>","text":""},{"location":"PythonRunner/","title":"PythonRunner","text":"<p><code>PythonRunner</code> is a command-line application to launch Python applications.</p> <p><code>PythonRunner</code> is used by spark-submit.</p> <p><code>PythonRunner</code> executes a configured python executable as a subprocess and then has it connect back to the JVM to access system properties, etc.</p>"},{"location":"PythonRunner/#arguments","title":"Arguments","text":"<p><code>PythonRunner</code> requires the following command-line arguments:</p> <ol> <li>Main python file (<code>pythonFile</code>)</li> <li>Extra python files (<code>pyFiles</code>)</li> <li>Application arguments</li> </ol>"},{"location":"PythonRunner/#main","title":"main <p><code>main</code> takes the arguments from command line.</p> <p> <code>main</code> determines what python executable to use based on (in that order): <ol> <li>spark.pyspark.driver.python configuration property</li> <li>spark.pyspark.python configuration property</li> <li><code>PYSPARK_DRIVER_PYTHON</code> environment variable</li> <li><code>PYSPARK_PYTHON</code> environment variable</li> <li><code>python3</code></li> </ol> <p><code>main</code> creates a Py4JServer that is started on a daemon py4j-gateway-init thread.</p> <p><code>main</code> waits until the gateway server has started.</p> <p><code>main</code> launches a Python process using the python executable and the following environment variables.</p>    Environment Variable Value     <code>PYTHONPATH</code>    <code>PYTHONUNBUFFERED</code> YES   PYSPARK_GATEWAY_PORT getListeningPort   PYSPARK_GATEWAY_SECRET secret   <code>PYSPARK_PYTHON</code> spark.pyspark.python if defined   <code>PYTHONHASHSEED</code> <code>PYTHONHASHSEED</code> env var if defined   <code>OMP_NUM_THREADS</code> <code>spark.driver.cores</code> unless defined    <p><code>main</code> waits for the Python process to finish and requests the <code>Py4JServer</code> to shutdown.</p>","text":""},{"location":"PythonRunner/#demo","title":"Demo <pre><code>./bin/spark-class org.apache.spark.deploy.PythonRunner\n</code></pre>","text":""},{"location":"PythonWorkerFactory/","title":"PythonWorkerFactory","text":""},{"location":"PythonWorkerFactory/#creating-instance","title":"Creating Instance","text":"<p><code>PythonWorkerFactory</code> takes the following to be created:</p> <ul> <li> Python Executable <li> Environment Variables (<code>Map[String, String]</code>) <p><code>PythonWorkerFactory</code> is created when <code>SparkEnv</code> is requested to <code>createPythonWorker</code> (when <code>BasePythonRunner</code> is requested to compute a partition).</p>"},{"location":"PythonWorkerFactory/#usedaemon-flag","title":"useDaemon Flag <p><code>PythonWorkerFactory</code> uses <code>useDaemon</code> internal flag that is the value of spark.python.use.daemon configuration property to decide whether to use lighter daemon or non-daemon workers.</p> <p><code>useDaemon</code> flag is used when <code>PythonWorkerFactory</code> requested to create, stop or release a worker and stop a daemon module.</p>","text":""},{"location":"PythonWorkerFactory/#python-daemon-module","title":"Python Daemon Module <p><code>PythonWorkerFactory</code> uses spark.python.daemon.module configuration property to define the Python Daemon Module.</p> <p>The Python Daemon Module is used when <code>PythonWorkerFactory</code> is requested to create and start a daemon module.</p>","text":""},{"location":"PythonWorkerFactory/#python-worker-module","title":"Python Worker Module <p><code>PythonWorkerFactory</code> uses spark.python.worker.module configuration property to specify the Python Worker Module.</p> <p>The Python Worker Module is used when <code>PythonWorkerFactory</code> is requested to create and start a worker.</p>","text":""},{"location":"PythonWorkerFactory/#creating-python-worker","title":"Creating Python Worker <pre><code>create(): Socket\n</code></pre> <p><code>create</code>...FIXME</p> <p><code>create</code> is used when <code>SparkEnv</code> is requested to <code>createPythonWorker</code>.</p>","text":""},{"location":"PythonWorkerFactory/#creating-daemon-worker","title":"Creating Daemon Worker <pre><code>createThroughDaemon(): Socket\n</code></pre> <p><code>createThroughDaemon</code>...FIXME</p> <p><code>createThroughDaemon</code> is used when <code>PythonWorkerFactory</code> is requested to create a Python worker (with useDaemon flag enabled).</p>","text":""},{"location":"PythonWorkerFactory/#starting-python-daemon-process","title":"Starting Python Daemon Process <pre><code>startDaemon(): Unit\n</code></pre> <p><code>startDaemon</code>...FIXME</p>","text":""},{"location":"PythonWorkerFactory/#creating-simple-non-daemon-worker","title":"Creating Simple Non-Daemon Worker <pre><code>createSimpleWorker(): Socket\n</code></pre> <p><code>createSimpleWorker</code>...FIXME</p> <p><code>createSimpleWorker</code> is used when <code>PythonWorkerFactory</code> is requested to create a Python worker (with useDaemon flag disabled).</p>","text":""},{"location":"PythonWorkerFactory/#logging","title":"Logging <p>Enable <code>ALL</code> logging level for <code>org.apache.spark.api.python.PythonWorkerFactory</code> logger to see what happens inside.</p> <p>Add the following line to <code>conf/log4j2.properties</code>:</p> <pre><code>logger.PythonWorkerFactory.name = org.apache.spark.api.python.PythonWorkerFactory\nlogger.PythonWorkerFactory.level = all\n</code></pre> <p>Refer to Logging.</p>","text":""},{"location":"Setup/","title":"PySpark Setup","text":""},{"location":"Setup/#install-ipython","title":"Install IPython","text":"<p>Follow the steps as described in the official documentation of IPython.</p> <pre><code>pip install ipython\n</code></pre>"},{"location":"Setup/#start-pyspark","title":"Start PySpark","text":"<pre><code>export PYSPARK_DRIVER_PYTHON=ipython\n</code></pre> <p>For Java 11, use <code>-Dio.netty.tryReflectionSetAccessible=true</code> (see Downloading in the official documentation of Apache Spark).</p> <pre><code>./bin/pyspark --driver-java-options=-Dio.netty.tryReflectionSetAccessible=true\n</code></pre> <pre><code>Python 3.9.1 (default, Feb  3 2021, 07:38:02)\nType 'copyright', 'credits' or 'license' for more information\nIPython 7.20.0 -- An enhanced Interactive Python. Type '?' for help.\nSetting default log level to \"WARN\".\nTo adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\nWelcome to\n      ____              __\n     / __/__  ___ _____/ /__\n    _\\ \\/ _ \\/ _ `/ __/  '_/\n   /__ / .__/\\_,_/_/ /_/\\_\\   version 3.1.1\n      /_/\n\nUsing Python version 3.9.1 (default, Feb  3 2021 07:38:02)\nSpark context Web UI available at http://192.168.68.101:4040\nSpark context available as 'sc' (master = local[*], app id = local-1613571272142).\nSparkSession available as 'spark'.\n\nIn [1]:\n</code></pre> <pre><code>In [1]: spark.version\nOut[1]: '3.1.1'\n</code></pre>"},{"location":"SimplePythonFunction/","title":"SimplePythonFunction","text":"<p><code>SimplePythonFunction</code> is...FIXME</p>"},{"location":"SocketAuthHelper/","title":"SocketAuthHelper","text":"<p><code>SocketAuthHelper</code> is...FIXME</p>"},{"location":"SocketAuthServer/","title":"SocketAuthServer","text":""},{"location":"SocketAuthServer/#servetostream-utility","title":"serveToStream Utility <pre><code>serveToStream(\n  threadName: String,\n  authHelper: SocketAuthHelper)(writeFunc: OutputStream =&gt; Unit): Array[Any]\n</code></pre> <p><code>serveToStream</code>...FIXME</p> <p><code>serveToStream</code>\u00a0is used when:</p> <ul> <li>FIXME</li> </ul>","text":""},{"location":"SocketFuncServer/","title":"SocketFuncServer","text":"<p><code>SocketFuncServer</code> is...FIXME</p>"},{"location":"SparkConf/","title":"SparkConf","text":"<p><code>SparkConf</code> is a Python class.</p>"},{"location":"SparkConf/#creating-instance","title":"Creating Instance","text":"<p><code>SparkConf</code> takes the following to be created:</p> <ul> <li> <code>loadDefaults</code> flag (default: <code>True</code>) <li> <code>JVMView</code> (py4j) <li> JConf (default: <code>None</code>) <p>While being created, <code>SparkConf</code> uses the JVMView (of the SparkContext) unless the <code>_jconf</code> and <code>_jvm</code> are given.</p>"},{"location":"SparkConf/#demo","title":"Demo","text":"<pre><code>from pyspark import SparkConf\n</code></pre>"},{"location":"SparkContext/","title":"SparkContext","text":""},{"location":"SparkContext/#creating-instance","title":"Creating Instance","text":"<p><code>SparkContext</code> takes the following to be created:</p> <ul> <li> Master URL (default: <code>None</code>) <li> Application Name (default: <code>None</code>) <li> Spark Home (default: <code>None</code>) <li> Py Files (default: <code>None</code>) <li> Environment (default: <code>None</code>) <li> Batch Size (default: <code>0</code>) <li> <code>PickleSerializer</code> <li> <code>SparkConf</code> (default: <code>None</code>) <li> Gateway (default: <code>None</code>) <li> Corresponding <code>SparkContext</code> on JVM (default: <code>None</code>) <li> <code>BasicProfiler</code> <p>While being created, <code>SparkContext</code> _ensure_initialized (with the gateway and the conf) followed by _do_init.</p>"},{"location":"SparkContext/#demo","title":"Demo","text":"<pre><code>from pyspark import SparkContext\n</code></pre>"},{"location":"SparkContext/#javagateway","title":"JavaGateway <p><code>SparkContext</code> defines <code>_gateway</code> property for a <code>JavaGateway</code> that is given or launched when _ensure_initialized.</p>","text":""},{"location":"SparkContext/#jvmview","title":"JVMView <p><code>SparkContext</code> defines <code>_jvm</code> property for a <code>JVMView</code> (py4j) to access to the Java Virtual Machine of the JavaGateway.</p>","text":""},{"location":"SparkContext/#_ensure_initialized","title":"_ensure_initialized <pre><code>_ensure_initialized(\n  cls, instance=None, gateway=None, conf=None)\n</code></pre> <p><code>_ensure_initialized</code> is a <code>@classmethod</code>.</p> <p><code>_ensure_initialized</code> takes the given gateway or launch_gateway.</p> <p><code>_ensure_initialized</code>...FIXME</p> <p><code>_ensure_initialized</code>\u00a0is used when:</p> <ul> <li><code>SparkContext</code> is created and <code>setSystemProperty</code></li> <li>shell.py is launched</li> </ul>","text":""},{"location":"SparkContext/#_do_init","title":"_do_init <pre><code>_do_init(\n  self, master, appName, sparkHome,\n  pyFiles, environment, batchSize, serializer,\n  conf, jsc, profiler_cls)\n</code></pre> <p><code>_do_init</code>...FIXME</p>","text":""},{"location":"building-from-sources/","title":"Building from Sources","text":"<pre><code>$ java -version\nopenjdk version \"11.0.10\" 2021-01-19\nOpenJDK Runtime Environment AdoptOpenJDK (build 11.0.10+9)\nOpenJDK 64-Bit Server VM AdoptOpenJDK (build 11.0.10+9, mixed mode)\n</code></pre> <pre><code>./build/mvn \\\n  -Pyarn,kubernetes,hive,hive-thriftserver,scala-2.12 \\\n  -DskipTests \\\n  clean install\n</code></pre>"},{"location":"building-from-sources/#building-pyspark-related-operators","title":"Building PySpark-Related Operators","text":"<pre><code>./build/mvn -DskipTests -pl :spark-sql_2.12 clean install\n</code></pre> <pre><code>cp sql/core/target/spark-sql_2.12-3.1.1.jar assembly/target/scala-2.12/jars/\n</code></pre>"},{"location":"configuration-properties/","title":"Configuration Properties","text":""},{"location":"configuration-properties/#sparkpysparkdriverpython","title":"spark.pyspark.driver.python <p>Default: (undefined)</p>","text":""},{"location":"configuration-properties/#sparkpysparkpython","title":"spark.pyspark.python <p>Default: (undefined)</p>","text":""},{"location":"configuration-properties/#sparkpythonusedaemon","title":"spark.python.use.daemon <p>Because forking processes from Java is expensive, we prefer to launch a single Python daemon, <code>pyspark/daemon.py</code> (by default) and tell it to fork new workers for our tasks. This daemon currently only works on UNIX-based systems now because it uses signals for child management, so we can also fall back to launching workers, <code>pyspark/worker.py</code> (by default) directly.</p> <p>Default: <code>true</code> (always disabled on Windows)</p> <p>Used when PythonWorkerFactory is created</p>","text":""},{"location":"configuration-properties/#sparkpythondaemonmodule","title":"spark.python.daemon.module <p>Default: <code>pyspark.daemon</code></p> <p>Used when PythonWorkerFactory is created</p>","text":""},{"location":"configuration-properties/#sparkpythonworkermodule","title":"spark.python.worker.module <p>Default: (undefined)</p> <p>Used when PythonWorkerFactory is created</p>","text":""},{"location":"configuration-properties/#spark.sql.execution","title":"spark.sql.execution","text":""},{"location":"configuration-properties/#spark.sql.execution.arrow.maxRecordsPerBatch","title":"arrow.maxRecordsPerBatch <p>spark.sql.execution.arrow.maxRecordsPerBatch</p> <p>When using Apache Arrow, the maximum number of records that can be written to a single <code>ArrowRecordBatch</code> in memory.</p> <p>If zero or negative there is no limit.</p> <p>Default: <code>10000</code></p> <p>Used when:</p> <ul> <li><code>ApplyInPandasWithStatePythonRunner</code> is requested for <code>workerConf</code></li> <li><code>ArrowEvalPythonExec</code> is created</li> <li><code>Dataset</code> is requested to <code>toArrowBatchRdd</code></li> <li><code>MapInBatchExec</code> is created</li> <li><code>SparkConnectPlanner</code> is requested to <code>handleSqlCommand</code></li> <li><code>SparkConnectStreamHandler</code> is requested to <code>processAsArrowBatches</code></li> </ul>","text":""},{"location":"configuration-properties/#spark.sql.execution.arrow.pyspark.enabled","title":"arrow.pyspark.enabled <p>spark.sql.execution.arrow.pyspark.enabled</p> <p>Enables Arrow Optimization</p> <p>Default: <code>false</code></p>","text":""},{"location":"configuration-properties/#spark.sql.execution.pandas.udf.buffer.size","title":"pandas.udf.buffer.size <p>spark.sql.execution.pandas.udf.buffer.size</p> <p><code>spark.buffer.size</code> for Pandas UDF executions</p> <p>Note that Pandas execution requires more than 4 bytes. Lowering this value could make small Pandas UDF batch iterated and pipelined; however, it might degrade performance. See SPARK-27870.</p> <p>Default: <code>spark.buffer.size</code> (Spark Core)</p> <p>Used when:</p> <ul> <li><code>ApplyInPandasWithStatePythonRunner</code> and ArrowPythonRunner are created (and initialize bufferSize)</li> </ul>","text":""},{"location":"configuration-properties/#spark.sql.execution.pyspark","title":"spark.sql.execution.pyspark","text":""},{"location":"configuration-properties/#spark.sql.execution.pyspark.udf.simplifiedTraceback.enabled","title":"udf.simplifiedTraceback.enabled <p>spark.sql.execution.pyspark.udf.simplifiedTraceback.enabled</p> <p>Controls the traceback from Python UDFs. When enabled (<code>true</code>), traceback is simplified and hides the Python worker, (de)serialization, etc. from PySpark in tracebacks, and only shows the exception messages from UDFs.</p> <p>Works only with CPython 3.7+</p> <p>Default: <code>true</code></p> <p>Used when:</p> <ul> <li><code>ApplyInPandasWithStatePythonRunner</code>, ArrowPythonRunner, <code>CoGroupedArrowPythonRunner</code>, PythonUDFRunner are created (and initialize simplifiedTraceback flag)</li> </ul>","text":""},{"location":"environment-variables/","title":"Environment Variables","text":"<p>PySpark uses environment variables to configure execution environment.</p>"},{"location":"environment-variables/#pyspark_gateway_port","title":"PYSPARK_GATEWAY_PORT","text":""},{"location":"environment-variables/#pyspark_gateway_secret","title":"PYSPARK_GATEWAY_SECRET","text":""},{"location":"environment-variables/#pyspark_pin_thread","title":"PYSPARK_PIN_THREAD <p>Enables pinned thread mode to synchronize PVM threads with JVM threads based on Py4J's ClientServer (<code>true</code>) or GatewayServer (<code>false</code>)</p> <p>Default: <code>false</code></p> <p>Used when:</p> <ul> <li>launch_gateway is executed</li> <li>Py4JServer is created (and initializes the server)</li> </ul>","text":""},{"location":"logging/","title":"Logging","text":"<p>Delta Lake uses the same logging infrastructure as Apache Spark.</p>"},{"location":"overview/","title":"PySpark","text":"<p>PySpark is the Python frontend for Apache Spark.</p>"},{"location":"overview/#shellpy","title":"shell.py","text":"<p>pyspark shell defines PYTHONSTARTUP environment variable to execute shell.py before the first prompt is displayed in Python interactive mode.</p>"},{"location":"overview/#py4j","title":"Py4J","text":"<p>java_gateway uses Py4J - A Bridge between Python and Java:</p> <p>Py4J enables Python programs running in a Python interpreter to dynamically access Java objects in a Java Virtual Machine. Methods are called as if the Java objects resided in the Python interpreter and Java collections can be accessed through standard Python collection methods. Py4J also enables Java programs to call back Python objects.</p>"},{"location":"overview/#pysparksql-package","title":"pyspark.sql Package","text":"<p><code>pyspark.sql</code> is a Python package for Spark SQL.</p> <pre><code>from pyspark.sql import *\n</code></pre> <p>Tip</p> <p>Learn more about Modules and Packages in Python in The Python Tutorial.</p>"},{"location":"overview/#__init__py","title":"__init__.py","text":"<p>The <code>__init__.py</code> files are required to make Python treat directories containing the file as packages.</p> <p>Per 6.4.1. Importing * From a Package:</p> <p>The import statement uses the following convention: if a package's <code>__init__.py</code> code defines a list named <code>__all__</code>, it is taken to be the list of module names that should be imported when <code>from package import *</code> is encountered.</p> <p>Per Public and Internal Interfaces in PEP 8 -- Style Guide for Python Code:</p> <p>To better support introspection, modules should explicitly declare the names in their public API using the <code>__all__</code> attribute.</p> <p>From <code>python/pyspark/sql/__init__.py</code>:</p> <pre><code>__all__ = [\n    'SparkSession', 'SQLContext', 'HiveContext', 'UDFRegistration',\n    'DataFrame', 'GroupedData', 'Column', 'Catalog', 'Row',\n    'DataFrameNaFunctions', 'DataFrameStatFunctions', 'Window', 'WindowSpec',\n    'DataFrameReader', 'DataFrameWriter', 'PandasCogroupedOps'\n]\n</code></pre>"},{"location":"overview/#pandas","title":"pandas","text":"<p>The minimum version of Pandas is <code>0.23.2</code> (and PandasConversionMixin asserts that).</p> <pre><code>import pandas as pd\n</code></pre>"},{"location":"overview/#pyarrow","title":"pyarrow","text":"<p>The minimum version of PyArrow is <code>1.0.0</code> (and PandasConversionMixin asserts that).</p> <pre><code>import pyarrow\n</code></pre>"},{"location":"overview/#python-mixins","title":"Python Mixins","text":"<p>From 8.7. Class definitions:</p> <p>classdef    ::=  [decorators] \"class\" classname [inheritance] \":\" suite</p> <p>The inheritance list usually gives a list of base classes</p> <p>PySpark uses mixins:</p> <ul> <li>PandasConversionMixin</li> <li>PandasMapOpsMixin</li> <li>SparkConversionMixin</li> </ul>"},{"location":"pyspark/","title":"PySpark Shell","text":"<p><code>pyspark</code> is a Spark shell for Python.</p>"},{"location":"arrow-optimization/","title":"Arrow Optimization","text":"<p>Arrow Optimization is an optimization that uses Apache Arrow for columnar data transfers in the following:</p> <ul> <li>pyspark.sql.DataFrame.toPandas</li> <li>pyspark.sql.SparkSession.createDataFrame (when called with a Pandas <code>DataFrame</code> or a NumPy <code>ndarray</code>)</li> </ul> <p>The following data types are unsupported: <code>ArrayType</code> of <code>TimestampType</code>.</p>"},{"location":"connect/","title":"Spark Connect","text":"<p>PySpark supports remote connection to Spark clusters using Spark Connect (Spark SQL).</p> <pre><code>$ ./bin/pyspark --help\nUsage: ./bin/pyspark [options]\n\nOptions:\n Spark Connect only:\n   --remote CONNECT_URL       URL to connect to the server for Spark Connect, e.g.,\n                              sc://host:port. --master and --deploy-mode cannot be set\n                              together with this option. This option is experimental, and\n                              might change between minor releases.\n ...\n</code></pre> <p>Spark Connect for Python requires the following Python libraries:</p> Module Version pandas 1.0.5 pyarrow 1.0.0 grpc 1.48.1 <pre><code>// switching to an conda environment with the libraries\n$ conda activate pyspark\n\n$ ./bin/pyspark --remote sc://localhost\nPython 3.10.10 (main, Mar 21 2023, 13:41:39) [Clang 14.0.6 ] on darwin\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\nWelcome to\n      ____              __\n     / __/__  ___ _____/ /__\n    _\\ \\/ _ \\/ _ `/ __/  '_/\n   /__ / .__/\\_,_/_/ /_/\\_\\   version 3.4.0\n      /_/\n\nUsing Python version 3.10.10 (main, Mar 21 2023 13:41:39)\nClient connected to the Spark Connect server at localhost\nSparkSession available as 'spark'.\n\n&gt;&gt;&gt; spark.client\n&lt;pyspark.sql.connect.client.SparkConnectClient object at 0x7fed8867ab90&gt;\n</code></pre>"},{"location":"connect/#is_remote","title":"is_remote","text":"<pre><code># from pyspark.sql.utils import is_remote\nis_remote() -&gt; bool\n</code></pre> <p><code>is_remote</code> is <code>True</code> when <code>SPARK_REMOTE</code> environment variable is defined (in <code>os.environ</code>).</p>"},{"location":"demo/","title":"Demos","text":"<ol> <li>Executing PySpark Applications Using spark-submit</li> <li>Running PySpark Application on minikube</li> </ol>"},{"location":"demo/executing-pyspark-applications-using-spark-submit/","title":"Demo: Executing PySpark Applications Using spark-submit","text":"<p>PySpark applications are executed using <code>spark-submit</code> (Spark Core) command-line application.</p> <pre><code>spark-submit 1.py extra args\n</code></pre> <p>For a PySpark application, <code>spark-submit</code> uses PythonRunner and launches an extra python process:</p> <pre><code>ps -o pid,ppid,command | grep python | grep -v grep\n</code></pre> <pre><code>org.apache.spark.deploy.SparkSubmit 1.py extra args\n</code></pre> <pre><code>Python /usr/local/bin/ipython 1.py extra args\n</code></pre>"},{"location":"demo/executing-pyspark-applications-using-spark-submit/#spark_print_launch_command-environment-variable","title":"SPARK_PRINT_LAUNCH_COMMAND Environment Variable","text":"<p>Use <code>SPARK_PRINT_LAUNCH_COMMAND</code> environment variable to have the complete Spark command printed out to the standard output (cf. spark-submit shell script).</p> <pre><code>SPARK_PRINT_LAUNCH_COMMAND=1 spark-submit 1.py extra args\n</code></pre>"},{"location":"demo/executing-pyspark-applications-using-spark-submit/#verbose-option","title":"verbose Option","text":"<p>Use <code>--verbose</code> option for verbose debugging output.</p> <pre><code>Parsed arguments:\n  ...\n  pyFiles                 null\n  ...\n  primaryResource         file:/Users/jacek/dev/sandbox/python-sandbox/1.py\n  name                    1.py\n  childArgs               [extra args]\n...\nMain class:\norg.apache.spark.deploy.PythonRunner\nArguments:\nfile:/Users/jacek/dev/sandbox/python-sandbox/1.py\nnull\nextra\nargs\nSpark config:\n(spark.app.name,1.py)\n(spark.master,local[*])\n(spark.submit.pyFiles,)\n(spark.submit.deployMode,client)\n</code></pre>"},{"location":"demo/running-pyspark-application-on-minikube/","title":"Demo: Running PySpark Application on minikube","text":"<p>This demo shows how to run a PySpark application on Kubernetes (using minikube).</p> <p>Tip</p> <p>This is a follow-up demo to Demo: Running Spark Application on minikube in the The Internals of Spark on Kubernetes.</p>"},{"location":"features/","title":"Features","text":""},{"location":"ml/","title":"PySpark MLlib","text":"<p>PySpark MLlib is a Python module to work with Spark MLlib for <code>DataFrame</code>-based machine learning pipelines.</p> <pre><code>from pyspark.ml import *\n</code></pre>"},{"location":"ml/Distributor/","title":"Distributor","text":"<p><code>Distributor</code> is the parent (abstract) class of TorchDistributor.</p>"},{"location":"ml/Distributor/#creating-instance","title":"Creating Instance","text":"<p><code>Distributor</code> takes the following to be created:</p> <ul> <li> Number of processes (default: <code>1</code>) <li> <code>local_mode</code> flag (default: <code>True</code>) <li> <code>use_gpu</code> flag (default: <code>True</code>) <p>Abstract Class</p> <p><code>Distributor</code> is not supposed to be created directly.</p>"},{"location":"ml/Distributor/#_get_num_tasks","title":"_get_num_tasks","text":"<pre><code>_get_num_tasks(\n  self) -&gt; int\n</code></pre> <p><code>_get_num_tasks</code>...FIXME</p>"},{"location":"ml/Distributor/#get_gpus_owned","title":"get_gpus_owned","text":"<pre><code>get_gpus_owned(\n  context: Union[SparkContext, BarrierTaskContext]) -&gt; List[str]\n</code></pre> <p><code>get_gpus_owned</code>...FIXME</p>"},{"location":"pandas-on-spark/","title":"pandas API on Spark","text":"<p>pandas API on Spark (pyspark.pandas package) has been added to PySpark to execute pandas code on Spark clusters with no changes (except the import).</p> <p>There are two related PySpark packages with pandas support:</p> <ul> <li>pyspark.pandas</li> <li>pyspark.sql.pandas</li> </ul> <p>Spark Structured Streaming</p> <p>pandas API on Spark does not support Spark Structured Streaming (streaming queries).</p>"},{"location":"pandas-on-spark/#modules","title":"Modules","text":"<p>pandas API on Spark requires that the following modules to be installed:</p> Module Version pandas 1.0.5 PyArrow 1.0.0"},{"location":"pandas-on-spark/#PYARROW_IGNORE_TIMEZONE","title":"PYARROW_IGNORE_TIMEZONE","text":"<p>For PyArrow 2.0.0 and above, pandas API on Spark requires <code>PYARROW_IGNORE_TIMEZONE</code> environment variable to be set to <code>1</code> (on the driver and executors).</p>"},{"location":"pandas-on-spark/#PYSPARK_PANDAS_USAGE_LOGGER","title":"PYSPARK_PANDAS_USAGE_LOGGER <p>pandas API on Spark uses <code>PYSPARK_PANDAS_USAGE_LOGGER</code> (formerly <code>KOALAS_USAGE_LOGGER</code>) environment variable for a usage logger.</p>","text":""},{"location":"pandas-on-spark/#demo","title":"Demo <pre><code># The following would be required if we used pandas\n# import pandas as pd\n\n# but we don't need it anymore \ud83d\ude0a\n\n# The only change is supposed to be this extra `pyspark` prefix\n# in the name of the package\n\nimport pyspark.pandas as pd\n</code></pre> Python   <pre><code>pd.read_csv(\"people.csv\")\n</code></pre>    <pre><code>   id  name\n0   0  zero\n1   1   one\n2   2   two\n</code></pre>","text":""},{"location":"pandas-udfs/","title":"pandas User-Defined Functions","text":"<p>pandas User-Defined Functions (Vectorized User-Defined Functions or pandas UDFs) are user-defined functions that are executed using Apache Arrow to transfer data and pandas to work with the data, which allows for vectorized operations.</p> <p>Pandas UDFs are defined using @pandas_udf decorator.</p> <p>A Pandas UDF behaves as a regular PySpark function API in general.</p> <p>As of Spark 3.0.0 (SPARK-28264), using Python type hints in pandas UDF is encouraged (instead of specifying pandas UDF type via functionType argument).</p> <p>The return type (type hint) of a user-defined function should be as follows:</p> <ul> <li><code>pandas.Series</code> (pandas) in most cases</li> <li><code>pandas.DataFrame</code> (pandas) for <code>struct</code> input or output</li> </ul>"},{"location":"pandas-udfs/#pandas_udf","title":"@pandas_udf Decorator","text":"<pre><code>pandas_udf(\n  f=None,\n  returnType=None,\n  functionType=None)\n</code></pre> <p>pandas_udf function is used a decorator (using <code>@pandas_udf</code> annotation).</p> Python Decorators <p>Learn more in PEP 318 \u2013 Decorators for Functions and Methods.</p> <p><code>pandas_udf</code> belongs to pyspark.sql.functions module.</p> <pre><code>from pyspark.sql.functions import pandas_udf\n</code></pre>"},{"location":"pandas-udfs/#functionType","title":"functionType","text":"<p><code>functionType</code> can be one of PandasUDFTypes (but is currently discouraged in favour of type hints).</p> <pre><code>@pandas_udf(returnType = \"long\", functionType = PandasUDFType.GROUPED_AGG)\ndef my_udaf(names: pd.Series) -&gt; 'long':\n  return pd.Series(names.count())\n</code></pre> <p><code>functionType</code> is also known as <code>evalType</code>.</p> <p>SQL_SCALAR_PANDAS_UDF is the default scalar UDF type.</p>"},{"location":"pandas-udfs/#returnType","title":"returnType","text":"<p><code>@pandas_udf</code> decorator can optionally specify a return type (as the first positional argument or using <code>returnType</code>).</p> <p>A return type can be one of the names of <code>pyspark.sql.types.DataType</code> instances or the <code>DataType</code> themselves.</p> <pre><code>@pandas_udf(dataType)\n@pandas_udf(returnType=dataType)\n</code></pre>"},{"location":"pandas-udfs/#group-aggregate","title":"Group Aggregate pandas UDFs","text":"<p>pandas UDFs can be used as aggregation functions using GroupedData.agg operator with the following caveats:</p> <ol> <li>There is no partial aggregation with group aggregate UDFs (i.e., a full shuffle is required).</li> <li>All the data of a group will be loaded into memory, so there is a potential OOM risk if data is skewed and certain groups are too large to fit in memory</li> </ol> <p>Note</p> <p>Group aggregate pandas UDFs and built-in aggregation functions cannot be mixed in a single GroupedData.agg operator. Otherwise, the following <code>AnalysisException</code> is thrown:</p> <pre><code>[INVALID_PANDAS_UDF_PLACEMENT] The group aggregate pandas UDF `my_udaf` cannot be invoked together with as other, non-pandas aggregate functions.\n</code></pre>"},{"location":"pandas-udfs/#examples","title":"Examples","text":"<pre><code>import pandas as pd\nfrom pyspark.sql.functions import pandas_udf\n</code></pre> <pre><code>@pandas_udf(\"string\")\ndef to_upper(s: pd.Series) -&gt; pd.Series:\n    return s.str.upper()\n</code></pre> <pre><code>@pandas_udf(\"string\")\ndef my_concat(names: pd.Series, ages: pd.Series) -&gt; pd.Series:\n    return pd.Series([f\"{n} is {a} years old\" for (n, a) in zip(names, ages)])\n</code></pre> <pre><code>pandas_df = pd.DataFrame({\n  'name': ['jacek', 'agata', 'iweta', 'patryk', 'maksym'],\n  'age': [50, 49, 29, 26, 11]\n  })\ndf = spark.createDataFrame(pandas_df)\n</code></pre> <pre><code>&gt;&gt;&gt; df.show()\n+------+---+\n|  name|age|\n+------+---+\n| jacek| 50|\n| agata| 49|\n| iweta| 29|\n|patryk| 26|\n|maksym| 11|\n+------+---+\n</code></pre> <pre><code>&gt;&gt;&gt; df.printSchema()\nroot\n |-- name: string (nullable = true)\n |-- age: long (nullable = true)\n</code></pre> <pre><code>df.select(to_upper(df.name).alias(\"upper_name\")).show()\n</code></pre> <pre><code>+----------+\n|upper_name|\n+----------+\n|     JACEK|\n|     AGATA|\n|     IWETA|\n|    PATRYK|\n|    MAKSYM|\n+----------+\n</code></pre> Python <pre><code>df.select(my_concat(df.name, df.age)).show(truncate = False)\n</code></pre> <pre><code>+----------------------+\n|my_concat(name, age)  |\n+----------------------+\n|jacek is 50 years old |\n|agata is 49 years old |\n|iweta is 29 years old |\n|patryk is 26 years old|\n|maksym is 11 years old|\n+----------------------+\n</code></pre>"},{"location":"pandas-udfs/#group-aggregate-pandas-udf","title":"Group Aggregate pandas UDF","text":"<pre><code>@pandas_udf(returnType = \"long\")\ndef my_count(s: pd.Series) -&gt; 'long':\n    return pd.Series(s.count())\n</code></pre> <pre><code>from pyspark.sql.functions import abs\ngrouped_nums = (nums\n    .withColumn(\"gid\", abs((nums.value * 100) % 2))\n    .groupBy(\"gid\"))\ncount_by_gid_agg = my_count(\"gid\").alias(\"count\")\ncounts_by_gid = grouped_nums.agg(count_by_gid_agg)\n</code></pre>"},{"location":"pyspark/java_gateway/","title":"java_gateway.py","text":"<p><code>java_gateway</code> is a Python module that allows launching a gateway process to establish communication channel to Py4JServer.</p>"},{"location":"pyspark/java_gateway/#launch_gateway","title":"launch_gateway <pre><code>launch_gateway(\n  conf=None,\n  popen_kwargs=None)\n</code></pre> <p><code>launch_gateway</code> reads PYSPARK_GATEWAY_PORT and PYSPARK_GATEWAY_SECRET environment variables if defined and assumes that the child Java gateway process has already been started (e.g. PythonGatewayServer).</p> <p> <p>Otherwise, <code>launch_gateway</code> builds the command to start <code>spark-submit</code>:</p> <ol> <li>Finds <code>SPARK_HOME</code> with <code>./bin/spark-submit</code></li> <li>Appends all the configuration properties (from the input <code>conf</code>) using <code>--conf</code></li> <li>Appends <code>PYSPARK_SUBMIT_ARGS</code> environment variable if defined or assumes <code>pyspark-shell</code></li> </ol> <p><code>launch_gateway</code> sets up <code>_PYSPARK_DRIVER_CONN_INFO_PATH</code> environment variable to point at an unique temporary file.</p> <p><code>launch_gateway</code> configures a pipe to stdin for the corresponding Java gateway process to use to monitor the Python process.</p> <p><code>launch_gateway</code> starts <code>bin/spark-submit</code> command and waits for a connection info file to be created at <code>_PYSPARK_DRIVER_CONN_INFO_PATH</code>. <code>launch_gateway</code> reads the port and the secret from the file once available.</p> <p><code>launch_gateway</code> connects to the gateway using py4j's <code>ClientServer</code> or <code>JavaGateway</code> based on PYSPARK_PIN_THREAD environment variable.</p> <p><code>launch_gateway</code> imports Spark packages and classes (using py4j):</p> <ul> <li><code>org.apache.spark.SparkConf</code></li> <li><code>org.apache.spark.api.java.*</code></li> <li><code>org.apache.spark.api.python.*</code></li> <li><code>org.apache.spark.ml.python.*</code></li> <li><code>org.apache.spark.mllib.api.python.*</code></li> <li><code>org.apache.spark.resource.*</code></li> <li><code>org.apache.spark.sql.*</code></li> <li><code>org.apache.spark.sql.api.python.*</code></li> <li><code>org.apache.spark.sql.hive.*</code></li> <li><code>scala.Tuple2</code></li> </ul> <p><code>launch_gateway</code>\u00a0is used when:</p> <ul> <li><code>SparkContext</code> is requested to _ensure_initialized</li> </ul>","text":""},{"location":"pyspark/shell/","title":"shell.py","text":"<p><code>shell.py</code> script is the interactive shell of PySpark.</p> <p><code>shell.py</code> defines the following variables:</p> <ul> <li><code>sc</code> being pyspark.SparkContext</li> <li><code>spark</code> being pyspark.sql.session.SparkSession</li> <li><code>sql</code> being SparkSession.sql</li> <li><code>sqlContext</code> and <code>sqlCtx</code> for compatibility</li> </ul>"},{"location":"pyspark/pandas/","title":"pyspark.pandas Package","text":"<p>When imported (that triggers <code>__init__.py</code>), <code>pyspark.pandas</code> does monkey-patching of <code>pandas.DataFrame</code> and <code>pandas.Series</code> classes (using __class_getitem__ dunder method).</p> Pandas PySpark pandas.DataFrame <code>pyspark.pandas.frame.DataFrame</code> pandas.Series <code>pyspark.pandas.series.Series</code>"},{"location":"pyspark/pandas/DataFrame/","title":"DataFrame","text":"<p><code>DataFrame</code> is a Frame with an InternalFrame.</p> <p><code>DataFrame</code> is a <code>Generic[T]</code> (Python).</p>"},{"location":"pyspark/pandas/DataFrame/#creating-instance","title":"Creating Instance","text":"<p><code>DataFrame</code> takes the following to be created:</p> <ul> <li> data (optional) <li> index (optional) <li> columns (optional) <li> dtype (optional) <li> copy (optional)"},{"location":"pyspark/pandas/DataFrame/#_internal_frame","title":"_internal_frame","text":"<p><code>DataFrame</code> is given or creates an InternalFrame when created.</p> <pre><code>object.__setattr__(self, \"_internal_frame\", internal)\n</code></pre>"},{"location":"pyspark/pandas/DataFrame/#_internal","title":"InternalFrame","text":"Frame <pre><code>@property\ndef _internal(\n    self) -&gt; InternalFrame\n</code></pre> <p><code>_internal</code> is part of the Frame abstraction.</p> <p><code>_internal</code> returns the _internal_frame (that is expected to be of type InternalFrame).</p>"},{"location":"pyspark/pandas/InternalFrame/","title":"InternalFrame","text":"<p><code>InternalFrame</code> is the underlying managed Spark DataFrame of pyspark.pandas.DataFrame.</p>"},{"location":"pyspark/pandas/InternalFrame/#creating-instance","title":"Creating Instance","text":"<p><code>InternalFrame</code> takes the following to be created:</p> <ul> <li>Spark DataFrame</li> <li> <code>index_spark_columns</code> (optional) <li> <code>index_names</code> (optional) <li> <code>index_fields</code> (optional) <li> <code>column_labels</code> (optional) <li> <code>data_spark_columns</code> (optional) <li> <code>data_fields</code> (optional) <li> <code>column_label_names</code> (optional)"},{"location":"pyspark/pandas/InternalFrame/#spark_frame","title":"Spark DataFrame","text":"<p><code>InternalFrame</code> is given a Spark DataFrame when created.</p>"},{"location":"pyspark/pandas/InternalFrame/#_sdf","title":"Managed Spark DataFrame","text":"<p><code>_sdf</code> is the underlying managed Spark DataFrame.</p> <p><code>_sdf</code> is the Spark DataFrame with attach_default_index and __natural_order__ columns selected.</p>"},{"location":"pyspark/pandas/InternalFrame/#SPARK_DEFAULT_INDEX_NAME","title":"Default Index Column Name","text":"<p><code>InternalFrame</code> uses the following as the name of the default index column:</p> <pre><code>__index_level_0__\n</code></pre>"},{"location":"pyspark/pandas/InternalFrame/#SPARK_INDEX_NAME_PATTERN","title":"Index Column Pattern","text":"<p><code>InternalFrame</code> defines a regular pattern to match the index columns.</p> <pre><code>__index_level_[0-9]+__\n</code></pre> <p>It is invalid to name columns in the Spark DataFrame to match the index column pattern. Index columns must not be in the columns of the Spark DataFrame.</p>"},{"location":"pyspark/pandas/InternalFrame/#to_internal_spark_frame","title":"to_internal_spark_frame","text":"<pre><code>@lazy_property\ndef to_internal_spark_frame(\n    self) -&gt; SparkDataFrame\n</code></pre> <p><code>to_internal_spark_frame</code> returns the spark_frame with the index_spark_columns followed by the data_spark_columns.</p>"},{"location":"pyspark/pandas/InternalFrame/#spark_frame","title":"spark_frame","text":"<pre><code>from pyspark.sql import DataFrame as SparkDataFrame\n\n@property\ndef spark_frame(\n    self) -&gt; SparkDataFrame\n</code></pre> <p><code>spark_frame</code> returns the underlying managed Spark DataFrame.</p>"},{"location":"pyspark/pandas/InternalFrame/#demo","title":"Demo","text":"<pre><code>from pyspark import pandas as ps\n\npsdf = ps.DataFrame({\n    'A': [1, 2, 3, 4],\n    'B': [5, 6, 7, 8],\n    'C': [9, 10, 11, 12],\n    'D': [13, 14, 15, 16],\n    'E': [17, 18, 19, 20]}, columns = ['A', 'B', 'C', 'D', 'E'])\n\npsdf._internal\n# &lt;pyspark.pandas.internal.InternalFrame object at 0x7f7ff024f820&gt;\n\npsdf._internal.spark_frame\n# DataFrame[__index_level_0__: bigint, A: bigint, B: bigint, C: bigint, D: bigint, E: bigint, __natural_order__: bigint]\n\npsdf._internal.spark_frame.show()\n# +-----------------+---+---+---+---+---+-----------------+\n# |__index_level_0__|  A|  B|  C|  D|  E|__natural_order__|\n# +-----------------+---+---+---+---+---+-----------------+\n# |                0|  1|  5|  9| 13| 17|      17179869184|\n# |                1|  2|  6| 10| 14| 18|      42949672960|\n# |                2|  3|  7| 11| 15| 19|      68719476736|\n# |                3|  4|  8| 12| 16| 20|      94489280512|\n# +-----------------+---+---+---+---+---+-----------------+\n\npsdf._internal.to_internal_spark_frame.show()\n# +-----------------+---+---+---+---+---+\n# |__index_level_0__|  A|  B|  C|  D|  E|\n# +-----------------+---+---+---+---+---+\n# |                0|  1|  5|  9| 13| 17|\n# |                1|  2|  6| 10| 14| 18|\n# |                2|  3|  7| 11| 15| 19|\n# |                3|  4|  8| 12| 16| 20|\n# +-----------------+---+---+---+---+---+\n</code></pre>"},{"location":"pyspark/pandas/generic/","title":"pyspark.pandas.generic Package","text":"<p><code>pyspark.pandas.generic</code> package is...FIXME</p>"},{"location":"pyspark/pandas/generic/Frame/","title":"Frame","text":"<p><code>Frame</code> is an abstraction of frames that behave like pandas.DataFrame and pandas.Series.</p> <pre><code>class Frame(object, metaclass=ABCMeta)\n</code></pre>"},{"location":"pyspark/pandas/generic/Frame/#contract","title":"Contract","text":""},{"location":"pyspark/pandas/generic/Frame/#__getitem","title":"__getitem__ <pre><code>@abstractmethod\ndef __getitem__(\n  self,\n  key: Any) -&gt; Any\n</code></pre> <pre><code>class hello():\n  def __getitem__(self, key):\n    print(f\"__getitem__({key})\")\n\nh = hello()\n\n&gt;&gt;&gt; h[4]\n__getitem__(4)\n</code></pre>","text":""},{"location":"pyspark/pandas/generic/Frame/#_internal","title":"_internal <pre><code>@property\n@abstractmethod\ndef _internal(\n  self) -&gt; InternalFrame\n</code></pre>","text":""},{"location":"pyspark/pandas/generic/Frame/#implementations","title":"Implementations","text":"<ul> <li>DataFrame</li> <li><code>Series</code></li> </ul>"},{"location":"pyspark/sql/functions/","title":"functions.py","text":"<p><code>functions.py</code> module belongs to <code>pyspark.sql</code> package.</p>"},{"location":"pyspark/sql/functions/#udf","title":"udf <pre><code>udf(f=None, returnType=StringType())\n</code></pre> <p><code>udf</code>...FIXME</p>","text":""},{"location":"pyspark/sql/udf/","title":"udf.py","text":"<p><code>udf.py</code> module belongs to <code>pyspark.sql</code> package.</p>"},{"location":"pyspark/sql/udf/#public-objects","title":"Public Objects <p><code>udf</code> module defines UDFRegistration as the public object.</p>","text":""},{"location":"pyspark/sql/udf/#_create_udf","title":"_create_udf <pre><code>_create_udf(f, returnType, evalType)\n</code></pre> <p><code>_create_udf</code> creates an UserDefinedFunction (with the name of the object to be the name of function <code>f</code>).</p> <p><code>_create_udf</code>\u00a0is used when:</p> <ul> <li>pandas_udf function is used</li> <li>udf function is used</li> </ul>","text":""},{"location":"pyspark/sql/pandas/","title":"pyspark.sql.pandas Package","text":"<p><code>pyspark.sql.pandas</code> package is...FIXME</p>"},{"location":"pyspark/sql/pandas/PandasUDFType/","title":"PandasUDFType","text":"<p>Deprecation Notice</p> <p>As of PySpark 3.0.0, <code>PandasUDFType</code> is deprecated in favour of Python type hints.</p> <p><code>PandasUDFType</code> is the <code>functionType</code> of pandas_udf for Python methods to be used as pandas UDFs (with the types matching PythonEvalType on the JVM/Scala side).</p> PandasUDFType PythonEvalType <code>GROUPED_AGG</code> SQL_GROUPED_AGG_PANDAS_UDF <code>GROUPED_MAP</code> SQL_GROUPED_MAP_PANDAS_UDF <code>SCALAR</code> SQL_SCALAR_PANDAS_UDF <code>SCALAR_ITER</code> SQL_SCALAR_PANDAS_ITER_UDF"},{"location":"pyspark/sql/pandas/functions/","title":"functions.py","text":"<p><code>functions.py</code> defines pandas_udf for pandas user-defined function.</p> <p><code>functions.py</code> is part of <code>pyspark.sql.pandas</code> package.</p> <pre><code>from pyspark.sql.functions import pandas_udf\n</code></pre>"},{"location":"pyspark/sql/pandas/functions/#pandas_udf","title":"pandas_udf","text":"<pre><code>pandas_udf(\n  f=None,\n  returnType=None,\n  functionType=None)\n</code></pre> <p><code>pandas_udf</code> creates a pandas user-defined function.</p> <p><code>pandas_udf</code> _create_pandas_udf (possibly creating a partial function with <code>functools.partial</code> (Python) when used as a decorator).</p>"},{"location":"pyspark/sql/pandas/functions/#pandas_udf_decorator","title":"Decorator","text":"<p><code>pandas_udf</code> can and usually is used as a Python decorator with two positional arguments for the return and function types.</p> <pre><code>@pandas_udf(returnType, functionType)\n</code></pre>"},{"location":"pyspark/sql/pandas/functions/#pandas_udf_returnType","title":"returnType","text":"<p><code>returnType</code> can be one of the following:</p> <ul> <li><code>pyspark.sql.types.DataType</code></li> <li>A DDL-formatted type string</li> </ul>"},{"location":"pyspark/sql/pandas/functions/#pandas_udf_functionType","title":"functionType","text":"<p><code>functionType</code> must be one the values from <code>PandasUDFType</code>:</p> <ul> <li>SQL_SCALAR_PANDAS_UDF</li> <li>SQL_SCALAR_PANDAS_ITER_UDF</li> <li>SQL_GROUPED_MAP_PANDAS_UDF</li> <li>SQL_GROUPED_AGG_PANDAS_UDF</li> <li>SQL_MAP_PANDAS_ITER_UDF</li> <li>SQL_COGROUPED_MAP_PANDAS_UDF</li> </ul>"},{"location":"pyspark/sql/pandas/functions/#_create_pandas_udf","title":"_create_pandas_udf","text":"<pre><code>_create_pandas_udf(\n  f,\n  returnType,\n  evalType)\n</code></pre> <p><code>_create_pandas_udf</code>...FIXME</p>"},{"location":"pytorch-distributed/","title":"Distributed Training using PyTorch","text":"<p>PySpark 3.4.0 introduces TorchDistributor for distributed training on Apache Spark clusters using PyTorch Distributed.</p>"},{"location":"pytorch-distributed/#learn-more","title":"Learn More","text":"<ol> <li>Distributed training with TorchDistributor</li> </ol>"},{"location":"pytorch-distributed/TorchDistributor/","title":"TorchDistributor","text":"<p><code>TorchDistributor</code> is a Distributor to run PyTorch's torch.distributed.run module on Apache Spark clusters.</p> <p><code>TorchDistributor</code> is a PySpark translation of torchrun (from Torch Distributed Elastic).</p>"},{"location":"pytorch-distributed/TorchDistributor/#demo","title":"Demo","text":"<pre><code>from pyspark.ml.torch.distributor import TorchDistributor\n\ndistributor = TorchDistributor(\n    num_processes=1,\n    local_mode=False,\n    use_gpu=False)\n</code></pre> <pre><code># Use a path to a training script\n# and variable-length kwargs\ndistributor.run(\n    \"train.py\",\n    \"--learning-rate=1e-3\",\n    \"--batch-size=64\",\n    \"--my-key=my-value\")\n\n# Started local training with 1 processes\n# NOTE: Redirects are currently not supported in Windows or MacOs.\n# Finished local training with 1 processes\n</code></pre> <pre><code># Use a Callable (function)\n# The number of positional arguments is the number of kwargs\ndef train(a, b, c):\n    print(f\"Got a={a}, b={b}, c={c}\")\n    return 'success'\n\ndistributor.run(\n    train,\n    \"--learning-rate=1e-3\",\n    \"--batch-size=64\",\n    \"--my-key=my-value\")\n\n# Started distributed training with 1 executor proceses\n# NOTE: Redirects are currently not supported in Windows or MacOs.    (0 + 1) / 1]\n# NOTE: Redirects are currently not supported in Windows or MacOs.\n# Got a=--learning-rate=1e-3, b=--batch-size=64, c=--my-key=my-value\n# Got a=--learning-rate=1e-3, b=--batch-size=64, c=--my-key=my-value\n# Finished distributed training with 1 executor proceses\n# 'success'\n</code></pre>"},{"location":"pytorch-distributed/TorchDistributor/#run","title":"Running Distributed Training","text":"<pre><code>run(\n    self,\n    train_object: Union[Callable, str],\n    *args: Any) -&gt; Optional[Any]\n</code></pre> <p><code>run</code> determines what to run (e.g., a function or a script based on the given <code>train_object</code>).</p> <ul> <li>With a function, <code>run</code> uses _run_training_on_pytorch_function</li> <li>With a script, <code>run</code> uses _run_training_on_pytorch_file</li> </ul> <p>In the end, <code>run</code> runs a local or distributed training.</p> <ul> <li>In local mode, <code>run</code> runs local training</li> <li>In non-local mode, <code>run</code> runs distributed training</li> </ul>"},{"location":"pytorch-distributed/TorchDistributor/#_run_local_training","title":"Local Training","text":"<pre><code>_run_local_training(\n    self,\n    framework_wrapper_fn: Callable,\n    train_object: Union[Callable, str],\n    *args: Any,\n) -&gt; Optional[Any]\n</code></pre> <p><code>_run_local_training</code> looks up <code>CUDA_VISIBLE_DEVICES</code> among the environment variables.</p> <p>With use_gpu, <code>_run_local_training</code>...FIXME</p> <p><code>_run_local_training</code> prints out the following INFO message to the logs:</p> <pre><code>Started local training with [num_processes] processes\n</code></pre> <p><code>_run_local_training</code> executes the given <code>framework_wrapper_fn</code> function (with the input_params, the given <code>train_object</code> and the <code>args</code>).</p> <p>In the end, <code>_run_local_training</code> prints out the following INFO message to the logs:</p> <pre><code>Finished local training with [num_processes] processes\n</code></pre>"},{"location":"pytorch-distributed/TorchDistributor/#_run_distributed_training","title":"Distributed Training","text":"<pre><code>_run_distributed_training(\n    self,\n    framework_wrapper_fn: Callable,\n    train_object: Union[Callable, str],\n    *args: Any,\n) -&gt; Optional[Any]\n</code></pre> <p><code>_run_distributed_training</code>...FIXME</p>"},{"location":"pytorch-distributed/TorchDistributor/#_run_training_on_pytorch_function","title":"_run_training_on_pytorch_function","text":"<pre><code>_run_training_on_pytorch_function(\n    input_params: Dict[str, Any],\n    train_fn: Callable,\n    *args: Any\n) -&gt; Any\n</code></pre> <p><code>_run_training_on_pytorch_function</code> prepares train and output files.</p> <p><code>_run_training_on_pytorch_function</code>...FIXME</p>"},{"location":"pytorch-distributed/TorchDistributor/#_setup_files","title":"Setting Up Files","text":"<pre><code># @contextmanager\n_setup_files(\n    train_fn: Callable,\n    *args: Any\n) -&gt; Generator[Tuple[str, str], None, None]\n</code></pre> <p><code>_setup_files</code> gives the paths of a TorchRun train file and <code>output.pickle</code> output file.</p> <p><code>_setup_files</code> creates a save directory.</p> <p><code>_setup_files</code> saves train_fn function to the save directory (that gives a <code>pickle_file_path</code>).</p> <p><code>_setup_files</code> uses the save directory and <code>output.pickle</code> name for the output file path.</p> <p><code>_setup_files</code> creates a torchrun_train_file with the following:</p> <ul> <li>Save directory</li> <li><code>pickle_file_path</code></li> <li><code>output.pickle</code> output file path</li> </ul> <p>In the end, <code>_setup_files</code> yields (gives) the <code>torchrun_train_file</code> and the <code>output.pickle</code> output file path.</p>"},{"location":"pytorch-distributed/TorchDistributor/#_create_torchrun_train_file","title":"Creating TorchRun Train File","text":"<pre><code>_create_torchrun_train_file(\n    save_dir_path: str,\n    pickle_file_path: str,\n    output_file_path: str\n) -&gt; str\n</code></pre> <p><code>_create_torchrun_train_file</code> creates <code>train.py</code> in the given <code>save_dir_path</code> with the following content (based on the given <code>pickle_file_path</code> and the <code>output_file_path</code>):</p> <pre><code>import cloudpickle\nimport os\n\nif __name__ == \"__main__\":\n    with open(\"[pickle_file_path]\", \"rb\") as f:\n        train_fn, args = cloudpickle.load(f)\n    output = train_fn(*args)\n    with open(\"[output_file_path]\", \"wb\") as f:\n        cloudpickle.dump(output, f)\n</code></pre>"},{"location":"pytorch-distributed/TorchDistributor/#_run_training_on_pytorch_file","title":"_run_training_on_pytorch_file","text":"<pre><code>_run_training_on_pytorch_file(\n    input_params: Dict[str, Any],\n    train_path: str,\n    *args: Any\n) -&gt; None\n</code></pre> <p><code>_run_training_on_pytorch_file</code> looks up the <code>log_streaming_client</code> in the given <code>input_params</code> (or assumes <code>None</code>).</p> <p>FIXME What's log_streaming_client?</p> <p><code>_run_training_on_pytorch_file</code> creates torchrun command.</p> <p><code>_run_training_on_pytorch_file</code> executes the command.</p>"},{"location":"pytorch-distributed/TorchDistributor/#_create_torchrun_command","title":"_create_torchrun_command","text":"<pre><code>_create_torchrun_command(\n    input_params: Dict[str, Any],\n    path_to_train_file: str,\n    *args: Any\n) -&gt; List[str]\n</code></pre> <p><code>_create_torchrun_command</code> takes the value of the following parameters (from the given <code>input_params</code>):</p> <ul> <li><code>local_mode</code></li> <li><code>num_processes</code></li> </ul> <p><code>_create_torchrun_command</code> determines the <code>torchrun_args</code> and <code>processes_per_node</code> based on <code>local_mode</code>.</p> local_mode torchrun_args processes_per_node <code>True</code> <ul><li><code>--standalone</code><li><code>--nnodes=1</code> <code>num_processes</code>(from the given <code>input_params</code>) <code>False</code> <ul><li><code>--nnodes=[num_processes]</code><li><code>--node_rank=[node_rank]</code><li><code>--rdzv_endpoint=[MASTER_ADDR]:[MASTER_PORT]</code><li><code>--rdzv_id=0</code> 1 <p>In the end, <code>_create_torchrun_command</code> returns a Python command to execute torch_run_process_wrapper module (<code>python -m</code>) with the following positional arguments:</p> <ul> <li><code>torchrun_args</code></li> <li><code>--nproc_per_node=[processes_per_node]</code></li> <li>The given <code>path_to_train_file</code></li> <li>The given <code>args</code></li> </ul>"},{"location":"pytorch-distributed/torch_run_process_wrapper/","title":"torch_run_process_wrapper Module","text":"<p><code>torch_run_process_wrapper</code> is used as the torchrun command in TorchDistributor.</p> <p><code>torch_run_process_wrapper</code> executes <code>torch.distributed.run</code> module (using <code>python -m</code>). <code>torch_run_process_wrapper</code> monitors the child process and prints out the output to the standard output.</p>"},{"location":"runners/ArrowPythonRunner/","title":"ArrowPythonRunner","text":"<p><code>ArrowPythonRunner</code> is a BasePythonRunner with <code>Iterator[InternalRow]</code> input and <code>ColumnarBatch</code> (vectorized) output.</p> <p><code>ArrowPythonRunner</code> supports <code>BasicPythonArrowInput</code> and BasicPythonArrowOutput.</p>"},{"location":"runners/ArrowPythonRunner/#creating-instance","title":"Creating Instance","text":"<p><code>ArrowPythonRunner</code> takes the following to be created:</p> <ul> <li> <code>ChainedPythonFunctions</code>es <li> Eval Type <li> Argument Offsets <li> <code>Schema</code> (Spark SQL) <li> TimeZone ID <li> Worker Configuration <li> Performance Metrics <p><code>ArrowPythonRunner</code> is created when the following physical operators (Spark SQL) are executed:</p> <ul> <li>AggregateInPandasExec</li> <li>ArrowEvalPythonExec</li> <li><code>FlatMapGroupsInPandasExec</code></li> <li><code>MapInPandasExec</code></li> <li><code>WindowInPandasExec</code></li> </ul>"},{"location":"runners/ArrowPythonRunner/#bufferSize","title":"bufferSize","text":"BasePythonRunner <pre><code>bufferSize: Int\n</code></pre> <p><code>bufferSize</code> is part of the BasePythonRunner abstraction.</p> <p><code>bufferSize</code> is the value of spark.sql.execution.pandas.udf.buffer.size configuration property.</p>"},{"location":"runners/ArrowPythonRunner/#simplifiedTraceback","title":"simplifiedTraceback","text":"BasePythonRunner <pre><code>simplifiedTraceback: Boolean\n</code></pre> <p><code>simplifiedTraceback</code> is part of the BasePythonRunner abstraction.</p> <p><code>simplifiedTraceback</code> is the value of spark.sql.execution.pyspark.udf.simplifiedTraceback.enabled configuration property.</p>"},{"location":"runners/BasePythonRunner/","title":"BasePythonRunner","text":"<p><code>BasePythonRunner</code> is an abstraction of Python Runners.</p> Scala Definition <p><code>BasePythonRunner</code> is a type constructor in Scala (generic class in Java) with the following definition:</p> <pre><code>abstract class BasePythonRunner[IN, OUT](...) {\n// ...\n}\n</code></pre> <p><code>BasePythonRunner</code> uses <code>IN</code> and <code>OUT</code> as the name of the types for the input and output values.</p>"},{"location":"runners/BasePythonRunner/#contract","title":"Contract","text":""},{"location":"runners/BasePythonRunner/#newReaderIterator","title":"newReaderIterator","text":"<pre><code>newReaderIterator(\nstream: DataInputStream,\nwriterThread: WriterThread,\nstartTime: Long,\nenv: SparkEnv,\nworker: Socket,\npid: Option[Int],\nreleasedOrClosed: AtomicBoolean,\ncontext: TaskContext): Iterator[OUT]\n</code></pre> <p>See:</p> <ul> <li>PythonRunner</li> <li>PythonUDFRunner</li> </ul> <p>Used when:</p> <ul> <li><code>BasePythonRunner</code> is requested to compute</li> </ul>"},{"location":"runners/BasePythonRunner/#newWriterThread","title":"newWriterThread","text":"<pre><code>newWriterThread(\nenv: SparkEnv,\nworker: Socket,\ninputIterator: Iterator[IN],\npartitionIndex: Int,\ncontext: TaskContext): WriterThread\n</code></pre> <p>See:</p> <ul> <li>PythonRunner</li> <li>PythonUDFRunner</li> </ul> <p>Used when:</p> <ul> <li><code>BasePythonRunner</code> is requested to compute</li> </ul>"},{"location":"runners/BasePythonRunner/#implementations","title":"Implementations","text":"<ul> <li><code>ApplyInPandasWithStatePythonRunner</code></li> <li>ArrowPythonRunner</li> <li><code>CoGroupedArrowPythonRunner</code></li> <li>PythonRunner</li> <li>PythonUDFRunner</li> </ul>"},{"location":"runners/BasePythonRunner/#creating-instance","title":"Creating Instance","text":"<p><code>BasePythonRunner</code> takes the following to be created:</p> <ul> <li> <code>ChainedPythonFunctions</code> <li> Eval Type <li> Argument Offsets <p><code>BasePythonRunner</code> requires that the number of ChainedPythonFunctions and Argument Offsets are the same.</p> <p>Abstract Class</p> <p><code>BasePythonRunner</code> is an abstract class and cannot be created directly. It is created indirectly for the concrete BasePythonRunners.</p>"},{"location":"runners/BasePythonRunner/#accumulator","title":"accumulator <pre><code>accumulator: PythonAccumulatorV2\n</code></pre> <p><code>BasePythonRunner</code> initializes a registry of a PythonAccumulatorV2 when created to be the accumulator of the head PythonFunction among the given ChainedPythonFunctions.</p> <p>The <code>PythonAccumulatorV2</code> is used when <code>ReaderIterator</code> is requested to handleEndOfDataSection (to update metrics).</p>","text":""},{"location":"runners/BasePythonRunner/#compute","title":"Computing Result","text":"<pre><code>compute(\ninputIterator: Iterator[IN],\npartitionIndex: Int,\ncontext: TaskContext): Iterator[OUT]\n</code></pre> <p><code>compute</code> uses the given <code>TaskContext</code> to look up the following local properties (if they were specified via <code>ResourceProfile</code>):</p> <ul> <li><code>resource.executor.cores</code></li> <li><code>resource.pyspark.memory</code></li> </ul> <p><code>compute</code> requests the <code>DiskBlockManager</code> for the local directories and creates a comma-separated list of them (<code>localdir</code>).</p> <p>Unless <code>spark.executorEnv.OMP_NUM_THREADS</code> is explicitly specified (in the SparkConf), <code>compute</code> sets <code>OMP_NUM_THREADS</code> (in the envVars) to be the value of<code>resource.executor.cores</code> (if defined).</p> <p><code>compute</code> sets the following in the envVars:</p> <ul> <li><code>SPARK_LOCAL_DIRS</code> as the local directories of the local <code>DiskBlockManager</code> (<code>localdir</code>)</li> </ul> <p><code>compute</code> can optionally define environment variables:</p> <ul> <li><code>SPARK_REUSE_WORKER</code> as <code>1</code> when <code>spark.python.worker.reuse</code> configuration property is enabled</li> <li><code>SPARK_SIMPLIFIED_TRACEBACK</code> as <code>1</code> when simplifiedTraceback is enabled</li> <li>others</li> </ul> <p><code>compute</code> requests the executor's <code>SparkEnv</code> to <code>createPythonWorker</code> (Spark Core) with the pythonExec and the envVars.</p> <p>SparkEnv.createPythonWorker</p> <p>When requested to create a new Python worker, <code>SparkEnv</code> creates a PythonWorkerFactory (unless already created for the <code>pythonExec</code> and the <code>envVars</code> pair) to create a worker process.</p> <p><code>compute</code> creates a new WriterThread (to feed the worker process input from the given <code>inputIterator</code>) and starts it.</p> <p><code>compute</code> creates and starts a <code>WriterMonitorThread</code>.</p> <p><code>compute</code> creates a <code>MonitorThread</code>.</p> <p><code>compute</code> creates a buffered <code>DataInputStream</code> to read from the worker (socket) output. <code>compute</code> uses the bufferSize.</p> <p>In the end, <code>compute</code> creates a new ReaderIterator to read lines from the Python worker's stdout (from the buffered <code>DataInputStream</code>).</p> <p><code>compute</code> is used when:</p> <ul> <li><code>PythonRDD</code> is requested to compute a partition</li> <li>AggregateInPandasExec, ArrowEvalPythonExec, <code>BatchEvalPythonExec</code>, <code>FlatMapCoGroupsInPandasExec</code>, <code>FlatMapGroupsInPandasExec</code> <code>MapInPandasExec</code>, <code>WindowInPandasExec</code> physical operators are executed</li> <li><code>PandasGroupUtils</code> is requested to <code>executePython</code></li> <li><code>PythonForeachWriter</code> is requested for the outputIterator</li> </ul>"},{"location":"runners/BasicPythonArrowOutput/","title":"BasicPythonArrowOutput","text":"<p><code>BasicPythonArrowOutput</code> is a marker extension of the PythonArrowOutput abstraction for vectorized outputs of BasePythonRunners that produce <code>ColumnarBatch</code>es (Spark SQL).</p>"},{"location":"runners/BasicPythonArrowOutput/#implementations","title":"Implementations","text":"<ul> <li>ArrowPythonRunner</li> <li><code>CoGroupedArrowPythonRunner</code></li> </ul>"},{"location":"runners/BasicPythonArrowOutput/#deserializeColumnarBatch","title":"Deserializing ColumnarBatch","text":"PythonArrowOutput <pre><code>deserializeColumnarBatch(\nbatch: ColumnarBatch,\nschema: StructType): ColumnarBatch\n</code></pre> <p><code>deserializeColumnarBatch</code> is part of the PythonArrowOutput abstraction.</p> <p><code>deserializeColumnarBatch</code> returns the given <code>ColumnarBatch</code> unchanged.</p>"},{"location":"runners/PythonArrowOutput/","title":"PythonArrowOutput","text":"<p><code>PythonArrowOutput</code> is an extension of the BasePythonRunner abstraction for vectorized (ColumnarBatch) runners.</p> Scala Definition <pre><code>trait PythonArrowOutput[OUT &lt;: AnyRef] {\nself: BasePythonRunner[_, OUT] =&gt;\n// ...\n}\n</code></pre>"},{"location":"runners/PythonArrowOutput/#contract","title":"Contract","text":""},{"location":"runners/PythonArrowOutput/#deserializeColumnarBatch","title":"Deserializing ColumnarBatch","text":"<pre><code>deserializeColumnarBatch(\nbatch: ColumnarBatch,\nschema: StructType): OUT\n</code></pre> <p>See:</p> <ul> <li>BasicPythonArrowOutput</li> </ul> <p>Used when:</p> <ul> <li><code>PythonArrowOutput</code> is requested to newReaderIterator (after a batch is loaded)</li> </ul>"},{"location":"runners/PythonArrowOutput/#pythonMetrics","title":"Performance Metrics","text":"<pre><code>pythonMetrics: Map[String, SQLMetric]\n</code></pre> <p><code>SQLMetric</code>s (Spark SQL):</p> <ul> <li><code>pythonNumRowsReceived</code></li> <li><code>pythonDataReceived</code></li> </ul> <p>Used when:</p> <ul> <li><code>PythonArrowOutput</code> is requested to newReaderIterator (after a batch is loaded)</li> </ul>"},{"location":"runners/PythonArrowOutput/#implementations","title":"Implementations","text":"<ul> <li><code>ApplyInPandasWithStatePythonRunner</code></li> <li>BasicPythonArrowOutput</li> </ul>"},{"location":"runners/PythonRunner/","title":"PythonRunner","text":"<p><code>PythonRunner</code> is a concrete BasePythonRunner.</p>"},{"location":"runners/PythonRunner/#creating-instance","title":"Creating Instance","text":"<p><code>PythonRunner</code> takes the following to be created:</p> <ul> <li> <code>ChainedPythonFunctions</code>es <p><code>PythonRunner</code> is created (indirectly using apply factory method) when:</p> <ul> <li><code>PythonRDD</code> is requested to compute a partition</li> <li><code>PythonForeachWriter</code> is requested for a PythonRunner</li> </ul>"},{"location":"runners/PythonRunner/#creating-pythonrunner","title":"Creating PythonRunner <pre><code>apply(\n  func: PythonFunction): PythonRunner\n</code></pre> <p><code>apply</code> simply creates a PythonRunner for the PythonFunction.</p>  <p><code>apply</code> is used when:</p> <ul> <li><code>PythonRDD</code> is requested to compute a partition</li> <li><code>PythonForeachWriter</code> is requested for a PythonRunner</li> </ul>","text":""},{"location":"runners/PythonUDFRunner/","title":"PythonUDFRunner","text":"<p><code>PythonUDFRunner</code> is...FIXME</p>"},{"location":"runners/ReaderIterator/","title":"ReaderIterator","text":"<p><code>ReaderIterator</code> is an extension of the <code>Iterator</code> (Scala) abstraction for iterators to read <code>OUT</code> values.</p> <pre><code>abstract class ReaderIterator(...)\nextends Iterator[OUT]\n</code></pre>"},{"location":"runners/ReaderIterator/#contract","title":"Contract","text":""},{"location":"runners/ReaderIterator/#read","title":"Reading Value","text":"<pre><code>read(): OUT\n</code></pre> <p>See:</p> <ul> <li>PythonArrowOutput</li> <li>PythonRunner</li> <li>PythonUDFRunner</li> </ul> <p>Used when:</p> <ul> <li><code>ReaderIterator</code> is requested to hasNext</li> </ul>"},{"location":"runners/ReaderIterator/#implementations","title":"Implementations","text":"<ul> <li>PythonArrowOutput</li> <li>PythonRunner</li> <li>PythonUDFRunner</li> </ul>"},{"location":"runners/ReaderIterator/#handleEndOfDataSection","title":"handleEndOfDataSection","text":"<pre><code>handleEndOfDataSection(): Unit\n</code></pre> <p><code>handleEndOfDataSection</code>...FIXME</p> <p><code>handleEndOfDataSection</code> is used when:</p> <ul> <li><code>PythonRunner</code> is requested to newReaderIterator</li> <li><code>PythonArrowOutput</code> is requested to newReaderIterator</li> <li><code>PythonUDFRunner</code> is requested to newReaderIterator</li> </ul>"},{"location":"sql/","title":"PySpark SQL","text":"<p>PySpark SQL is a Python module to work with Spark SQL.</p> <pre><code>from pyspark.sql import *\n</code></pre>"},{"location":"sql/AggregateInPandasExec/","title":"AggregateInPandasExec Physical Operator","text":"<p><code>AggregateInPandasExec</code> is a unary physical operator (Spark SQL).</p>"},{"location":"sql/AggregateInPandasExec/#creating-instance","title":"Creating Instance","text":"<p><code>AggregateInPandasExec</code> takes the following to be created:</p> <ul> <li> Grouping Expressions (Spark SQL) (<code>Seq[NamedExpression]</code>) <li> PythonUDFs <li> Result Named Expressions (Spark SQL) (<code>Seq[NamedExpression]</code>) <li> Child Physical Operator (Spark SQL) <p><code>AggregateInPandasExec</code> is created when <code>Aggregation</code> execution planning strategy (Spark SQL) is executed for <code>Aggregate</code> logical operators (Spark SQL) with PythonUDF aggregate expressions only.</p>"},{"location":"sql/AggregateInPandasExec/#executing-operator","title":"Executing Operator <pre><code>doExecute(): RDD[InternalRow]\n</code></pre> <p><code>doExecute</code> uses ArrowPythonRunner (one per partition) to execute PythonUDFs.</p> <p><code>doExecute</code> is part of the <code>SparkPlan</code> (Spark SQL) abstraction.</p>","text":""},{"location":"sql/ArrowEvalPython/","title":"ArrowEvalPython Logical Operator","text":"<p><code>ArrowEvalPython</code> is a BaseEvalPython unary logical operator that evaluates scalar PythonUDFs with Apache Arrow.</p> <p><code>ArrowEvalPython</code> is planned as ArrowEvalPythonExec physical operator.</p>"},{"location":"sql/ArrowEvalPython/#creating-instance","title":"Creating Instance","text":"<p><code>ArrowEvalPython</code> takes the following to be created:</p> <ul> <li> Scalar PythonUDFs <li> Result <code>Attribute</code>s (Spark SQL) <li> Child <code>LogicalPlan</code> (Spark SQL) <li>Eval Type</li> <p><code>ArrowEvalPython</code> is created when:</p> <ul> <li><code>ExtractPythonUDFs</code> logical optimization is executed (and requested to extract scalar PythonUDFs from a logical query plan)</li> </ul>"},{"location":"sql/ArrowEvalPython/#evalType","title":"evalType","text":"<pre><code>evalType: Int\n</code></pre> <p><code>ArrowEvalPython</code> is given an <code>evalType</code> when created that can only be one of the following:</p> <ul> <li>SQL_SCALAR_PANDAS_UDF</li> <li>SQL_SCALAR_PANDAS_ITER_UDF</li> </ul>"},{"location":"sql/ArrowEvalPythonExec/","title":"ArrowEvalPythonExec Physical Operator","text":"<p><code>ArrowEvalPythonExec</code> is an EvalPythonExec physical operator to evaluate scalar PythonUDFs using ArrowPythonRunner.</p> <p><code>ArrowEvalPythonExec</code> represents ArrowEvalPython logical operator at execution time.</p>"},{"location":"sql/ArrowEvalPythonExec/#creating-instance","title":"Creating Instance","text":"<p><code>ArrowEvalPythonExec</code> takes the following to be created:</p> <ul> <li> Scalar PythonUDFs <li> Result <code>Attribute</code>s (Spark SQL) <li> Child <code>SparkPlan</code> (Spark SQL) <li> Eval Type <p><code>ArrowEvalPythonExec</code> is created when:</p> <ul> <li><code>PythonEvals</code> physical execution strategy is executed (and plans ArrowEvalPython logical operators)</li> </ul>"},{"location":"sql/ArrowEvalPythonExec/#performance-metrics","title":"Performance Metrics","text":"<p><code>ArrowEvalPythonExec</code> is a PythonSQLMetrics.</p>"},{"location":"sql/ArrowEvalPythonExec/#batchSize","title":"Maximum Records per Batch","text":"<p><code>batchSize</code> is the value of spark.sql.execution.arrow.maxRecordsPerBatch configuration property.</p> <p><code>batchSize</code> is used while evaluating PythonUDFs.</p>"},{"location":"sql/ArrowEvalPythonExec/#evaluate","title":"Evaluating PythonUDFs","text":"EvalPythonExec <pre><code>evaluate(\nfuncs: Seq[ChainedPythonFunctions],\nargOffsets: Array[Array[Int]],\niter: Iterator[InternalRow],\nschema: StructType,\ncontext: TaskContext): Iterator[InternalRow]\n</code></pre> <p><code>evaluate</code> is part of the EvalPythonExec abstraction.</p> <p><code>evaluate</code> creates an ArrowPythonRunner to compute partitions.</p> <p>In the end, <code>evaluate</code> converts <code>ColumnarBatch</code>es into <code>InternalRow</code>s.</p>"},{"location":"sql/BaseEvalPython/","title":"BaseEvalPython","text":"<p><code>BaseEvalPython</code> is...FIXME</p>"},{"location":"sql/DataFrame/","title":"DataFrame","text":"<p><code>DataFrame</code> is a Python class with PandasMapOpsMixin and PandasConversionMixin mixins.</p> <p><code>DataFrame</code> lives in <code>pyspark.sql.dataframe</code> module (together with <code>DataFrameNaFunctions</code> and <code>DataFrameStatFunctions</code>).</p> <pre><code>from pyspark.sql.dataframe import DataFrame\n</code></pre>"},{"location":"sql/DataFrame/#creating-instance","title":"Creating Instance","text":"<p><code>DataFrame</code> takes the following to be created:</p> <ul> <li> jdf <li> SQLContext"},{"location":"sql/DataFrame/#groupby","title":"groupBy <pre><code>groupBy(self, *cols)\n</code></pre> <p><code>groupBy</code> requests the _jdf to <code>groupBy</code> and creates a GroupedData with it.</p>","text":""},{"location":"sql/EvalPythonExec/","title":"EvalPythonExec Unary Physical Operators","text":"<p><code>EvalPythonExec</code> is an extension of the <code>UnaryExecNode</code> (Spark SQL) abstraction for unary physical operators that evaluate PythonUDFs (when executed).</p>"},{"location":"sql/EvalPythonExec/#contract","title":"Contract","text":""},{"location":"sql/EvalPythonExec/#evaluate","title":"Evaluating PythonUDFs","text":"<pre><code>evaluate(\nfuncs: Seq[ChainedPythonFunctions],\nargOffsets: Array[Array[Int]],\niter: Iterator[InternalRow],\nschema: StructType,\ncontext: TaskContext): Iterator[InternalRow]\n</code></pre> <p>See:</p> <ul> <li>ArrowEvalPythonExec</li> </ul> <p>Used when:</p> <ul> <li><code>EvalPythonExec</code> physical operator is requested to doExecute</li> </ul>"},{"location":"sql/EvalPythonExec/#resultAttrs","title":"Result Attributes","text":"<pre><code>resultAttrs: Seq[Attribute]\n</code></pre> <p>Result <code>Attribute</code>s (Spark SQL)</p> <p>See:</p> <ul> <li>ArrowEvalPythonExec</li> </ul> <p>Used when:</p> <ul> <li><code>EvalPythonExec</code> physical operator is requested for the output and producedAttributes</li> </ul>"},{"location":"sql/EvalPythonExec/#udfs","title":"Python UDFs","text":"<pre><code>udfs: Seq[PythonUDF]\n</code></pre> <p>PythonUDFs to evaluate</p> <p>See:</p> <ul> <li>ArrowEvalPythonExec</li> </ul> <p>Used when:</p> <ul> <li><code>EvalPythonExec</code> physical operator is requested to doExecute</li> </ul>"},{"location":"sql/EvalPythonExec/#implementations","title":"Implementations","text":"<ul> <li>ArrowEvalPythonExec</li> <li><code>BatchEvalPythonExec</code></li> </ul>"},{"location":"sql/EvalPythonExec/#doExecute","title":"Executing Physical Operator","text":"SparkPlan <pre><code>doExecute(): RDD[InternalRow]\n</code></pre> <p><code>doExecute</code> is part of the <code>SparkPlan</code> (Spark SQL) abstraction.</p> <p>The gist of <code>doExecute</code> is to evaluate Python UDFs (for every <code>InternalRow</code>) with some pre- and post-processing.</p> <p><code>doExecute</code> requests the child physical operator to <code>execute</code> (to produce an input <code>RDD[InternalRow]</code>).</p> <p>Note</p> <p><code>EvalPythonExec</code>s are <code>UnaryExecNode</code>s (Spark SQL).</p> <p><code>doExecute</code> uses <code>RDD.mapPartitions</code> operator to execute a function over partitions of <code>InternalRow</code>s.</p> <p>For every partition, <code>doExecute</code> creates a <code>MutableProjection</code> for the inputs (and the child's output) and requests it to <code>initialize</code>.</p> <p><code>doExecute</code> evaluates Python UDFs (for every <code>InternalRow</code>).</p> <p>In the end, <code>doExecute</code> creates an <code>UnsafeProjection</code> for the output to \"map over\" the rows (from evaluating Python UDFs).</p>"},{"location":"sql/FlatMapGroupsInPandas/","title":"FlatMapGroupsInPandas Logical Operator","text":"<p><code>FlatMapGroupsInPandas</code> is a unary logical operator (Spark SQL).</p> <p><code>FlatMapGroupsInPandas</code> is planned as a FlatMapGroupsInPandasExec physical operator.</p>"},{"location":"sql/FlatMapGroupsInPandas/#creating-instance","title":"Creating Instance","text":"<p><code>FlatMapGroupsInPandas</code> takes the following to be created:</p> <ul> <li> Grouping Attributes (Spark SQL) <li> Function Expression (Spark SQL) <li> Output Attributes (Spark SQL) <li> Child Logical Operator (Spark SQL) <p><code>FlatMapGroupsInPandas</code> is created\u00a0when:</p> <ul> <li><code>RelationalGroupedDataset</code> is requested to flatMapGroupsInPandas (with a PythonUDF)</li> </ul>"},{"location":"sql/FlatMapGroupsInPandasExec/","title":"FlatMapGroupsInPandasExec Physical Operator","text":"<p><code>FlatMapGroupsInPandasExec</code> is a unary physical operator (Spark SQL) to execute a PythonUDF.</p> <p><code>FlatMapGroupsInPandasExec</code> represents a FlatMapGroupsInPandas logical operator at execution time.</p>"},{"location":"sql/FlatMapGroupsInPandasExec/#creating-instance","title":"Creating Instance","text":"<p><code>FlatMapGroupsInPandasExec</code> takes the following to be created:</p> <ul> <li> Grouping Attributes (Spark SQL) <li> Function Expression (Spark SQL) <li> Output Attributes (Spark SQL) <li> Child Physical Operator (Spark SQL) <p><code>FlatMapGroupsInPandasExec</code> is created\u00a0when:</p> <ul> <li><code>BasicOperators</code> (Spark SQL) execution planning strategy is executed (on a logical query plan with FlatMapGroupsInPandas logical operators)</li> </ul>"},{"location":"sql/GroupedData/","title":"GroupedData","text":"<p><code>GroupedData</code> is created\u00a0for the following <code>DataFrame</code> operators:</p> <ul> <li>cube</li> <li>groupBy</li> <li>rollup</li> <li>pivot</li> </ul> <p>GroupedData.agg is used to apply aggregation functions to groups of rows (execution environment):</p> <ul> <li>Built-In Aggregation Functions</li> <li>Group Aggregate pandas UDFs</li> </ul> <p><code>GroupedData</code> is a Python class with PandasGroupedOpsMixin mixin.</p> <p><code>GroupedData</code> is defined in <code>pyspark.sql.group</code> module.</p> <pre><code>from pyspark.sql.group import GroupedData\n</code></pre>"},{"location":"sql/GroupedData/#creating-instance","title":"Creating Instance","text":"<p><code>GroupedData</code> takes the following to be created:</p> <ul> <li> RelationalGroupedDataset <li> DataFrame"},{"location":"sql/GroupedData/#agg","title":"agg","text":"<pre><code>agg(\n    self,\n    *exprs: Union[Column, Dict[str, str]]) -&gt; DataFrame\n</code></pre> <p>Note</p> <p>Built-in aggregation functions and group aggregate pandas UDFs cannot be mixed in a single <code>agg</code>.</p>"},{"location":"sql/PandasCogroupedOps/","title":"PandasCogroupedOps","text":"<p><code>PandasCogroupedOps</code> is a logical grouping created by GroupedData.cogroup.</p> <pre><code>from pyspark.sql.pandas.group_ops import PandasCogroupedOps\n</code></pre> <p><code>PandasCogroupedOps</code> is included in <code>__all__</code> of <code>pyspark.sql</code> module (via <code>__init__.py</code>).</p>"},{"location":"sql/PandasCogroupedOps/#creating-instance","title":"Creating Instance","text":"<p><code>PandasCogroupedOps</code> takes the following to be created:</p> <ul> <li> GroupedData <li> GroupedData <p><code>PandasCogroupedOps</code> is created\u00a0when:</p> <ul> <li><code>PandasGroupedOpsMixin</code> is requested to cogroup</li> </ul>"},{"location":"sql/PandasCogroupedOps/#applyinpandas","title":"applyInPandas <pre><code>applyInPandas(self, func, schema)\n</code></pre> <p><code>applyInPandas</code> creates a DataFrame with the result of flatMapCoGroupsInPandas with a pandas user defined function of <code>SQL_COGROUPED_MAP_PANDAS_UDF</code> type.</p>  <p><code>applyInPandas</code> creates a pandas user defined function for the given <code>func</code> and the return type by the given <code>schema</code>. The pandas UDF is of <code>SQL_COGROUPED_MAP_PANDAS_UDF</code> type.</p> <p><code>applyInPandas</code> applies the pandas UDF on all the columns of the two GroupedDatas (that creates a <code>Column</code> expression).</p> <p><code>applyInPandas</code> requests the GroupedData for the associated RelationalGroupedDataset that is in turn requested to flatMapCoGroupsInPandas.</p>","text":""},{"location":"sql/PandasConversionMixin/","title":"PandasConversionMixin","text":"<p><code>PandasConversionMixin</code> is a Python mixin of DataFrame to convert to Pandas (pandas.DataFrame).</p>"},{"location":"sql/PandasConversionMixin/#toPandas","title":"toPandas","text":"<pre><code>toPandas(self)\n</code></pre> <p><code>toPandas</code> can only be used with DataFrame.</p> <p>With Arrow optimization enabled, <code>toPandas</code> to_arrow_schema.</p> <p>pyarrow</p> <p>Arrow Optimization uses <code>pyarrow</code> module.</p> <p><code>toPandas</code> renames the columns to be of <code>col_[index]</code> format and _collect_as_arrow (with <code>split_batches</code> based on <code>arrowPySparkSelfDestructEnabled</code> configuration property).</p> <p><code>toPandas</code> creates a <code>pyarrow.Table</code> (from the <code>RecordBatch</code>es) and converts the table to a pandas-compatible NumPy array or <code>DataFrame</code>. <code>toPandas</code> renames the columns back to the initial column names.</p> <p>Note</p> <p>Column order is assumed.</p> <p>With Arrow optimization disabled, <code>toPandas</code> collects the records (<code>DataFrame.collect</code>) and creates a <code>pandas.DataFrame</code> (with some type munging).</p>"},{"location":"sql/PandasGroupedOpsMixin/","title":"PandasGroupedOpsMixin","text":"<p><code>PandasGroupedOpsMixin</code> is a Python mixin for GroupedData class.</p>"},{"location":"sql/PandasGroupedOpsMixin/#applyinpandas","title":"applyInPandas <pre><code>applyInPandas(self, func, schema)\n</code></pre> <p><code>applyInPandas</code> creates a <code>DataFrame</code> with flatMapGroupsInPandas.</p>","text":""},{"location":"sql/PandasGroupedOpsMixin/#example","title":"Example <pre><code>df1 = spark.createDataFrame(\n    [(20000101, 1, 1.0), (20000101, 2, 2.0), (20000102, 1, 3.0), (20000102, 2, 4.0)],\n    (\"time\", \"id\", \"v1\"))\ndf2 = spark.createDataFrame(\n    [(20000101, 1, \"x\"), (20000101, 2, \"y\")],\n    (\"time\", \"id\", \"v2\"))\n</code></pre> <pre><code>import pandas as pd\ndef asof_join(k, l, r):\n  if k == (1,):\n    return pd.merge_asof(l, r, on=\"time\", by=\"id\")\n  else:\n    return pd.DataFrame(columns=['time', 'id', 'v1', 'v2'])\n</code></pre> <pre><code>gd1 = df1.groupby(\"id\")\ngd2 = df2.groupby(\"id\")\n\ngd1.cogroup(gd2).applyInPandas(\n  asof_join, \"time int, id int, v1 double, v2 string\").show()\n</code></pre>","text":""},{"location":"sql/PandasGroupedOpsMixin/#cogroup","title":"cogroup <pre><code>cogroup(self, other)\n</code></pre> <p><code>cogroup</code> creates a PandasCogroupedOps.</p>","text":""},{"location":"sql/PandasMapOpsMixin/","title":"PandasMapOpsMixin","text":"<p><code>PandasMapOpsMixin</code> is a Python mixin for DataFrame class.</p>"},{"location":"sql/PythonEvalType/","title":"PythonEvalType","text":"<p><code>PythonEvalType</code> are the types of commands that will be sent to the Python worker.</p> Name Value PandasUDFType SQL_GROUPED_AGG_PANDAS_UDF 202 GROUPED_AGG SQL_GROUPED_MAP_PANDAS_UDF 201 GROUPED_MAP SQL_SCALAR_PANDAS_UDF 200 SCALAR SQL_SCALAR_PANDAS_ITER_UDF 204 SCALAR_ITER <p><code>PythonEvalType</code> is defined in <code>org.apache.spark.api.python</code> Scala package with the same values defined on Python side in the PythonEvalType Python class (in <code>pyspark/rdd.py</code> package).</p>"},{"location":"sql/PythonEvalType/#SQL_SCALAR_PANDAS_UDF","title":"SQL_SCALAR_PANDAS_UDF","text":"<p><code>SQL_SCALAR_PANDAS_UDF</code> is among SCALAR_TYPES of PythonUDF.</p> <p><code>SQL_SCALAR_PANDAS_UDF</code> is equivalent to PandasUDFType.SCALAR.</p> <p><code>SQL_SCALAR_PANDAS_UDF</code> (with SQL_SCALAR_PANDAS_ITER_UDF) are evaluated using ArrowEvalPython.</p> <p><code>SQL_SCALAR_PANDAS_UDF</code> is used (on Python side) when:</p> <ul> <li><code>pyspark/worker.py</code> is requested to <code>read_single_udf</code> and <code>read_udfs</code></li> <li><code>UDFRegistration</code> is requested to register a Python UDF</li> <li><code>UserDefinedFunction</code> is requested to returnType</li> <li><code>pyspark/pandas/functions.py</code> is requested to <code>_create_pandas_udf</code> and <code>pandas_udf</code></li> </ul>"},{"location":"sql/PythonEvalType/#SQL_SCALAR_PANDAS_ITER_UDF","title":"SQL_SCALAR_PANDAS_ITER_UDF","text":""},{"location":"sql/PythonSQLMetrics/","title":"PythonSQLMetrics","text":"<p><code>PythonSQLMetrics</code> is a collection of SQL metrics of the physical operators in PySpark.</p>"},{"location":"sql/PythonSQLMetrics/#performance-metrics","title":"Performance Metrics","text":""},{"location":"sql/PythonSQLMetrics/#pythonDataReceived","title":"data returned from Python workers","text":""},{"location":"sql/PythonSQLMetrics/#pythonDataSent","title":"data sent to Python workers","text":""},{"location":"sql/PythonSQLMetrics/#pythonNumRowsReceived","title":"number of output rows","text":""},{"location":"sql/PythonSQLMetrics/#implementations","title":"Implementations","text":"<ul> <li>AggregateInPandasExec</li> <li>ArrowEvalPythonExec</li> <li><code>BatchEvalPythonExec</code></li> <li><code>FlatMapCoGroupsInPandasExec</code></li> <li>FlatMapGroupsInPandasExec</li> <li><code>MapInBatchExec</code></li> <li><code>StateStoreWriter</code></li> <li><code>WindowInPandasExec</code></li> </ul>"},{"location":"sql/PythonUDF/","title":"PythonUDF","text":"<p><code>PythonUDF</code> is a Catalyst expression (Spark SQL).</p>"},{"location":"sql/PythonUDF/#creating-instance","title":"Creating Instance","text":"<p><code>PythonUDF</code> takes the following to be created:</p> <ul> <li> Name <li> PythonFunction <li> <code>DataType</code> (Spark SQL) <li> Children Catalyst Expressions (Spark SQL) <li> Python Eval Type <li> <code>udfDeterministic</code> flag <li> Result ID (<code>ExprId</code>) <p><code>PythonUDF</code> is created\u00a0when:</p> <ul> <li><code>UserDefinedPythonFunction</code> is requested to builder</li> </ul>"},{"location":"sql/PythonUDF/#unevaluable","title":"Unevaluable","text":"<p><code>PythonUDF</code> is an <code>Unevaluable</code> expression (Spark SQL).</p>"},{"location":"sql/PythonUDF/#nonsqlexpression","title":"NonSQLExpression","text":"<p><code>PythonUDF</code> is a <code>NonSQLExpression</code> expression (Spark SQL).</p>"},{"location":"sql/PythonUDF/#userdefinedexpression","title":"UserDefinedExpression","text":"<p><code>PythonUDF</code> is a <code>UserDefinedExpression</code> expression (Spark SQL).</p>"},{"location":"sql/PythonUDF/#isScalarPythonUDF","title":"isScalarPythonUDF","text":"<pre><code>isScalarPythonUDF(\ne: Expression): Boolean\n</code></pre> <p><code>isScalarPythonUDF</code> holds <code>true</code> when the following all hold <code>true</code>:</p> <ul> <li>The given <code>Expression</code> (Spark SQL) is a PythonUDF</li> <li>The evalType is scalar</li> </ul> <p><code>isScalarPythonUDF</code> is used when:</p> <ul> <li><code>ExtractPythonUDFFromJoinCondition</code> is requested to <code>hasUnevaluablePythonUDF</code></li> <li><code>ExtractPythonUDFFromAggregate</code> is requested to <code>hasPythonUdfOverAggregate</code></li> <li><code>ExtractGroupingPythonUDFFromAggregate</code> is requested to <code>hasScalarPythonUDF</code></li> <li><code>ExtractPythonUDFs</code> is requested to <code>hasScalarPythonUDF</code>, <code>collectEvaluableUDFs</code>, <code>extract</code></li> </ul>"},{"location":"sql/PythonUDF/#SCALAR_TYPES","title":"Scalar PythonUDF Types","text":"<p><code>PythonUDF</code> is scalar for the following eval types:</p> <ul> <li>SQL_BATCHED_UDF</li> <li>SQL_SCALAR_PANDAS_UDF</li> <li>SQL_SCALAR_PANDAS_ITER_UDF</li> </ul>"},{"location":"sql/RelationalGroupedDataset/","title":"RelationalGroupedDataset","text":"<p><code>RelationalGroupedDataset</code> is a result of executing high-level grouping operators.</p> <p>This is a stub</p> <p>This page is a stub to describe PySpark-related methods only. Learn more about RelationalGroupedDataset in The Internals of Spark SQL.</p>"},{"location":"sql/RelationalGroupedDataset/#flatmapcogroupsinpandas","title":"flatMapCoGroupsInPandas <pre><code>flatMapCoGroupsInPandas(\n  r: RelationalGroupedDataset,\n  expr: PythonUDF): DataFrame\n</code></pre> <p><code>flatMapCoGroupsInPandas</code>...FIXME</p> <p><code>flatMapCoGroupsInPandas</code>\u00a0is used when:</p> <ul> <li><code>PandasCogroupedOps</code> is requested to applyInPandas</li> </ul>","text":""},{"location":"sql/RelationalGroupedDataset/#flatmapgroupsinpandas","title":"flatMapGroupsInPandas <pre><code>flatMapGroupsInPandas(\n  expr: PythonUDF): DataFrame\n</code></pre> <p><code>flatMapGroupsInPandas</code> creates a <code>DataFrame</code> with a FlatMapGroupsInPandas logical operator.</p> <p><code>flatMapGroupsInPandas</code> asserts that the input PythonUDF is a grouped map udf (the evalType is <code>PythonEvalType.SQL_GROUPED_MAP_PANDAS_UDF</code>).</p> <p><code>flatMapGroupsInPandas</code> asserts that the return type of the input PythonUDF is <code>StructType</code>.</p> <p><code>flatMapGroupsInPandas</code>\u00a0is used when:</p> <ul> <li><code>PandasGroupedOpsMixin</code> is requested to applyInPandas</li> </ul>","text":""},{"location":"sql/SQLContext/","title":"SQLContext","text":"<p><code>SQLContext</code> is...FIXME</p>"},{"location":"sql/SparkConversionMixin/","title":"SparkConversionMixin","text":"<p><code>SparkConversionMixin</code> is a Python mixin for SparkSession class.</p>"},{"location":"sql/SparkSession/","title":"SparkSession","text":""},{"location":"sql/SparkSession/#_create_shell_session","title":"_create_shell_session <pre><code>_create_shell_session()\n</code></pre> <p><code>_create_shell_session</code> is a <code>@staticmethod</code>.</p> <p><code>_create_shell_session</code>...FIXME</p> <p><code>_create_shell_session</code>\u00a0is used when:</p> <ul> <li>FIXME</li> </ul>","text":""},{"location":"sql/SparkSession/#sql","title":"sql <pre><code>sql(\n  self, sqlQuery)\n</code></pre> <p><code>sql</code> creates a <code>DataFrame</code> with the <code>sqlQuery</code> query executed.</p>","text":""},{"location":"sql/UDFRegistration/","title":"UDFRegistration","text":"<p><code>UDFRegistration</code> is available as <code>spark.udf</code>.</p>"},{"location":"sql/UDFRegistration/#register","title":"register","text":"<pre><code>register(\n  self,\n  name: str,\n  f: Union[Callable[..., Any], \"UserDefinedFunctionLike\"],\n  returnType: Optional[Union[pyspark.sql.types.DataType, str]] = None,\n) -&gt; \"UserDefinedFunctionLike\"\n</code></pre> <p><code>register</code> registers a Python function (incl. lambda function) or a user-defined function as a SQL function (under the given <code>name</code>).</p> Function <code>f</code> Description A Python function <ul><li>Includes lambda (unnamed) functions<li><code>Callable[..., Any]</code><li>The return type is <code>StringType</code> when not specified<li>Always <code>PythonEvalType.SQL_BATCHED_UDF</code> <code>pyspark.sql.functions.udf</code> <ul><li>row-at-a-time<li><code>UserDefinedFunctionLike</code> <code>pyspark.sql.functions.pandas_udf</code> <ul><li>vectorized<li><code>UserDefinedFunctionLike</code> <p><code>evalType</code> of the a user-defined function can be one of the following:</p> <ul> <li>SQL_BATCHED_UDF</li> <li>SQL_SCALAR_PANDAS_UDF</li> <li>SQL_SCALAR_PANDAS_ITER_UDF</li> <li>SQL_GROUPED_AGG_PANDAS_UDF</li> </ul> <p><code>register</code> _create_udf and requests the <code>_jsparkSession</code> for the <code>UDFRegistration</code> (Spark SQL) to <code>registerPython</code> (Spark SQL).</p> <pre><code>from pyspark.sql.functions import call_udf, col\nfrom pyspark.sql.types import IntegerType, StringType\n\nrows = [(1, \"a\"),(2, \"b\"), (3, \"c\")]\ncolumns = [\"id\", \"name\"]\ndf = spark.createDataFrame(rows, columns)\n\nspark.udf.register(\"intX2\", lambda i: i * 2, IntegerType())\ndf.select(call_udf(\"intX2\", \"id\")).show()\n</code></pre>"},{"location":"sql/UserDefinedFunction/","title":"UserDefinedFunction","text":"<p><code>UserDefinedFunction</code> is...FIXME</p>"},{"location":"sql/UserDefinedPythonFunction/","title":"UserDefinedPythonFunction","text":""},{"location":"sql/UserDefinedPythonFunction/#creating-instance","title":"Creating Instance","text":"<p><code>UserDefinedPythonFunction</code> takes the following to be created:</p> <ul> <li> Name <li> <code>PythonFunction</code> <li> <code>DataType</code> (Spark SQL) <li> Python Eval Type <li> <code>udfDeterministic</code> flag"},{"location":"sql/UserDefinedPythonFunction/#creating-pythonudf","title":"Creating PythonUDF <pre><code>builder(\n  e: Seq[Expression]): Expression\n</code></pre> <p><code>builder</code> creates a PythonUDF for the arguments and the given children expressions.</p> <p><code>builder</code>\u00a0is used when:</p> <ul> <li><code>UDFRegistration</code> is requested to register a Python UDF (Spark SQL)</li> <li><code>UserDefinedPythonFunction</code> is requested to apply</li> </ul>","text":""},{"location":"sql/UserDefinedPythonFunction/#applying-pythonudf","title":"Applying PythonUDF <pre><code>apply(\n  exprs: Column*): Column\n</code></pre> <p><code>apply</code> creates a PythonUDF with the input <code>Column</code> (Spark SQL) expressions and creates a new <code>Column</code>.</p> <p><code>apply</code>\u00a0is used when:</p> <ul> <li><code>UDFRegistration</code> is requested to register a Python UDF (Spark SQL)</li> <li><code>UserDefinedPythonFunction</code> is requested to apply</li> </ul>","text":""}]}