{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"The Internals of PySpark (Apache Spark 3.1.1) \u00b6 Welcome to The Internals of PySpark online book! \ud83e\udd19 I'm Jacek Laskowski , an IT freelancer specializing in Apache Spark , Delta Lake and Apache Kafka (with brief forays into a wider data engineering space, e.g. Trino and ksqlDB , mostly during Warsaw Data Engineering meetups). I'm very excited to have you here and hope you will enjoy exploring the internals of Spark SQL as much as I have. Flannery O'Connor I write to discover what I know. \"The Internals Of\" series I'm also writing other online books in the \"The Internals Of\" series. Please visit \"The Internals Of\" Online Books home page. Expect text and code snippets from a variety of public sources. Attribution follows. Now, let's deep dive into PySpark \ud83d\udd25","title":"Home"},{"location":"#the-internals-of-pyspark-apache-spark-311","text":"Welcome to The Internals of PySpark online book! \ud83e\udd19 I'm Jacek Laskowski , an IT freelancer specializing in Apache Spark , Delta Lake and Apache Kafka (with brief forays into a wider data engineering space, e.g. Trino and ksqlDB , mostly during Warsaw Data Engineering meetups). I'm very excited to have you here and hope you will enjoy exploring the internals of Spark SQL as much as I have. Flannery O'Connor I write to discover what I know. \"The Internals Of\" series I'm also writing other online books in the \"The Internals Of\" series. Please visit \"The Internals Of\" Online Books home page. Expect text and code snippets from a variety of public sources. Attribution follows. Now, let's deep dive into PySpark \ud83d\udd25","title":"The Internals of PySpark (Apache Spark 3.1.1)"},{"location":"MonitorThread/","text":"MonitorThread \u00b6 MonitorThread is...FIXME","title":"MonitorThread"},{"location":"MonitorThread/#monitorthread","text":"MonitorThread is...FIXME","title":"MonitorThread"},{"location":"Py4JServer/","text":"Py4JServer \u00b6 Py4JServer is a gateway server between Python and Java Virtual Machine (JVM) using Py4J . Py4JServer is a wrapper for a py4j Server . Creating Instance \u00b6 Py4JServer takes the following to be created: SparkConf ( Spark Core ) Py4JServer is created when: PythonGatewayServer command-line application is started PythonRunner command-line application is started py4j Server \u00b6 Py4JServer creates a ClientServer ( py4j ) or GatewayServer ( py4j ) based on PYSPARK_PIN_THREAD environment variable. Connection Secret \u00b6 secret : String Py4JServer creates a connection secret for a secure communication. start \u00b6 start () : Unit start requests the py4j Server to start. getListeningPort \u00b6 getListeningPort : Int getListeningPort requests the py4j Server for the listening port.","title":"Py4JServer"},{"location":"Py4JServer/#py4jserver","text":"Py4JServer is a gateway server between Python and Java Virtual Machine (JVM) using Py4J . Py4JServer is a wrapper for a py4j Server .","title":"Py4JServer"},{"location":"Py4JServer/#creating-instance","text":"Py4JServer takes the following to be created: SparkConf ( Spark Core ) Py4JServer is created when: PythonGatewayServer command-line application is started PythonRunner command-line application is started","title":"Creating Instance"},{"location":"Py4JServer/#py4j-server","text":"Py4JServer creates a ClientServer ( py4j ) or GatewayServer ( py4j ) based on PYSPARK_PIN_THREAD environment variable.","title":" py4j Server"},{"location":"Py4JServer/#connection-secret","text":"secret : String Py4JServer creates a connection secret for a secure communication.","title":" Connection Secret"},{"location":"Py4JServer/#start","text":"start () : Unit start requests the py4j Server to start.","title":" start"},{"location":"Py4JServer/#getlisteningport","text":"getListeningPort : Int getListeningPort requests the py4j Server for the listening port.","title":" getListeningPort"},{"location":"PythonAccumulatorV2/","text":"PythonAccumulatorV2 \u00b6 PythonAccumulatorV2 is...FIXME","title":"PythonAccumulatorV2"},{"location":"PythonAccumulatorV2/#pythonaccumulatorv2","text":"PythonAccumulatorV2 is...FIXME","title":"PythonAccumulatorV2"},{"location":"PythonBroadcast/","text":"PythonBroadcast \u00b6 PythonBroadcast is...FIXME","title":"PythonBroadcast"},{"location":"PythonBroadcast/#pythonbroadcast","text":"PythonBroadcast is...FIXME","title":"PythonBroadcast"},{"location":"PythonForeachWriter/","text":"PythonForeachWriter \u00b6 PythonForeachWriter is...FIXME","title":"PythonForeachWriter"},{"location":"PythonForeachWriter/#pythonforeachwriter","text":"PythonForeachWriter is...FIXME","title":"PythonForeachWriter"},{"location":"PythonFunction/","text":"PythonFunction \u00b6 PythonFunction is a metadata of a Python function to be executed in PythonRunner . Creating Instance \u00b6 PythonFunction takes the following to be created: Command ( Array[Byte] ) Environment Variables ( Map[String, String] ) Python Includes ( List[String] ) Python Executable Python Version Broadcast Variables with PythonBroadcast ]s PythonAccumulatorV2 PythonFunction is created when...FIXME","title":"PythonFunction"},{"location":"PythonFunction/#pythonfunction","text":"PythonFunction is a metadata of a Python function to be executed in PythonRunner .","title":"PythonFunction"},{"location":"PythonFunction/#creating-instance","text":"PythonFunction takes the following to be created: Command ( Array[Byte] ) Environment Variables ( Map[String, String] ) Python Includes ( List[String] ) Python Executable Python Version Broadcast Variables with PythonBroadcast ]s PythonAccumulatorV2 PythonFunction is created when...FIXME","title":"Creating Instance"},{"location":"PythonGatewayServer/","text":"PythonGatewayServer \u00b6 PythonGatewayServer is a command-line application ( process ) that starts a Py4JServer on an ephemeral port. PythonGatewayServer is the Python runner for pyspark shell ( Spark Core ). main \u00b6 main creates a Py4JServer and requests it to start . main requests the Py4JServer for the listening port ( boundPort ) and prints out the following DEBUG message to the logs: Started PythonGatewayServer on port [boundPort] main uses _PYSPARK_DRIVER_CONN_INFO_PATH environment variable for the path of a connection info file (for the associated python process) with the listening port and the secret . main pauses ( blocks ) until the Python driver finishes (by reading from the system input that blocks until input data is available, the end of the stream is detected, or an exception is thrown). In the end, once the Python driver finishes, main prints out the following DEBUG message to the logs: Exiting due to broken pipe from Python driver main prints out the following ERROR message to the logs and exists when the listening port is -1 : [server] failed to bind; exiting _PYSPARK_DRIVER_CONN_INFO_PATH \u00b6 PythonGatewayServer uses _PYSPARK_DRIVER_CONN_INFO_PATH environment variable for the path of a connection info file for communication between this and the Python processes. _PYSPARK_DRIVER_CONN_INFO_PATH is configured when java_gateway.py module is requested to launch_gateway . Logging \u00b6 Enable ALL logging level for org.apache.spark.api.python.PythonGatewayServer logger to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.api.python.PythonGatewayServer=ALL Refer to Logging .","title":"PythonGatewayServer"},{"location":"PythonGatewayServer/#pythongatewayserver","text":"PythonGatewayServer is a command-line application ( process ) that starts a Py4JServer on an ephemeral port. PythonGatewayServer is the Python runner for pyspark shell ( Spark Core ).","title":"PythonGatewayServer"},{"location":"PythonGatewayServer/#main","text":"main creates a Py4JServer and requests it to start . main requests the Py4JServer for the listening port ( boundPort ) and prints out the following DEBUG message to the logs: Started PythonGatewayServer on port [boundPort] main uses _PYSPARK_DRIVER_CONN_INFO_PATH environment variable for the path of a connection info file (for the associated python process) with the listening port and the secret . main pauses ( blocks ) until the Python driver finishes (by reading from the system input that blocks until input data is available, the end of the stream is detected, or an exception is thrown). In the end, once the Python driver finishes, main prints out the following DEBUG message to the logs: Exiting due to broken pipe from Python driver main prints out the following ERROR message to the logs and exists when the listening port is -1 : [server] failed to bind; exiting","title":" main"},{"location":"PythonGatewayServer/#_pyspark_driver_conn_info_path","text":"PythonGatewayServer uses _PYSPARK_DRIVER_CONN_INFO_PATH environment variable for the path of a connection info file for communication between this and the Python processes. _PYSPARK_DRIVER_CONN_INFO_PATH is configured when java_gateway.py module is requested to launch_gateway .","title":" _PYSPARK_DRIVER_CONN_INFO_PATH"},{"location":"PythonGatewayServer/#logging","text":"Enable ALL logging level for org.apache.spark.api.python.PythonGatewayServer logger to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.api.python.PythonGatewayServer=ALL Refer to Logging .","title":"Logging"},{"location":"PythonRDD/","text":"PythonRDD \u00b6 PythonRDD is an RDD ( RDD[Array[Byte]] ) that uses PythonRunner (to compute a partition ). Creating Instance \u00b6 PythonRDD takes the following to be created: Parent RDD PythonFunction preservePartitoning flag isFromBarrier flag (default: false ) PythonRDD is created when...FIXME runJob \u00b6 runJob ( sc : SparkContext , rdd : JavaRDD [ Array [ Byte ]], partitions : JArrayList [ Int ]) : Array [ Any ] runJob ...FIXME collectAndServe \u00b6 collectAndServe [ T ]( rdd : RDD [ T ]) : Array [ Any ] collectAndServe ...FIXME collectAndServeWithJobGroup \u00b6 collectAndServeWithJobGroup [ T ]( rdd : RDD [ T ], groupId : String , description : String , interruptOnCancel : Boolean ) : Array [ Any ] collectAndServeWithJobGroup ...FIXME serveIterator Utility \u00b6 serveIterator ( items : Iterator [ _ ], threadName : String ) : Array [ Any ] serveIterator serveToStream with a writer function that...FIXME serveIterator is used when: PythonRDD utility is used to runJob , collectAndServe and collectAndServeWithJobGroup Dataset is requested to collectToPython , tailToPython , getRowsToPython serveToStream Utility \u00b6 serveToStream ( threadName : String )( writeFunc : OutputStream => Unit ) : Array [ Any ] serveToStream serveToStream with the authHelper and the input arguments. serveToStream is used when: PythonRDD utility is used to serveIterator Dataset is requested to collectAsArrowToPython SocketAuthHelper \u00b6 PythonRDD uses a SocketAuthHelper .","title":"PythonRDD"},{"location":"PythonRDD/#pythonrdd","text":"PythonRDD is an RDD ( RDD[Array[Byte]] ) that uses PythonRunner (to compute a partition ).","title":"PythonRDD"},{"location":"PythonRDD/#creating-instance","text":"PythonRDD takes the following to be created: Parent RDD PythonFunction preservePartitoning flag isFromBarrier flag (default: false ) PythonRDD is created when...FIXME","title":"Creating Instance"},{"location":"PythonRDD/#runjob","text":"runJob ( sc : SparkContext , rdd : JavaRDD [ Array [ Byte ]], partitions : JArrayList [ Int ]) : Array [ Any ] runJob ...FIXME","title":" runJob"},{"location":"PythonRDD/#collectandserve","text":"collectAndServe [ T ]( rdd : RDD [ T ]) : Array [ Any ] collectAndServe ...FIXME","title":" collectAndServe"},{"location":"PythonRDD/#collectandservewithjobgroup","text":"collectAndServeWithJobGroup [ T ]( rdd : RDD [ T ], groupId : String , description : String , interruptOnCancel : Boolean ) : Array [ Any ] collectAndServeWithJobGroup ...FIXME","title":" collectAndServeWithJobGroup"},{"location":"PythonRDD/#serveiterator-utility","text":"serveIterator ( items : Iterator [ _ ], threadName : String ) : Array [ Any ] serveIterator serveToStream with a writer function that...FIXME serveIterator is used when: PythonRDD utility is used to runJob , collectAndServe and collectAndServeWithJobGroup Dataset is requested to collectToPython , tailToPython , getRowsToPython","title":" serveIterator Utility"},{"location":"PythonRDD/#servetostream-utility","text":"serveToStream ( threadName : String )( writeFunc : OutputStream => Unit ) : Array [ Any ] serveToStream serveToStream with the authHelper and the input arguments. serveToStream is used when: PythonRDD utility is used to serveIterator Dataset is requested to collectAsArrowToPython","title":" serveToStream Utility"},{"location":"PythonRDD/#socketauthhelper","text":"PythonRDD uses a SocketAuthHelper .","title":" SocketAuthHelper"},{"location":"PythonRunner/","text":"PythonRunner \u00b6 PythonRunner is a command-line application to launch Python applications. PythonRunner is used by spark-submit . PythonRunner executes a configured python executable as a subprocess and then has it connect back to the JVM to access system properties, etc. Arguments \u00b6 PythonRunner requires the following command-line arguments: Main python file ( pythonFile ) Extra python files ( pyFiles ) Application arguments main \u00b6 main takes the arguments from command line. main determines what python executable to use based on (in that order): spark.pyspark.driver.python configuration property spark.pyspark.python configuration property PYSPARK_DRIVER_PYTHON environment variable PYSPARK_PYTHON environment variable python3 main creates a Py4JServer that is started on a daemon py4j-gateway-init thread. main waits until the gateway server has started. main launches a Python process using the python executable and the following environment variables. Environment Variable Value PYTHONPATH PYTHONUNBUFFERED YES PYSPARK_GATEWAY_PORT getListeningPort PYSPARK_GATEWAY_SECRET secret PYSPARK_PYTHON spark.pyspark.python if defined PYTHONHASHSEED PYTHONHASHSEED env var if defined OMP_NUM_THREADS spark.driver.cores unless defined main waits for the Python process to finish and requests the Py4JServer to shutdown . Demo \u00b6 ./bin/spark-class org.apache.spark.deploy.PythonRunner","title":"PythonRunner"},{"location":"PythonRunner/#pythonrunner","text":"PythonRunner is a command-line application to launch Python applications. PythonRunner is used by spark-submit . PythonRunner executes a configured python executable as a subprocess and then has it connect back to the JVM to access system properties, etc.","title":"PythonRunner"},{"location":"PythonRunner/#arguments","text":"PythonRunner requires the following command-line arguments: Main python file ( pythonFile ) Extra python files ( pyFiles ) Application arguments","title":"Arguments"},{"location":"PythonRunner/#main","text":"main takes the arguments from command line. main determines what python executable to use based on (in that order): spark.pyspark.driver.python configuration property spark.pyspark.python configuration property PYSPARK_DRIVER_PYTHON environment variable PYSPARK_PYTHON environment variable python3 main creates a Py4JServer that is started on a daemon py4j-gateway-init thread. main waits until the gateway server has started. main launches a Python process using the python executable and the following environment variables. Environment Variable Value PYTHONPATH PYTHONUNBUFFERED YES PYSPARK_GATEWAY_PORT getListeningPort PYSPARK_GATEWAY_SECRET secret PYSPARK_PYTHON spark.pyspark.python if defined PYTHONHASHSEED PYTHONHASHSEED env var if defined OMP_NUM_THREADS spark.driver.cores unless defined main waits for the Python process to finish and requests the Py4JServer to shutdown .","title":" main"},{"location":"PythonRunner/#demo","text":"./bin/spark-class org.apache.spark.deploy.PythonRunner","title":"Demo"},{"location":"PythonWorkerFactory/","text":"PythonWorkerFactory \u00b6 Creating Instance \u00b6 PythonWorkerFactory takes the following to be created: Python Executable Environment Variables ( Map[String, String] ) PythonWorkerFactory is created when SparkEnv is requested to createPythonWorker (when BasePythonRunner is requested to compute a partition ). useDaemon Flag \u00b6 PythonWorkerFactory uses useDaemon internal flag that is the value of spark.python.use.daemon configuration property to decide whether to use lighter daemon or non-daemon workers. useDaemon flag is used when PythonWorkerFactory requested to create , stop or release a worker and stop a daemon module . Python Daemon Module \u00b6 PythonWorkerFactory uses spark.python.daemon.module configuration property to define the Python Daemon Module . The Python Daemon Module is used when PythonWorkerFactory is requested to create and start a daemon module . Python Worker Module \u00b6 PythonWorkerFactory uses spark.python.worker.module configuration property to specify the Python Worker Module . The Python Worker Module is used when PythonWorkerFactory is requested to create and start a worker . Creating Python Worker \u00b6 create () : Socket create ...FIXME create is used when SparkEnv is requested to createPythonWorker . Creating Daemon Worker \u00b6 createThroughDaemon () : Socket createThroughDaemon ...FIXME createThroughDaemon is used when PythonWorkerFactory is requested to create a Python worker (with useDaemon flag enabled). Starting Python Daemon Process \u00b6 startDaemon () : Unit startDaemon ...FIXME Creating Simple Non-Daemon Worker \u00b6 createSimpleWorker () : Socket createSimpleWorker ...FIXME createSimpleWorker is used when PythonWorkerFactory is requested to create a Python worker (with useDaemon flag disabled). Logging \u00b6 Enable ALL logging level for org.apache.spark.api.python.PythonWorkerFactory logger to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.api.python.PythonWorkerFactory=ALL Refer to Logging .","title":"PythonWorkerFactory"},{"location":"PythonWorkerFactory/#pythonworkerfactory","text":"","title":"PythonWorkerFactory"},{"location":"PythonWorkerFactory/#creating-instance","text":"PythonWorkerFactory takes the following to be created: Python Executable Environment Variables ( Map[String, String] ) PythonWorkerFactory is created when SparkEnv is requested to createPythonWorker (when BasePythonRunner is requested to compute a partition ).","title":"Creating Instance"},{"location":"PythonWorkerFactory/#usedaemon-flag","text":"PythonWorkerFactory uses useDaemon internal flag that is the value of spark.python.use.daemon configuration property to decide whether to use lighter daemon or non-daemon workers. useDaemon flag is used when PythonWorkerFactory requested to create , stop or release a worker and stop a daemon module .","title":" useDaemon Flag"},{"location":"PythonWorkerFactory/#python-daemon-module","text":"PythonWorkerFactory uses spark.python.daemon.module configuration property to define the Python Daemon Module . The Python Daemon Module is used when PythonWorkerFactory is requested to create and start a daemon module .","title":" Python Daemon Module"},{"location":"PythonWorkerFactory/#python-worker-module","text":"PythonWorkerFactory uses spark.python.worker.module configuration property to specify the Python Worker Module . The Python Worker Module is used when PythonWorkerFactory is requested to create and start a worker .","title":" Python Worker Module"},{"location":"PythonWorkerFactory/#creating-python-worker","text":"create () : Socket create ...FIXME create is used when SparkEnv is requested to createPythonWorker .","title":" Creating Python Worker"},{"location":"PythonWorkerFactory/#creating-daemon-worker","text":"createThroughDaemon () : Socket createThroughDaemon ...FIXME createThroughDaemon is used when PythonWorkerFactory is requested to create a Python worker (with useDaemon flag enabled).","title":" Creating Daemon Worker"},{"location":"PythonWorkerFactory/#starting-python-daemon-process","text":"startDaemon () : Unit startDaemon ...FIXME","title":" Starting Python Daemon Process"},{"location":"PythonWorkerFactory/#creating-simple-non-daemon-worker","text":"createSimpleWorker () : Socket createSimpleWorker ...FIXME createSimpleWorker is used when PythonWorkerFactory is requested to create a Python worker (with useDaemon flag disabled).","title":" Creating Simple Non-Daemon Worker"},{"location":"PythonWorkerFactory/#logging","text":"Enable ALL logging level for org.apache.spark.api.python.PythonWorkerFactory logger to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.api.python.PythonWorkerFactory=ALL Refer to Logging .","title":"Logging"},{"location":"Setup/","text":"PySpark Setup \u00b6 Install IPython \u00b6 Follow the steps as described in the official documentation of IPython. pip install ipython Start PySpark \u00b6 export PYSPARK_DRIVER_PYTHON = ipython For Java 11, use -Dio.netty.tryReflectionSetAccessible=true (see Downloading in the official documentation of Apache Spark). ./bin/pyspark --driver-java-options = -Dio.netty.tryReflectionSetAccessible = true Python 3.9.1 (default, Feb 3 2021, 07:38:02) Type 'copyright', 'credits' or 'license' for more information IPython 7.20.0 -- An enhanced Interactive Python. Type '?' for help. Setting default log level to \"WARN\". To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel). Welcome to ____ __ / __/__ ___ _____/ /__ _\\ \\/ _ \\/ _ `/ __/ '_/ /__ / .__/\\_,_/_/ /_/\\_\\ version 3.1.1 /_/ Using Python version 3.9.1 (default, Feb 3 2021 07:38:02) Spark context Web UI available at http://192.168.68.101:4040 Spark context available as 'sc' (master = local[*], app id = local-1613571272142). SparkSession available as 'spark'. In [1]: In [1]: spark.version Out[1]: '3.1.1'","title":"Setup"},{"location":"Setup/#pyspark-setup","text":"","title":"PySpark Setup"},{"location":"Setup/#install-ipython","text":"Follow the steps as described in the official documentation of IPython. pip install ipython","title":"Install IPython"},{"location":"Setup/#start-pyspark","text":"export PYSPARK_DRIVER_PYTHON = ipython For Java 11, use -Dio.netty.tryReflectionSetAccessible=true (see Downloading in the official documentation of Apache Spark). ./bin/pyspark --driver-java-options = -Dio.netty.tryReflectionSetAccessible = true Python 3.9.1 (default, Feb 3 2021, 07:38:02) Type 'copyright', 'credits' or 'license' for more information IPython 7.20.0 -- An enhanced Interactive Python. Type '?' for help. Setting default log level to \"WARN\". To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel). Welcome to ____ __ / __/__ ___ _____/ /__ _\\ \\/ _ \\/ _ `/ __/ '_/ /__ / .__/\\_,_/_/ /_/\\_\\ version 3.1.1 /_/ Using Python version 3.9.1 (default, Feb 3 2021 07:38:02) Spark context Web UI available at http://192.168.68.101:4040 Spark context available as 'sc' (master = local[*], app id = local-1613571272142). SparkSession available as 'spark'. In [1]: In [1]: spark.version Out[1]: '3.1.1'","title":"Start PySpark"},{"location":"SocketAuthHelper/","text":"SocketAuthHelper \u00b6 SocketAuthHelper is...FIXME","title":"SocketAuthHelper"},{"location":"SocketAuthHelper/#socketauthhelper","text":"SocketAuthHelper is...FIXME","title":"SocketAuthHelper"},{"location":"SocketAuthServer/","text":"SocketAuthServer \u00b6 serveToStream Utility \u00b6 serveToStream ( threadName : String , authHelper : SocketAuthHelper )( writeFunc : OutputStream => Unit ) : Array [ Any ] serveToStream ...FIXME serveToStream is used when: FIXME","title":"SocketAuthServer"},{"location":"SocketAuthServer/#socketauthserver","text":"","title":"SocketAuthServer"},{"location":"SocketAuthServer/#servetostream-utility","text":"serveToStream ( threadName : String , authHelper : SocketAuthHelper )( writeFunc : OutputStream => Unit ) : Array [ Any ] serveToStream ...FIXME serveToStream is used when: FIXME","title":" serveToStream Utility"},{"location":"SocketFuncServer/","text":"SocketFuncServer \u00b6 SocketFuncServer is...FIXME","title":"SocketFuncServer"},{"location":"SocketFuncServer/#socketfuncserver","text":"SocketFuncServer is...FIXME","title":"SocketFuncServer"},{"location":"SparkConf/","text":"SparkConf \u00b6 SparkConf is a Python class. Creating Instance \u00b6 SparkConf takes the following to be created: loadDefaults flag (default: True ) JVMView ( py4j ) JConf (default: None ) While being created, SparkConf uses the JVMView (of the SparkContext ) unless the _jconf and _jvm are given. Demo \u00b6 from pyspark import SparkConf","title":"SparkConf"},{"location":"SparkConf/#sparkconf","text":"SparkConf is a Python class.","title":"SparkConf"},{"location":"SparkConf/#creating-instance","text":"SparkConf takes the following to be created: loadDefaults flag (default: True ) JVMView ( py4j ) JConf (default: None ) While being created, SparkConf uses the JVMView (of the SparkContext ) unless the _jconf and _jvm are given.","title":"Creating Instance"},{"location":"SparkConf/#demo","text":"from pyspark import SparkConf","title":"Demo"},{"location":"SparkContext/","text":"SparkContext \u00b6 Creating Instance \u00b6 SparkContext takes the following to be created: Master URL (default: None ) Application Name (default: None ) Spark Home (default: None ) Py Files (default: None ) Environment (default: None ) Batch Size (default: 0 ) PickleSerializer SparkConf (default: None ) Gateway (default: None ) Corresponding SparkContext on JVM (default: None ) BasicProfiler While being created, SparkContext _ensure_initialized (with the gateway and the conf ) followed by _do_init . Demo \u00b6 from pyspark import SparkContext JavaGateway \u00b6 SparkContext defines _gateway property for a JavaGateway that is given or launched when _ensure_initialized . JVMView \u00b6 SparkContext defines _jvm property for a JVMView ( py4j ) to access to the Java Virtual Machine of the JavaGateway . _ensure_initialized \u00b6 _ensure_initialized ( cls , instance = None , gateway = None , conf = None ) _ensure_initialized is a @classmethod . _ensure_initialized takes the given gateway or launch_gateway . _ensure_initialized ...FIXME _ensure_initialized is used when: SparkContext is created and setSystemProperty shell.py is launched _do_init \u00b6 _do_init ( self , master , appName , sparkHome , pyFiles , environment , batchSize , serializer , conf , jsc , profiler_cls ) _do_init ...FIXME","title":"SparkContext"},{"location":"SparkContext/#sparkcontext","text":"","title":"SparkContext"},{"location":"SparkContext/#creating-instance","text":"SparkContext takes the following to be created: Master URL (default: None ) Application Name (default: None ) Spark Home (default: None ) Py Files (default: None ) Environment (default: None ) Batch Size (default: 0 ) PickleSerializer SparkConf (default: None ) Gateway (default: None ) Corresponding SparkContext on JVM (default: None ) BasicProfiler While being created, SparkContext _ensure_initialized (with the gateway and the conf ) followed by _do_init .","title":"Creating Instance"},{"location":"SparkContext/#demo","text":"from pyspark import SparkContext","title":"Demo"},{"location":"SparkContext/#javagateway","text":"SparkContext defines _gateway property for a JavaGateway that is given or launched when _ensure_initialized .","title":" JavaGateway"},{"location":"SparkContext/#jvmview","text":"SparkContext defines _jvm property for a JVMView ( py4j ) to access to the Java Virtual Machine of the JavaGateway .","title":" JVMView"},{"location":"SparkContext/#_ensure_initialized","text":"_ensure_initialized ( cls , instance = None , gateway = None , conf = None ) _ensure_initialized is a @classmethod . _ensure_initialized takes the given gateway or launch_gateway . _ensure_initialized ...FIXME _ensure_initialized is used when: SparkContext is created and setSystemProperty shell.py is launched","title":" _ensure_initialized"},{"location":"SparkContext/#_do_init","text":"_do_init ( self , master , appName , sparkHome , pyFiles , environment , batchSize , serializer , conf , jsc , profiler_cls ) _do_init ...FIXME","title":" _do_init"},{"location":"building-from-sources/","text":"Building from Sources \u00b6 $ java -version openjdk version \"11.0.10\" 2021-01-19 OpenJDK Runtime Environment AdoptOpenJDK (build 11.0.10+9) OpenJDK 64-Bit Server VM AdoptOpenJDK (build 11.0.10+9, mixed mode) ./build/mvn \\ -Pyarn,kubernetes,hive,hive-thriftserver,scala-2.12 \\ -DskipTests \\ clean install Building PySpark-Related Operators \u00b6 ./build/mvn -DskipTests -pl :spark-sql_2.12 clean install cp sql/core/target/spark-sql_2.12-3.1.1.jar assembly/target/scala-2.12/jars/","title":"Building from Sources"},{"location":"building-from-sources/#building-from-sources","text":"$ java -version openjdk version \"11.0.10\" 2021-01-19 OpenJDK Runtime Environment AdoptOpenJDK (build 11.0.10+9) OpenJDK 64-Bit Server VM AdoptOpenJDK (build 11.0.10+9, mixed mode) ./build/mvn \\ -Pyarn,kubernetes,hive,hive-thriftserver,scala-2.12 \\ -DskipTests \\ clean install","title":"Building from Sources"},{"location":"building-from-sources/#building-pyspark-related-operators","text":"./build/mvn -DskipTests -pl :spark-sql_2.12 clean install cp sql/core/target/spark-sql_2.12-3.1.1.jar assembly/target/scala-2.12/jars/","title":"Building PySpark-Related Operators"},{"location":"configuration-properties/","text":"Configuration Properties \u00b6 spark.pyspark.driver.python \u00b6 Default: (undefined) spark.pyspark.python \u00b6 Default: (undefined) spark.python.use.daemon \u00b6 Because forking processes from Java is expensive, we prefer to launch a single Python daemon, pyspark/daemon.py (by default) and tell it to fork new workers for our tasks. This daemon currently only works on UNIX-based systems now because it uses signals for child management, so we can also fall back to launching workers, pyspark/worker.py (by default) directly. Default: true (always disabled on Windows) Used when PythonWorkerFactory is created spark.python.daemon.module \u00b6 Default: pyspark.daemon Used when PythonWorkerFactory is created spark.python.worker.module \u00b6 Default: (undefined) Used when PythonWorkerFactory is created","title":"Configuration Properties"},{"location":"configuration-properties/#configuration-properties","text":"","title":"Configuration Properties"},{"location":"configuration-properties/#sparkpysparkdriverpython","text":"Default: (undefined)","title":" spark.pyspark.driver.python"},{"location":"configuration-properties/#sparkpysparkpython","text":"Default: (undefined)","title":" spark.pyspark.python"},{"location":"configuration-properties/#sparkpythonusedaemon","text":"Because forking processes from Java is expensive, we prefer to launch a single Python daemon, pyspark/daemon.py (by default) and tell it to fork new workers for our tasks. This daemon currently only works on UNIX-based systems now because it uses signals for child management, so we can also fall back to launching workers, pyspark/worker.py (by default) directly. Default: true (always disabled on Windows) Used when PythonWorkerFactory is created","title":" spark.python.use.daemon"},{"location":"configuration-properties/#sparkpythondaemonmodule","text":"Default: pyspark.daemon Used when PythonWorkerFactory is created","title":" spark.python.daemon.module"},{"location":"configuration-properties/#sparkpythonworkermodule","text":"Default: (undefined) Used when PythonWorkerFactory is created","title":" spark.python.worker.module"},{"location":"environment-variables/","text":"Environment Variables \u00b6 PySpark uses environment variables to configure execution environment. PYSPARK_PIN_THREAD \u00b6 Default: false","title":"Environment Variables"},{"location":"environment-variables/#environment-variables","text":"PySpark uses environment variables to configure execution environment.","title":"Environment Variables"},{"location":"environment-variables/#pyspark_pin_thread","text":"Default: false","title":" PYSPARK_PIN_THREAD"},{"location":"overview/","text":"PySpark \u00b6 PySpark is the Python frontend for Apache Spark. shell.py \u00b6 pyspark shell defines PYTHONSTARTUP environment variable to execute shell.py before the first prompt is displayed in Python interactive mode. Py4J \u00b6 java_gateway uses Py4J - A Bridge between Python and Java : Py4J enables Python programs running in a Python interpreter to dynamically access Java objects in a Java Virtual Machine. Methods are called as if the Java objects resided in the Python interpreter and Java collections can be accessed through standard Python collection methods. Py4J also enables Java programs to call back Python objects. pyspark.sql Package \u00b6 pyspark.sql is a Python package for Spark SQL. from pyspark.sql import * Tip Learn more about Modules and Packages in Python in The Python Tutorial . __init__.py \u00b6 The __init__.py files are required to make Python treat directories containing the file as packages. Per 6.4.1. Importing * From a Package : The import statement uses the following convention: if a package's __init__.py code defines a list named __all__ , it is taken to be the list of module names that should be imported when from package import * is encountered. Per Public and Internal Interfaces in PEP 8 -- Style Guide for Python Code : To better support introspection, modules should explicitly declare the names in their public API using the __all__ attribute. From python/pyspark/sql/__init__.py : __all__ = [ 'SparkSession' , 'SQLContext' , 'HiveContext' , 'UDFRegistration' , 'DataFrame' , 'GroupedData' , 'Column' , 'Catalog' , 'Row' , 'DataFrameNaFunctions' , 'DataFrameStatFunctions' , 'Window' , 'WindowSpec' , 'DataFrameReader' , 'DataFrameWriter' , 'PandasCogroupedOps' ] pandas \u00b6 The minimum version of Pandas is 0.23.2 (and PandasConversionMixin asserts that). import pandas as pd pyarrow \u00b6 The minimum version of PyArrow is 1.0.0 (and PandasConversionMixin asserts that). import pyarrow Python Mixins \u00b6 From 8.7. Class definitions : classdef ::= [decorators] \"class\" classname [inheritance] \":\" suite The inheritance list usually gives a list of base classes PySpark uses mixins: PandasConversionMixin PandasMapOpsMixin SparkConversionMixin Pandas User Defined Functions \u00b6 Pandas User Defined Functions ( vectorized user defined functions ) are user-defined functions that are executed by Spark using Arrow to transfer data and Pandas to work with the data, which allows vectorized operations. Pandas UDFs are defined using pandas_udf function as a decorator (using @pandas_udf(returnType, functionType) annotation) or to wrap the function, and no additional configuration. A Pandas UDF behaves as a regular PySpark function API in general. The minimum versions supported: pandas 0.23.2 pyarrow 1.0.0 As of Spark 3.0 with Python 3.6+, using Python type hints to specify type hints for the pandas UDF is encouraged (instead of specifying pandas UDF type via functionType argument). The type hint should use pandas.Series in most cases (except pandas.DataFrame ).","title":"Overview"},{"location":"overview/#pyspark","text":"PySpark is the Python frontend for Apache Spark.","title":"PySpark"},{"location":"overview/#shellpy","text":"pyspark shell defines PYTHONSTARTUP environment variable to execute shell.py before the first prompt is displayed in Python interactive mode.","title":"shell.py"},{"location":"overview/#py4j","text":"java_gateway uses Py4J - A Bridge between Python and Java : Py4J enables Python programs running in a Python interpreter to dynamically access Java objects in a Java Virtual Machine. Methods are called as if the Java objects resided in the Python interpreter and Java collections can be accessed through standard Python collection methods. Py4J also enables Java programs to call back Python objects.","title":"Py4J"},{"location":"overview/#pysparksql-package","text":"pyspark.sql is a Python package for Spark SQL. from pyspark.sql import * Tip Learn more about Modules and Packages in Python in The Python Tutorial .","title":"pyspark.sql Package"},{"location":"overview/#__init__py","text":"The __init__.py files are required to make Python treat directories containing the file as packages. Per 6.4.1. Importing * From a Package : The import statement uses the following convention: if a package's __init__.py code defines a list named __all__ , it is taken to be the list of module names that should be imported when from package import * is encountered. Per Public and Internal Interfaces in PEP 8 -- Style Guide for Python Code : To better support introspection, modules should explicitly declare the names in their public API using the __all__ attribute. From python/pyspark/sql/__init__.py : __all__ = [ 'SparkSession' , 'SQLContext' , 'HiveContext' , 'UDFRegistration' , 'DataFrame' , 'GroupedData' , 'Column' , 'Catalog' , 'Row' , 'DataFrameNaFunctions' , 'DataFrameStatFunctions' , 'Window' , 'WindowSpec' , 'DataFrameReader' , 'DataFrameWriter' , 'PandasCogroupedOps' ]","title":"__init__.py"},{"location":"overview/#pandas","text":"The minimum version of Pandas is 0.23.2 (and PandasConversionMixin asserts that). import pandas as pd","title":"pandas"},{"location":"overview/#pyarrow","text":"The minimum version of PyArrow is 1.0.0 (and PandasConversionMixin asserts that). import pyarrow","title":"pyarrow"},{"location":"overview/#python-mixins","text":"From 8.7. Class definitions : classdef ::= [decorators] \"class\" classname [inheritance] \":\" suite The inheritance list usually gives a list of base classes PySpark uses mixins: PandasConversionMixin PandasMapOpsMixin SparkConversionMixin","title":"Python Mixins"},{"location":"overview/#pandas-user-defined-functions","text":"Pandas User Defined Functions ( vectorized user defined functions ) are user-defined functions that are executed by Spark using Arrow to transfer data and Pandas to work with the data, which allows vectorized operations. Pandas UDFs are defined using pandas_udf function as a decorator (using @pandas_udf(returnType, functionType) annotation) or to wrap the function, and no additional configuration. A Pandas UDF behaves as a regular PySpark function API in general. The minimum versions supported: pandas 0.23.2 pyarrow 1.0.0 As of Spark 3.0 with Python 3.6+, using Python type hints to specify type hints for the pandas UDF is encouraged (instead of specifying pandas UDF type via functionType argument). The type hint should use pandas.Series in most cases (except pandas.DataFrame ).","title":"Pandas User Defined Functions"},{"location":"pyspark/","text":"PySpark Shell \u00b6 pyspark is a Spark shell for Python.","title":"PySpark Shell"},{"location":"pyspark/#pyspark-shell","text":"pyspark is a Spark shell for Python.","title":"PySpark Shell"},{"location":"spark-logging/","text":"Logging \u00b6 Spark uses log4j for logging. Note Learn more on Spark Logging in The Internals of Apache Spark online book.","title":"Logging"},{"location":"spark-logging/#logging","text":"Spark uses log4j for logging. Note Learn more on Spark Logging in The Internals of Apache Spark online book.","title":"Logging"},{"location":"demo/executing-pyspark-applications-using-spark-submit/","text":"Demo: Executing PySpark Applications Using spark-submit \u00b6 PySpark applications are executed using spark-submit ( Spark Core ) command-line application. spark-submit 1.py extra args For a PySpark application, spark-submit uses PythonRunner and launches an extra python process: ps -o pid,ppid,command | grep python | grep -v grep org.apache.spark.deploy.SparkSubmit 1.py extra args Python /usr/local/bin/ipython 1.py extra args SPARK_PRINT_LAUNCH_COMMAND Environment Variable \u00b6 Use SPARK_PRINT_LAUNCH_COMMAND environment variable to have the complete Spark command printed out to the standard output (cf. spark-submit shell script ). SPARK_PRINT_LAUNCH_COMMAND=1 spark-submit 1.py extra args verbose Option \u00b6 Use --verbose option for verbose debugging output. Parsed arguments: ... pyFiles null ... primaryResource file:/Users/jacek/dev/sandbox/python-sandbox/1.py name 1.py childArgs [extra args] ... Main class: org.apache.spark.deploy.PythonRunner Arguments: file:/Users/jacek/dev/sandbox/python-sandbox/1.py null extra args Spark config: (spark.app.name,1.py) (spark.master,local[*]) (spark.submit.pyFiles,) (spark.submit.deployMode,client)","title":"Executing PySpark Applications Using spark-submit"},{"location":"demo/executing-pyspark-applications-using-spark-submit/#demo-executing-pyspark-applications-using-spark-submit","text":"PySpark applications are executed using spark-submit ( Spark Core ) command-line application. spark-submit 1.py extra args For a PySpark application, spark-submit uses PythonRunner and launches an extra python process: ps -o pid,ppid,command | grep python | grep -v grep org.apache.spark.deploy.SparkSubmit 1.py extra args Python /usr/local/bin/ipython 1.py extra args","title":"Demo: Executing PySpark Applications Using spark-submit"},{"location":"demo/executing-pyspark-applications-using-spark-submit/#spark_print_launch_command-environment-variable","text":"Use SPARK_PRINT_LAUNCH_COMMAND environment variable to have the complete Spark command printed out to the standard output (cf. spark-submit shell script ). SPARK_PRINT_LAUNCH_COMMAND=1 spark-submit 1.py extra args","title":"SPARK_PRINT_LAUNCH_COMMAND Environment Variable"},{"location":"demo/executing-pyspark-applications-using-spark-submit/#verbose-option","text":"Use --verbose option for verbose debugging output. Parsed arguments: ... pyFiles null ... primaryResource file:/Users/jacek/dev/sandbox/python-sandbox/1.py name 1.py childArgs [extra args] ... Main class: org.apache.spark.deploy.PythonRunner Arguments: file:/Users/jacek/dev/sandbox/python-sandbox/1.py null extra args Spark config: (spark.app.name,1.py) (spark.master,local[*]) (spark.submit.pyFiles,) (spark.submit.deployMode,client)","title":"verbose Option"},{"location":"pyspark/java_gateway/","text":"java_gateway.py \u00b6 java_gateway is a Python module that allows launching a gateway process to establish communication channel to Py4JServer . launch_gateway \u00b6 launch_gateway ( conf = None , popen_kwargs = None ) launch_gateway reads PYSPARK_GATEWAY_PORT and PYSPARK_GATEWAY_SECRET environment variables if defined and assumes that the child Java gateway process has already been started (e.g. PythonGatewayServer ). Otherwise, launch_gateway builds the command to start spark-submit : Finds SPARK_HOME with ./bin/spark-submit Appends all the configuration properties (from the input conf ) using --conf Appends PYSPARK_SUBMIT_ARGS environment variable if defined or assumes pyspark-shell launch_gateway sets up _PYSPARK_DRIVER_CONN_INFO_PATH environment variable to point at an unique temporary file. launch_gateway configures a pipe to stdin for the corresponding Java gateway process to use to monitor the Python process. launch_gateway starts bin/spark-submit command and waits for a connection info file to be created at _PYSPARK_DRIVER_CONN_INFO_PATH . launch_gateway reads the port and the secret from the file once available. launch_gateway connects to the gateway using py4j's ClientServer or JavaGateway based on PYSPARK_PIN_THREAD environment variable (default: false ). PYSPARK_PIN_THREAD Gateway true ClientServer false JavaGateway launch_gateway imports Spark packages and classes (using py4j): org.apache.spark.SparkConf org.apache.spark.api.java.* org.apache.spark.api.python.* org.apache.spark.ml.python.* org.apache.spark.mllib.api.python.* org.apache.spark.resource.* org.apache.spark.sql.* org.apache.spark.sql.api.python.* org.apache.spark.sql.hive.* scala.Tuple2 launch_gateway is used when: SparkContext is requested to _ensure_initialized","title":"java_gateway.py"},{"location":"pyspark/java_gateway/#java_gatewaypy","text":"java_gateway is a Python module that allows launching a gateway process to establish communication channel to Py4JServer .","title":"java_gateway.py"},{"location":"pyspark/java_gateway/#launch_gateway","text":"launch_gateway ( conf = None , popen_kwargs = None ) launch_gateway reads PYSPARK_GATEWAY_PORT and PYSPARK_GATEWAY_SECRET environment variables if defined and assumes that the child Java gateway process has already been started (e.g. PythonGatewayServer ). Otherwise, launch_gateway builds the command to start spark-submit : Finds SPARK_HOME with ./bin/spark-submit Appends all the configuration properties (from the input conf ) using --conf Appends PYSPARK_SUBMIT_ARGS environment variable if defined or assumes pyspark-shell launch_gateway sets up _PYSPARK_DRIVER_CONN_INFO_PATH environment variable to point at an unique temporary file. launch_gateway configures a pipe to stdin for the corresponding Java gateway process to use to monitor the Python process. launch_gateway starts bin/spark-submit command and waits for a connection info file to be created at _PYSPARK_DRIVER_CONN_INFO_PATH . launch_gateway reads the port and the secret from the file once available. launch_gateway connects to the gateway using py4j's ClientServer or JavaGateway based on PYSPARK_PIN_THREAD environment variable (default: false ). PYSPARK_PIN_THREAD Gateway true ClientServer false JavaGateway launch_gateway imports Spark packages and classes (using py4j): org.apache.spark.SparkConf org.apache.spark.api.java.* org.apache.spark.api.python.* org.apache.spark.ml.python.* org.apache.spark.mllib.api.python.* org.apache.spark.resource.* org.apache.spark.sql.* org.apache.spark.sql.api.python.* org.apache.spark.sql.hive.* scala.Tuple2 launch_gateway is used when: SparkContext is requested to _ensure_initialized","title":" launch_gateway"},{"location":"pyspark/shell/","text":"shell.py \u00b6 shell.py script is the interactive shell of PySpark. shell.py defines the following variables: sc being pyspark.SparkContext spark being pyspark.sql.session.SparkSession sql being SparkSession.sql sqlContext and sqlCtx for compatibility","title":"shell.py"},{"location":"pyspark/shell/#shellpy","text":"shell.py script is the interactive shell of PySpark. shell.py defines the following variables: sc being pyspark.SparkContext spark being pyspark.sql.session.SparkSession sql being SparkSession.sql sqlContext and sqlCtx for compatibility","title":"shell.py"},{"location":"pyspark/sql/functions/","text":"functions.py \u00b6 functions.py module belongs to pyspark.sql package. udf \u00b6 udf ( f = None , returnType = StringType ()) udf ...FIXME","title":"functions.py"},{"location":"pyspark/sql/functions/#functionspy","text":"functions.py module belongs to pyspark.sql package.","title":"functions.py"},{"location":"pyspark/sql/functions/#udf","text":"udf ( f = None , returnType = StringType ()) udf ...FIXME","title":" udf"},{"location":"pyspark/sql/udf/","text":"udf.py \u00b6 udf.py module belongs to pyspark.sql package. Public Objects \u00b6 udf module defines UDFRegistration as the public object. _create_udf \u00b6 _create_udf ( f , returnType , evalType ) _create_udf creates an UserDefinedFunction (with the name of the object to be the name of function f ). _create_udf is used when: pandas_udf function is used udf function is used","title":"udf.py"},{"location":"pyspark/sql/udf/#udfpy","text":"udf.py module belongs to pyspark.sql package.","title":"udf.py"},{"location":"pyspark/sql/udf/#public-objects","text":"udf module defines UDFRegistration as the public object.","title":" Public Objects"},{"location":"pyspark/sql/udf/#_create_udf","text":"_create_udf ( f , returnType , evalType ) _create_udf creates an UserDefinedFunction (with the name of the object to be the name of function f ). _create_udf is used when: pandas_udf function is used udf function is used","title":" _create_udf"},{"location":"pyspark/sql/pandas/functions/","text":"functions.py \u00b6 functions.py defines pandas_udf for pandas user defined functions . functions.py is part of pyspark.sql.pandas package. from pyspark.sql.functions import pandas_udf pandas_udf \u00b6 pandas_udf ( f = None , returnType = None , functionType = None ) pandas_udf creates a pandas user defined functions . functionType must be one the values from PandasUDFType : PythonEvalType.SQL_SCALAR_PANDAS_UDF PythonEvalType.SQL_SCALAR_PANDAS_ITER_UDF PythonEvalType.SQL_GROUPED_MAP_PANDAS_UDF PythonEvalType.SQL_GROUPED_AGG_PANDAS_UDF PythonEvalType.SQL_MAP_PANDAS_ITER_UDF PythonEvalType.SQL_COGROUPED_MAP_PANDAS_UDF","title":"functions.py"},{"location":"pyspark/sql/pandas/functions/#functionspy","text":"functions.py defines pandas_udf for pandas user defined functions . functions.py is part of pyspark.sql.pandas package. from pyspark.sql.functions import pandas_udf","title":"functions.py"},{"location":"pyspark/sql/pandas/functions/#pandas_udf","text":"pandas_udf ( f = None , returnType = None , functionType = None ) pandas_udf creates a pandas user defined functions . functionType must be one the values from PandasUDFType : PythonEvalType.SQL_SCALAR_PANDAS_UDF PythonEvalType.SQL_SCALAR_PANDAS_ITER_UDF PythonEvalType.SQL_GROUPED_MAP_PANDAS_UDF PythonEvalType.SQL_GROUPED_AGG_PANDAS_UDF PythonEvalType.SQL_MAP_PANDAS_ITER_UDF PythonEvalType.SQL_COGROUPED_MAP_PANDAS_UDF","title":" pandas_udf"},{"location":"runners/ArrowPythonRunner/","text":"ArrowPythonRunner \u00b6 ArrowPythonRunner is a pyspark runner with PythonArrowOutput for...FIXME Creating Instance \u00b6 ArrowPythonRunner takes the following to be created: Seq[ChainedPythonFunctions] Eval Type Argument Offsets ( Array[Array[Int]] ) Schema ( Spark SQL ) TimeZone ID Configuration ( Map[String, String] ) ArrowPythonRunner is created when AggregateInPandasExec , ArrowEvalPythonExec , FlatMapGroupsInPandasExec , MapInPandasExec , WindowInPandasExec physical operators are executed.","title":"ArrowPythonRunner"},{"location":"runners/ArrowPythonRunner/#arrowpythonrunner","text":"ArrowPythonRunner is a pyspark runner with PythonArrowOutput for...FIXME","title":"ArrowPythonRunner"},{"location":"runners/ArrowPythonRunner/#creating-instance","text":"ArrowPythonRunner takes the following to be created: Seq[ChainedPythonFunctions] Eval Type Argument Offsets ( Array[Array[Int]] ) Schema ( Spark SQL ) TimeZone ID Configuration ( Map[String, String] ) ArrowPythonRunner is created when AggregateInPandasExec , ArrowEvalPythonExec , FlatMapGroupsInPandasExec , MapInPandasExec , WindowInPandasExec physical operators are executed.","title":"Creating Instance"},{"location":"runners/BasePythonRunner/","text":"BasePythonRunner \u00b6 BasePythonRunner is an abstraction of Python Runners . Contract \u00b6 newReaderIterator \u00b6 newReaderIterator ( stream : DataInputStream , writerThread : WriterThread , startTime : Long , env : SparkEnv , worker : Socket , releasedOrClosed : AtomicBoolean , context : TaskContext ) : Iterator [ OUT ] Used when BasePythonRunner is requested to compute newWriterThread \u00b6 newWriterThread ( env : SparkEnv , worker : Socket , inputIterator : Iterator [ IN ], partitionIndex : Int , context : TaskContext ) : WriterThread Used when BasePythonRunner is requested to compute Implementations \u00b6 ArrowPythonRunner CoGroupedArrowPythonRunner PythonRunner PythonUDFRunner Scala Definition \u00b6 BasePythonRunner is a type constructor in Scala ( generic class in Java) with the following definition: abstract class BasePythonRunner [ IN , OUT ](...) { // ... } BasePythonRunner uses IN and OUT as the name of the types for the input and output values. Creating Instance \u00b6 BasePythonRunner takes the following to be created: ChainedPythonFunctions Eval Type Argument Offsets BasePythonRunner requires that the number of ChainedPythonFunctions and Argument Offsets are the same. Abstract Class BasePythonRunner is an abstract class and cannot be created directly. It is created indirectly for the concrete BasePythonRunners . Computing Result \u00b6 compute ( inputIterator : Iterator [ IN ], partitionIndex : Int , context : TaskContext ) : Iterator [ OUT ] compute makes sure that spark.executorEnv.OMP_NUM_THREADS configuration option is set or defaults to spark.executor.cores property. compute defines the following environment variables: SPARK_LOCAL_DIRS to be the local directories of the local DiskBlockManager SPARK_BUFFER_SIZE to be the value of spark.buffer.size configuration property (default: 65536 ) compute can optionally define environment variables: SPARK_REUSE_WORKER to be 1 based on spark.python.worker.reuse configuration property (default: true ) PYSPARK_EXECUTOR_MEMORY_MB to be the value of spark.executor.pyspark.memory configuration property if defined compute requests the executor's SparkEnv to createPythonWorker (for a pythonExec and the environment variables) that requests PythonWorkerFactory to create a Python worker (and give a java.net.Socket ). FIXME Describe pythonExec . compute [newWriterThread] with the Python worker and the input arguments. compute creates and starts a MonitorThread to watch the Python worker. compute creates a new reader iterator to read lines from the Python worker's stdout. compute is used when: PythonRDD is requested to compute AggregateInPandasExec , ArrowEvalPythonExec , BatchEvalPythonExec , FlatMapCoGroupsInPandasExec , FlatMapGroupsInPandasExec MapInPandasExec , WindowInPandasExec physical operators are executed","title":"BasePythonRunner"},{"location":"runners/BasePythonRunner/#basepythonrunner","text":"BasePythonRunner is an abstraction of Python Runners .","title":"BasePythonRunner"},{"location":"runners/BasePythonRunner/#contract","text":"","title":"Contract"},{"location":"runners/BasePythonRunner/#newreaderiterator","text":"newReaderIterator ( stream : DataInputStream , writerThread : WriterThread , startTime : Long , env : SparkEnv , worker : Socket , releasedOrClosed : AtomicBoolean , context : TaskContext ) : Iterator [ OUT ] Used when BasePythonRunner is requested to compute","title":" newReaderIterator"},{"location":"runners/BasePythonRunner/#newwriterthread","text":"newWriterThread ( env : SparkEnv , worker : Socket , inputIterator : Iterator [ IN ], partitionIndex : Int , context : TaskContext ) : WriterThread Used when BasePythonRunner is requested to compute","title":" newWriterThread"},{"location":"runners/BasePythonRunner/#implementations","text":"ArrowPythonRunner CoGroupedArrowPythonRunner PythonRunner PythonUDFRunner","title":"Implementations"},{"location":"runners/BasePythonRunner/#scala-definition","text":"BasePythonRunner is a type constructor in Scala ( generic class in Java) with the following definition: abstract class BasePythonRunner [ IN , OUT ](...) { // ... } BasePythonRunner uses IN and OUT as the name of the types for the input and output values.","title":"Scala Definition"},{"location":"runners/BasePythonRunner/#creating-instance","text":"BasePythonRunner takes the following to be created: ChainedPythonFunctions Eval Type Argument Offsets BasePythonRunner requires that the number of ChainedPythonFunctions and Argument Offsets are the same. Abstract Class BasePythonRunner is an abstract class and cannot be created directly. It is created indirectly for the concrete BasePythonRunners .","title":"Creating Instance"},{"location":"runners/BasePythonRunner/#computing-result","text":"compute ( inputIterator : Iterator [ IN ], partitionIndex : Int , context : TaskContext ) : Iterator [ OUT ] compute makes sure that spark.executorEnv.OMP_NUM_THREADS configuration option is set or defaults to spark.executor.cores property. compute defines the following environment variables: SPARK_LOCAL_DIRS to be the local directories of the local DiskBlockManager SPARK_BUFFER_SIZE to be the value of spark.buffer.size configuration property (default: 65536 ) compute can optionally define environment variables: SPARK_REUSE_WORKER to be 1 based on spark.python.worker.reuse configuration property (default: true ) PYSPARK_EXECUTOR_MEMORY_MB to be the value of spark.executor.pyspark.memory configuration property if defined compute requests the executor's SparkEnv to createPythonWorker (for a pythonExec and the environment variables) that requests PythonWorkerFactory to create a Python worker (and give a java.net.Socket ). FIXME Describe pythonExec . compute [newWriterThread] with the Python worker and the input arguments. compute creates and starts a MonitorThread to watch the Python worker. compute creates a new reader iterator to read lines from the Python worker's stdout. compute is used when: PythonRDD is requested to compute AggregateInPandasExec , ArrowEvalPythonExec , BatchEvalPythonExec , FlatMapCoGroupsInPandasExec , FlatMapGroupsInPandasExec MapInPandasExec , WindowInPandasExec physical operators are executed","title":" Computing Result"},{"location":"runners/PythonRunner/","text":"PythonRunner \u00b6 PythonRunner is a concrete BasePythonRunner . Creating Instance \u00b6 PythonRunner takes the following to be created: Seq[ChainedPythonFunctions] PythonRunner is created (indirectly using apply factory method) when: PythonRDD is requested to compute a partition PythonForeachWriter is requested for a PythonRunner Creating PythonRunner \u00b6 apply ( func : PythonFunction ) : PythonRunner apply simply creates a PythonRunner for the PythonFunction . apply is used when: PythonRDD is requested to compute a partition PythonForeachWriter is requested for a PythonRunner","title":"PythonRunner"},{"location":"runners/PythonRunner/#pythonrunner","text":"PythonRunner is a concrete BasePythonRunner .","title":"PythonRunner"},{"location":"runners/PythonRunner/#creating-instance","text":"PythonRunner takes the following to be created: Seq[ChainedPythonFunctions] PythonRunner is created (indirectly using apply factory method) when: PythonRDD is requested to compute a partition PythonForeachWriter is requested for a PythonRunner","title":"Creating Instance"},{"location":"runners/PythonRunner/#creating-pythonrunner","text":"apply ( func : PythonFunction ) : PythonRunner apply simply creates a PythonRunner for the PythonFunction . apply is used when: PythonRDD is requested to compute a partition PythonForeachWriter is requested for a PythonRunner","title":" Creating PythonRunner"},{"location":"runners/PythonUDFRunner/","text":"PythonUDFRunner \u00b6 PythonUDFRunner is...FIXME","title":"PythonUDFRunner"},{"location":"runners/PythonUDFRunner/#pythonudfrunner","text":"PythonUDFRunner is...FIXME","title":"PythonUDFRunner"},{"location":"sql/","text":"PySpark SQL \u00b6","title":"PySpark SQL"},{"location":"sql/#pyspark-sql","text":"","title":"PySpark SQL"},{"location":"sql/AggregateInPandasExec/","text":"AggregateInPandasExec Physical Operator \u00b6 AggregateInPandasExec is a unary physical operator ( Spark SQL ). Creating Instance \u00b6 AggregateInPandasExec takes the following to be created: Grouping Expressions ( Spark SQL ) ( Seq[NamedExpression] ) PythonUDF s Result Named Expressions ( Spark SQL ) ( Seq[NamedExpression] ) Child Physical Operator ( Spark SQL ) AggregateInPandasExec is created when Aggregation execution planning strategy ( Spark SQL ) is executed for Aggregate logical operators ( Spark SQL ) with PythonUDF aggregate expressions only. Executing Operator \u00b6 doExecute () : RDD [ InternalRow ] doExecute uses ArrowPythonRunner (one per partition) to execute PythonUDFs . doExecute is part of the SparkPlan ( Spark SQL ) abstraction.","title":"AggregateInPandasExec"},{"location":"sql/AggregateInPandasExec/#aggregateinpandasexec-physical-operator","text":"AggregateInPandasExec is a unary physical operator ( Spark SQL ).","title":"AggregateInPandasExec Physical Operator"},{"location":"sql/AggregateInPandasExec/#creating-instance","text":"AggregateInPandasExec takes the following to be created: Grouping Expressions ( Spark SQL ) ( Seq[NamedExpression] ) PythonUDF s Result Named Expressions ( Spark SQL ) ( Seq[NamedExpression] ) Child Physical Operator ( Spark SQL ) AggregateInPandasExec is created when Aggregation execution planning strategy ( Spark SQL ) is executed for Aggregate logical operators ( Spark SQL ) with PythonUDF aggregate expressions only.","title":"Creating Instance"},{"location":"sql/AggregateInPandasExec/#executing-operator","text":"doExecute () : RDD [ InternalRow ] doExecute uses ArrowPythonRunner (one per partition) to execute PythonUDFs . doExecute is part of the SparkPlan ( Spark SQL ) abstraction.","title":" Executing Operator"},{"location":"sql/DataFrame/","text":"DataFrame \u00b6 DataFrame is a Python class with PandasMapOpsMixin and PandasConversionMixin mixins. DataFrame lives in pyspark.sql.dataframe module (together with DataFrameNaFunctions and DataFrameStatFunctions ). from pyspark.sql.dataframe import DataFrame Creating Instance \u00b6 DataFrame takes the following to be created: jdf SQLContext groupBy \u00b6 groupBy ( self , * cols ) groupBy requests the _jdf to groupBy and creates a GroupedData with it.","title":"DataFrame"},{"location":"sql/DataFrame/#dataframe","text":"DataFrame is a Python class with PandasMapOpsMixin and PandasConversionMixin mixins. DataFrame lives in pyspark.sql.dataframe module (together with DataFrameNaFunctions and DataFrameStatFunctions ). from pyspark.sql.dataframe import DataFrame","title":"DataFrame"},{"location":"sql/DataFrame/#creating-instance","text":"DataFrame takes the following to be created: jdf SQLContext","title":"Creating Instance"},{"location":"sql/DataFrame/#groupby","text":"groupBy ( self , * cols ) groupBy requests the _jdf to groupBy and creates a GroupedData with it.","title":" groupBy"},{"location":"sql/FlatMapGroupsInPandas/","text":"FlatMapGroupsInPandas \u00b6 FlatMapGroupsInPandas is a unary logical operator ( Spark SQL ). FlatMapGroupsInPandas is planned as a FlatMapGroupsInPandasExec physical operator. Creating Instance \u00b6 FlatMapGroupsInPandas takes the following to be created: Grouping Attributes ( Spark SQL ) Function Expression ( Spark SQL ) Output Attributes ( Spark SQL ) Child Logical Operator ( Spark SQL ) FlatMapGroupsInPandas is created when: RelationalGroupedDataset is requested to flatMapGroupsInPandas (with a PythonUDF )","title":"FlatMapGroupsInPandas"},{"location":"sql/FlatMapGroupsInPandas/#flatmapgroupsinpandas","text":"FlatMapGroupsInPandas is a unary logical operator ( Spark SQL ). FlatMapGroupsInPandas is planned as a FlatMapGroupsInPandasExec physical operator.","title":"FlatMapGroupsInPandas"},{"location":"sql/FlatMapGroupsInPandas/#creating-instance","text":"FlatMapGroupsInPandas takes the following to be created: Grouping Attributes ( Spark SQL ) Function Expression ( Spark SQL ) Output Attributes ( Spark SQL ) Child Logical Operator ( Spark SQL ) FlatMapGroupsInPandas is created when: RelationalGroupedDataset is requested to flatMapGroupsInPandas (with a PythonUDF )","title":"Creating Instance"},{"location":"sql/FlatMapGroupsInPandasExec/","text":"FlatMapGroupsInPandasExec \u00b6 FlatMapGroupsInPandasExec is a unary physical operator ( Spark SQL ) to execute a PythonUDF . FlatMapGroupsInPandasExec represents a FlatMapGroupsInPandas logical operator at execution time. Creating Instance \u00b6 FlatMapGroupsInPandasExec takes the following to be created: Grouping Attributes ( Spark SQL ) Function Expression ( Spark SQL ) Output Attributes ( Spark SQL ) Child Physical Operator ( Spark SQL ) FlatMapGroupsInPandasExec is created when: BasicOperators ( Spark SQL ) execution planning strategy is executed (on a logical query plan with FlatMapGroupsInPandas logical operators)","title":"FlatMapGroupsInPandasExec"},{"location":"sql/FlatMapGroupsInPandasExec/#flatmapgroupsinpandasexec","text":"FlatMapGroupsInPandasExec is a unary physical operator ( Spark SQL ) to execute a PythonUDF . FlatMapGroupsInPandasExec represents a FlatMapGroupsInPandas logical operator at execution time.","title":"FlatMapGroupsInPandasExec"},{"location":"sql/FlatMapGroupsInPandasExec/#creating-instance","text":"FlatMapGroupsInPandasExec takes the following to be created: Grouping Attributes ( Spark SQL ) Function Expression ( Spark SQL ) Output Attributes ( Spark SQL ) Child Physical Operator ( Spark SQL ) FlatMapGroupsInPandasExec is created when: BasicOperators ( Spark SQL ) execution planning strategy is executed (on a logical query plan with FlatMapGroupsInPandas logical operators)","title":"Creating Instance"},{"location":"sql/GroupedData/","text":"GroupedData \u00b6 GroupedData is created by DataFrame.groupBy . GroupedData is a Python class with PandasGroupedOpsMixin mixin. GroupedData is defined in pyspark.sql.group module. from pyspark.sql.group import GroupedData Creating Instance \u00b6 GroupedData takes the following to be created: RelationalGroupedDataset DataFrame GroupedData is created when: FIXME","title":"GroupedData"},{"location":"sql/GroupedData/#groupeddata","text":"GroupedData is created by DataFrame.groupBy . GroupedData is a Python class with PandasGroupedOpsMixin mixin. GroupedData is defined in pyspark.sql.group module. from pyspark.sql.group import GroupedData","title":"GroupedData"},{"location":"sql/GroupedData/#creating-instance","text":"GroupedData takes the following to be created: RelationalGroupedDataset DataFrame GroupedData is created when: FIXME","title":"Creating Instance"},{"location":"sql/PandasCogroupedOps/","text":"PandasCogroupedOps \u00b6 PandasCogroupedOps is a logical grouping created by GroupedData.cogroup . from pyspark.sql.pandas.group_ops import PandasCogroupedOps PandasCogroupedOps is included in __all__ of pyspark.sql module (via __init__.py ). Creating Instance \u00b6 PandasCogroupedOps takes the following to be created: GroupedData GroupedData PandasCogroupedOps is created when: PandasGroupedOpsMixin is requested to cogroup applyInPandas \u00b6 applyInPandas ( self , func , schema ) applyInPandas creates a DataFrame with the result of flatMapCoGroupsInPandas with a pandas user defined function of SQL_COGROUPED_MAP_PANDAS_UDF type. applyInPandas creates a pandas user defined function for the given func and the return type by the given schema . The pandas UDF is of SQL_COGROUPED_MAP_PANDAS_UDF type. applyInPandas applies the pandas UDF on all the columns of the two GroupedData s (that creates a Column expression). applyInPandas requests the GroupedData for the associated RelationalGroupedDataset that is in turn requested to flatMapCoGroupsInPandas .","title":"PandasCogroupedOps"},{"location":"sql/PandasCogroupedOps/#pandascogroupedops","text":"PandasCogroupedOps is a logical grouping created by GroupedData.cogroup . from pyspark.sql.pandas.group_ops import PandasCogroupedOps PandasCogroupedOps is included in __all__ of pyspark.sql module (via __init__.py ).","title":"PandasCogroupedOps"},{"location":"sql/PandasCogroupedOps/#creating-instance","text":"PandasCogroupedOps takes the following to be created: GroupedData GroupedData PandasCogroupedOps is created when: PandasGroupedOpsMixin is requested to cogroup","title":"Creating Instance"},{"location":"sql/PandasCogroupedOps/#applyinpandas","text":"applyInPandas ( self , func , schema ) applyInPandas creates a DataFrame with the result of flatMapCoGroupsInPandas with a pandas user defined function of SQL_COGROUPED_MAP_PANDAS_UDF type. applyInPandas creates a pandas user defined function for the given func and the return type by the given schema . The pandas UDF is of SQL_COGROUPED_MAP_PANDAS_UDF type. applyInPandas applies the pandas UDF on all the columns of the two GroupedData s (that creates a Column expression). applyInPandas requests the GroupedData for the associated RelationalGroupedDataset that is in turn requested to flatMapCoGroupsInPandas .","title":" applyInPandas"},{"location":"sql/PandasConversionMixin/","text":"PandasConversionMixin \u00b6 PandasConversionMixin is a Python mixin for DataFrame class. toPandas \u00b6 toPandas ( self ) toPandas can only be used with DataFrame (from pyspark.sql.dataframe ). toPandas ...FIXME","title":"PandasConversionMixin"},{"location":"sql/PandasConversionMixin/#pandasconversionmixin","text":"PandasConversionMixin is a Python mixin for DataFrame class.","title":"PandasConversionMixin"},{"location":"sql/PandasConversionMixin/#topandas","text":"toPandas ( self ) toPandas can only be used with DataFrame (from pyspark.sql.dataframe ). toPandas ...FIXME","title":" toPandas"},{"location":"sql/PandasGroupedOpsMixin/","text":"PandasGroupedOpsMixin \u00b6 PandasGroupedOpsMixin is a Python mixin for GroupedData class. applyInPandas \u00b6 applyInPandas ( self , func , schema ) applyInPandas creates a DataFrame with flatMapGroupsInPandas . Example \u00b6 df1 = spark . createDataFrame ( [( 20000101 , 1 , 1.0 ), ( 20000101 , 2 , 2.0 ), ( 20000102 , 1 , 3.0 ), ( 20000102 , 2 , 4.0 )], ( \"time\" , \"id\" , \"v1\" )) df2 = spark . createDataFrame ( [( 20000101 , 1 , \"x\" ), ( 20000101 , 2 , \"y\" )], ( \"time\" , \"id\" , \"v2\" )) import pandas as pd def asof_join ( k , l , r ): if k == ( 1 ,): return pd . merge_asof ( l , r , on = \"time\" , by = \"id\" ) else : return pd . DataFrame ( columns = [ 'time' , 'id' , 'v1' , 'v2' ]) gd1 = df1 . groupby ( \"id\" ) gd2 = df2 . groupby ( \"id\" ) gd1 . cogroup ( gd2 ) . applyInPandas ( asof_join , \"time int, id int, v1 double, v2 string\" ) . show () cogroup \u00b6 cogroup ( self , other ) cogroup creates a PandasCogroupedOps .","title":"PandasGroupedOpsMixin"},{"location":"sql/PandasGroupedOpsMixin/#pandasgroupedopsmixin","text":"PandasGroupedOpsMixin is a Python mixin for GroupedData class.","title":"PandasGroupedOpsMixin"},{"location":"sql/PandasGroupedOpsMixin/#applyinpandas","text":"applyInPandas ( self , func , schema ) applyInPandas creates a DataFrame with flatMapGroupsInPandas .","title":" applyInPandas"},{"location":"sql/PandasGroupedOpsMixin/#example","text":"df1 = spark . createDataFrame ( [( 20000101 , 1 , 1.0 ), ( 20000101 , 2 , 2.0 ), ( 20000102 , 1 , 3.0 ), ( 20000102 , 2 , 4.0 )], ( \"time\" , \"id\" , \"v1\" )) df2 = spark . createDataFrame ( [( 20000101 , 1 , \"x\" ), ( 20000101 , 2 , \"y\" )], ( \"time\" , \"id\" , \"v2\" )) import pandas as pd def asof_join ( k , l , r ): if k == ( 1 ,): return pd . merge_asof ( l , r , on = \"time\" , by = \"id\" ) else : return pd . DataFrame ( columns = [ 'time' , 'id' , 'v1' , 'v2' ]) gd1 = df1 . groupby ( \"id\" ) gd2 = df2 . groupby ( \"id\" ) gd1 . cogroup ( gd2 ) . applyInPandas ( asof_join , \"time int, id int, v1 double, v2 string\" ) . show ()","title":" Example"},{"location":"sql/PandasGroupedOpsMixin/#cogroup","text":"cogroup ( self , other ) cogroup creates a PandasCogroupedOps .","title":" cogroup"},{"location":"sql/PandasMapOpsMixin/","text":"PandasMapOpsMixin \u00b6 PandasMapOpsMixin is a Python mixin for DataFrame class.","title":"PandasMapOpsMixin"},{"location":"sql/PandasMapOpsMixin/#pandasmapopsmixin","text":"PandasMapOpsMixin is a Python mixin for DataFrame class.","title":"PandasMapOpsMixin"},{"location":"sql/PythonUDF/","text":"PythonUDF \u00b6 PythonUDF is a Catalyst expression ( Spark SQL ). Creating Instance \u00b6 PythonUDF takes the following to be created: Name PythonFunction DataType ( Spark SQL ) Children Catalyst Expressions ( Spark SQL ) Python Eval Type udfDeterministic flag Result ID ( ExprId ) PythonUDF is created when: UserDefinedPythonFunction is requested to builder Unevaluable \u00b6 PythonUDF is an Unevaluable expression ( Spark SQL ). NonSQLExpression \u00b6 PythonUDF is a NonSQLExpression expression ( Spark SQL ). UserDefinedExpression \u00b6 PythonUDF is a UserDefinedExpression expression ( Spark SQL ).","title":"PythonUDF"},{"location":"sql/PythonUDF/#pythonudf","text":"PythonUDF is a Catalyst expression ( Spark SQL ).","title":"PythonUDF"},{"location":"sql/PythonUDF/#creating-instance","text":"PythonUDF takes the following to be created: Name PythonFunction DataType ( Spark SQL ) Children Catalyst Expressions ( Spark SQL ) Python Eval Type udfDeterministic flag Result ID ( ExprId ) PythonUDF is created when: UserDefinedPythonFunction is requested to builder","title":"Creating Instance"},{"location":"sql/PythonUDF/#unevaluable","text":"PythonUDF is an Unevaluable expression ( Spark SQL ).","title":"Unevaluable"},{"location":"sql/PythonUDF/#nonsqlexpression","text":"PythonUDF is a NonSQLExpression expression ( Spark SQL ).","title":"NonSQLExpression"},{"location":"sql/PythonUDF/#userdefinedexpression","text":"PythonUDF is a UserDefinedExpression expression ( Spark SQL ).","title":"UserDefinedExpression"},{"location":"sql/RelationalGroupedDataset/","text":"RelationalGroupedDataset \u00b6 RelationalGroupedDataset is a result of executing high-level grouping operators. This is a stub This page is a stub to describe PySpark-related methods only. Learn more about RelationalGroupedDataset in The Internals of Spark SQL . flatMapCoGroupsInPandas \u00b6 flatMapCoGroupsInPandas ( r : RelationalGroupedDataset , expr : PythonUDF ) : DataFrame flatMapCoGroupsInPandas ...FIXME flatMapCoGroupsInPandas is used when: PandasCogroupedOps is requested to applyInPandas flatMapGroupsInPandas \u00b6 flatMapGroupsInPandas ( expr : PythonUDF ) : DataFrame flatMapGroupsInPandas creates a DataFrame with a FlatMapGroupsInPandas logical operator. flatMapGroupsInPandas asserts that the input PythonUDF is a grouped map udf (the evalType is PythonEvalType.SQL_GROUPED_MAP_PANDAS_UDF ). flatMapGroupsInPandas asserts that the return type of the input PythonUDF is StructType . flatMapGroupsInPandas is used when: PandasGroupedOpsMixin is requested to applyInPandas","title":"RelationalGroupedDataset"},{"location":"sql/RelationalGroupedDataset/#relationalgroupeddataset","text":"RelationalGroupedDataset is a result of executing high-level grouping operators. This is a stub This page is a stub to describe PySpark-related methods only. Learn more about RelationalGroupedDataset in The Internals of Spark SQL .","title":"RelationalGroupedDataset"},{"location":"sql/RelationalGroupedDataset/#flatmapcogroupsinpandas","text":"flatMapCoGroupsInPandas ( r : RelationalGroupedDataset , expr : PythonUDF ) : DataFrame flatMapCoGroupsInPandas ...FIXME flatMapCoGroupsInPandas is used when: PandasCogroupedOps is requested to applyInPandas","title":" flatMapCoGroupsInPandas"},{"location":"sql/RelationalGroupedDataset/#flatmapgroupsinpandas","text":"flatMapGroupsInPandas ( expr : PythonUDF ) : DataFrame flatMapGroupsInPandas creates a DataFrame with a FlatMapGroupsInPandas logical operator. flatMapGroupsInPandas asserts that the input PythonUDF is a grouped map udf (the evalType is PythonEvalType.SQL_GROUPED_MAP_PANDAS_UDF ). flatMapGroupsInPandas asserts that the return type of the input PythonUDF is StructType . flatMapGroupsInPandas is used when: PandasGroupedOpsMixin is requested to applyInPandas","title":" flatMapGroupsInPandas"},{"location":"sql/SQLContext/","text":"SQLContext \u00b6 SQLContext is...FIXME","title":"SQLContext"},{"location":"sql/SQLContext/#sqlcontext","text":"SQLContext is...FIXME","title":"SQLContext"},{"location":"sql/SparkConversionMixin/","text":"SparkConversionMixin \u00b6 SparkConversionMixin is a Python mixin for SparkSession class.","title":"SparkConversionMixin"},{"location":"sql/SparkConversionMixin/#sparkconversionmixin","text":"SparkConversionMixin is a Python mixin for SparkSession class.","title":"SparkConversionMixin"},{"location":"sql/SparkSession/","text":"SparkSession \u00b6 _create_shell_session \u00b6 _create_shell_session () _create_shell_session is a @staticmethod . _create_shell_session ...FIXME _create_shell_session is used when: FIXME sql \u00b6 sql ( self , sqlQuery ) sql creates a DataFrame with the sqlQuery query executed.","title":"SparkSession"},{"location":"sql/SparkSession/#sparksession","text":"","title":"SparkSession"},{"location":"sql/SparkSession/#_create_shell_session","text":"_create_shell_session () _create_shell_session is a @staticmethod . _create_shell_session ...FIXME _create_shell_session is used when: FIXME","title":" _create_shell_session"},{"location":"sql/SparkSession/#sql","text":"sql ( self , sqlQuery ) sql creates a DataFrame with the sqlQuery query executed.","title":" sql"},{"location":"sql/UDFRegistration/","text":"UDFRegistration \u00b6 UDFRegistration is...FIXME","title":"UDFRegistration"},{"location":"sql/UDFRegistration/#udfregistration","text":"UDFRegistration is...FIXME","title":"UDFRegistration"},{"location":"sql/UserDefinedFunction/","text":"UserDefinedFunction \u00b6 UserDefinedFunction is...FIXME","title":"UserDefinedFunction"},{"location":"sql/UserDefinedFunction/#userdefinedfunction","text":"UserDefinedFunction is...FIXME","title":"UserDefinedFunction"},{"location":"sql/UserDefinedPythonFunction/","text":"UserDefinedPythonFunction \u00b6 Creating Instance \u00b6 UserDefinedPythonFunction takes the following to be created: Name PythonFunction DataType ( Spark SQL ) Python Eval Type udfDeterministic flag Creating PythonUDF \u00b6 builder ( e : Seq [ Expression ]) : Expression builder creates a PythonUDF for the arguments and the given children expressions. builder is used when: UDFRegistration is requested to register a Python UDF ( Spark SQL ) UserDefinedPythonFunction is requested to apply Applying PythonUDF \u00b6 apply ( exprs : Column* ) : Column apply creates a PythonUDF with the input Column ( Spark SQL ) expressions and creates a new Column . apply is used when: UDFRegistration is requested to register a Python UDF ( Spark SQL ) UserDefinedPythonFunction is requested to apply","title":"UserDefinedPythonFunction"},{"location":"sql/UserDefinedPythonFunction/#userdefinedpythonfunction","text":"","title":"UserDefinedPythonFunction"},{"location":"sql/UserDefinedPythonFunction/#creating-instance","text":"UserDefinedPythonFunction takes the following to be created: Name PythonFunction DataType ( Spark SQL ) Python Eval Type udfDeterministic flag","title":"Creating Instance"},{"location":"sql/UserDefinedPythonFunction/#creating-pythonudf","text":"builder ( e : Seq [ Expression ]) : Expression builder creates a PythonUDF for the arguments and the given children expressions. builder is used when: UDFRegistration is requested to register a Python UDF ( Spark SQL ) UserDefinedPythonFunction is requested to apply","title":" Creating PythonUDF"},{"location":"sql/UserDefinedPythonFunction/#applying-pythonudf","text":"apply ( exprs : Column* ) : Column apply creates a PythonUDF with the input Column ( Spark SQL ) expressions and creates a new Column . apply is used when: UDFRegistration is requested to register a Python UDF ( Spark SQL ) UserDefinedPythonFunction is requested to apply","title":" Applying PythonUDF"}]}