{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"The Internals of PySpark (Apache Spark 3.1.1-rc2) \u00b6 Welcome to The Internals of PySpark online book! \ud83e\udd19 I'm Jacek Laskowski , an IT freelancer specializing in Apache Spark , Delta Lake and Apache Kafka (with brief forays into a wider data engineering space, e.g. Trino and ksqlDB , mostly during Warsaw Data Engineering meetups). I'm very excited to have you here and hope you will enjoy exploring the internals of Spark SQL as much as I have. Flannery O'Connor I write to discover what I know. \"The Internals Of\" series I'm also writing other online books in the \"The Internals Of\" series. Please visit \"The Internals Of\" Online Books home page. Expect text and code snippets from a variety of public sources. Attribution follows. Now, let me introduce you to PySpark \ud83d\udd25","title":"Welcome"},{"location":"#the-internals-of-pyspark-apache-spark-311-rc2","text":"Welcome to The Internals of PySpark online book! \ud83e\udd19 I'm Jacek Laskowski , an IT freelancer specializing in Apache Spark , Delta Lake and Apache Kafka (with brief forays into a wider data engineering space, e.g. Trino and ksqlDB , mostly during Warsaw Data Engineering meetups). I'm very excited to have you here and hope you will enjoy exploring the internals of Spark SQL as much as I have. Flannery O'Connor I write to discover what I know. \"The Internals Of\" series I'm also writing other online books in the \"The Internals Of\" series. Please visit \"The Internals Of\" Online Books home page. Expect text and code snippets from a variety of public sources. Attribution follows. Now, let me introduce you to PySpark \ud83d\udd25","title":"The Internals of PySpark (Apache Spark 3.1.1-rc2)"},{"location":"AggregateInPandasExec/","text":"AggregateInPandasExec Physical Operator \u00b6 AggregateInPandasExec is a unary physical operator ( Spark SQL ). Creating Instance \u00b6 AggregateInPandasExec takes the following to be created: Grouping Expressions ( Spark SQL ) ( Seq[NamedExpression] ) PythonUDF s Result Named Expressions ( Spark SQL ) ( Seq[NamedExpression] ) Child Physical Operator ( Spark SQL ) AggregateInPandasExec is created when Aggregation execution planning strategy ( Spark SQL ) is executed for Aggregate logical operators ( Spark SQL ) with PythonUDF aggregate expressions only. Executing Operator \u00b6 doExecute () : RDD [ InternalRow ] doExecute uses ArrowPythonRunner (one per partition) to execute PythonUDFs . doExecute is part of the SparkPlan ( Spark SQL ) abstraction.","title":"AggregateInPandasExec"},{"location":"AggregateInPandasExec/#aggregateinpandasexec-physical-operator","text":"AggregateInPandasExec is a unary physical operator ( Spark SQL ).","title":"AggregateInPandasExec Physical Operator"},{"location":"AggregateInPandasExec/#creating-instance","text":"AggregateInPandasExec takes the following to be created: Grouping Expressions ( Spark SQL ) ( Seq[NamedExpression] ) PythonUDF s Result Named Expressions ( Spark SQL ) ( Seq[NamedExpression] ) Child Physical Operator ( Spark SQL ) AggregateInPandasExec is created when Aggregation execution planning strategy ( Spark SQL ) is executed for Aggregate logical operators ( Spark SQL ) with PythonUDF aggregate expressions only.","title":"Creating Instance"},{"location":"AggregateInPandasExec/#executing-operator","text":"doExecute () : RDD [ InternalRow ] doExecute uses ArrowPythonRunner (one per partition) to execute PythonUDFs . doExecute is part of the SparkPlan ( Spark SQL ) abstraction.","title":" Executing Operator"},{"location":"ArrowPythonRunner/","text":"ArrowPythonRunner \u00b6 ArrowPythonRunner is a pyspark runner with PythonArrowOutput for...FIXME Creating Instance \u00b6 ArrowPythonRunner takes the following to be created: Seq[ChainedPythonFunctions] Eval Type Argument Offsets ( Array[Array[Int]] ) Schema ( Spark SQL ) TimeZone ID Configuration ( Map[String, String] ) ArrowPythonRunner is created when AggregateInPandasExec , ArrowEvalPythonExec , FlatMapGroupsInPandasExec , MapInPandasExec , WindowInPandasExec physical operators are executed.","title":"ArrowPythonRunner"},{"location":"ArrowPythonRunner/#arrowpythonrunner","text":"ArrowPythonRunner is a pyspark runner with PythonArrowOutput for...FIXME","title":"ArrowPythonRunner"},{"location":"ArrowPythonRunner/#creating-instance","text":"ArrowPythonRunner takes the following to be created: Seq[ChainedPythonFunctions] Eval Type Argument Offsets ( Array[Array[Int]] ) Schema ( Spark SQL ) TimeZone ID Configuration ( Map[String, String] ) ArrowPythonRunner is created when AggregateInPandasExec , ArrowEvalPythonExec , FlatMapGroupsInPandasExec , MapInPandasExec , WindowInPandasExec physical operators are executed.","title":"Creating Instance"},{"location":"BasePythonRunner/","text":"BasePythonRunner \u00b6 BasePythonRunner is an abstraction of Python Runners . Contract \u00b6 newReaderIterator \u00b6 newReaderIterator ( stream : DataInputStream , writerThread : WriterThread , startTime : Long , env : SparkEnv , worker : Socket , releasedOrClosed : AtomicBoolean , context : TaskContext ) : Iterator [ OUT ] Used when BasePythonRunner is requested to compute newWriterThread \u00b6 newWriterThread ( env : SparkEnv , worker : Socket , inputIterator : Iterator [ IN ], partitionIndex : Int , context : TaskContext ) : WriterThread Used when BasePythonRunner is requested to compute Implementations \u00b6 ArrowPythonRunner CoGroupedArrowPythonRunner PythonRunner PythonUDFRunner Scala Definition \u00b6 BasePythonRunner is a type constructor in Scala ( generic class in Java) with the following definition: abstract class BasePythonRunner [ IN , OUT ](...) { // ... } BasePythonRunner uses IN and OUT as the name of the types for the input and output values. Creating Instance \u00b6 BasePythonRunner takes the following to be created: ChainedPythonFunctions Eval Type Argument Offsets BasePythonRunner requires that the number of ChainedPythonFunctions and Argument Offsets are the same. Abstract Class BasePythonRunner is an abstract class and cannot be created directly. It is created indirectly for the concrete BasePythonRunners . Computing Result \u00b6 compute ( inputIterator : Iterator [ IN ], partitionIndex : Int , context : TaskContext ) : Iterator [ OUT ] compute makes sure that spark.executorEnv.OMP_NUM_THREADS configuration option is set or defaults to spark.executor.cores property. compute defines the following environment variables: SPARK_LOCAL_DIRS to be the local directories of the local DiskBlockManager SPARK_BUFFER_SIZE to be the value of spark.buffer.size configuration property (default: 65536 ) compute can optionally define environment variables: SPARK_REUSE_WORKER to be 1 based on spark.python.worker.reuse configuration property (default: true ) PYSPARK_EXECUTOR_MEMORY_MB to be the value of spark.executor.pyspark.memory configuration property if defined compute requests the executor's SparkEnv to createPythonWorker (for a pythonExec and the environment variables) that requests PythonWorkerFactory to create a Python worker (and give a java.net.Socket ). FIXME Describe pythonExec . compute [newWriterThread] with the Python worker and the input arguments. compute creates and starts a MonitorThread to watch the Python worker. compute creates a new reader iterator to read lines from the Python worker's stdout. compute is used when: PythonRDD is requested to compute AggregateInPandasExec , ArrowEvalPythonExec , BatchEvalPythonExec , FlatMapCoGroupsInPandasExec , FlatMapGroupsInPandasExec MapInPandasExec , WindowInPandasExec physical operators are executed","title":"BasePythonRunner"},{"location":"BasePythonRunner/#basepythonrunner","text":"BasePythonRunner is an abstraction of Python Runners .","title":"BasePythonRunner"},{"location":"BasePythonRunner/#contract","text":"","title":"Contract"},{"location":"BasePythonRunner/#newreaderiterator","text":"newReaderIterator ( stream : DataInputStream , writerThread : WriterThread , startTime : Long , env : SparkEnv , worker : Socket , releasedOrClosed : AtomicBoolean , context : TaskContext ) : Iterator [ OUT ] Used when BasePythonRunner is requested to compute","title":" newReaderIterator"},{"location":"BasePythonRunner/#newwriterthread","text":"newWriterThread ( env : SparkEnv , worker : Socket , inputIterator : Iterator [ IN ], partitionIndex : Int , context : TaskContext ) : WriterThread Used when BasePythonRunner is requested to compute","title":" newWriterThread"},{"location":"BasePythonRunner/#implementations","text":"ArrowPythonRunner CoGroupedArrowPythonRunner PythonRunner PythonUDFRunner","title":"Implementations"},{"location":"BasePythonRunner/#scala-definition","text":"BasePythonRunner is a type constructor in Scala ( generic class in Java) with the following definition: abstract class BasePythonRunner [ IN , OUT ](...) { // ... } BasePythonRunner uses IN and OUT as the name of the types for the input and output values.","title":"Scala Definition"},{"location":"BasePythonRunner/#creating-instance","text":"BasePythonRunner takes the following to be created: ChainedPythonFunctions Eval Type Argument Offsets BasePythonRunner requires that the number of ChainedPythonFunctions and Argument Offsets are the same. Abstract Class BasePythonRunner is an abstract class and cannot be created directly. It is created indirectly for the concrete BasePythonRunners .","title":"Creating Instance"},{"location":"BasePythonRunner/#computing-result","text":"compute ( inputIterator : Iterator [ IN ], partitionIndex : Int , context : TaskContext ) : Iterator [ OUT ] compute makes sure that spark.executorEnv.OMP_NUM_THREADS configuration option is set or defaults to spark.executor.cores property. compute defines the following environment variables: SPARK_LOCAL_DIRS to be the local directories of the local DiskBlockManager SPARK_BUFFER_SIZE to be the value of spark.buffer.size configuration property (default: 65536 ) compute can optionally define environment variables: SPARK_REUSE_WORKER to be 1 based on spark.python.worker.reuse configuration property (default: true ) PYSPARK_EXECUTOR_MEMORY_MB to be the value of spark.executor.pyspark.memory configuration property if defined compute requests the executor's SparkEnv to createPythonWorker (for a pythonExec and the environment variables) that requests PythonWorkerFactory to create a Python worker (and give a java.net.Socket ). FIXME Describe pythonExec . compute [newWriterThread] with the Python worker and the input arguments. compute creates and starts a MonitorThread to watch the Python worker. compute creates a new reader iterator to read lines from the Python worker's stdout. compute is used when: PythonRDD is requested to compute AggregateInPandasExec , ArrowEvalPythonExec , BatchEvalPythonExec , FlatMapCoGroupsInPandasExec , FlatMapGroupsInPandasExec MapInPandasExec , WindowInPandasExec physical operators are executed","title":" Computing Result"},{"location":"MonitorThread/","text":"MonitorThread \u00b6 MonitorThread is...FIXME","title":"MonitorThread"},{"location":"MonitorThread/#monitorthread","text":"MonitorThread is...FIXME","title":"MonitorThread"},{"location":"PythonAccumulatorV2/","text":"PythonAccumulatorV2 \u00b6 PythonAccumulatorV2 is...FIXME","title":"PythonAccumulatorV2"},{"location":"PythonAccumulatorV2/#pythonaccumulatorv2","text":"PythonAccumulatorV2 is...FIXME","title":"PythonAccumulatorV2"},{"location":"PythonBroadcast/","text":"PythonBroadcast \u00b6 PythonBroadcast is...FIXME","title":"PythonBroadcast"},{"location":"PythonBroadcast/#pythonbroadcast","text":"PythonBroadcast is...FIXME","title":"PythonBroadcast"},{"location":"PythonForeachWriter/","text":"PythonForeachWriter \u00b6 PythonForeachWriter is...FIXME","title":"PythonForeachWriter"},{"location":"PythonForeachWriter/#pythonforeachwriter","text":"PythonForeachWriter is...FIXME","title":"PythonForeachWriter"},{"location":"PythonFunction/","text":"PythonFunction \u00b6 PythonFunction is a metadata of a Python function to be executed in PythonRunner . Creating Instance \u00b6 PythonFunction takes the following to be created: Command ( Array[Byte] ) Environment Variables ( Map[String, String] ) Python Includes ( List[String] ) Python Executable Python Version Broadcast Variables with PythonBroadcast ]s PythonAccumulatorV2 PythonFunction is created when...FIXME","title":"PythonFunction"},{"location":"PythonFunction/#pythonfunction","text":"PythonFunction is a metadata of a Python function to be executed in PythonRunner .","title":"PythonFunction"},{"location":"PythonFunction/#creating-instance","text":"PythonFunction takes the following to be created: Command ( Array[Byte] ) Environment Variables ( Map[String, String] ) Python Includes ( List[String] ) Python Executable Python Version Broadcast Variables with PythonBroadcast ]s PythonAccumulatorV2 PythonFunction is created when...FIXME","title":"Creating Instance"},{"location":"PythonRDD/","text":"PythonRDD \u00b6 PythonRDD is an RDD ( RDD[Array[Byte]] ) that uses PythonRunner (to compute a partition ). Creating Instance \u00b6 PythonRDD takes the following to be created: Parent RDD PythonFunction preservePartitoning flag isFromBarrier flag (default: false ) PythonRDD is created when...FIXME runJob \u00b6 runJob ( sc : SparkContext , rdd : JavaRDD [ Array [ Byte ]], partitions : JArrayList [ Int ]) : Array [ Any ] runJob ...FIXME collectAndServe \u00b6 collectAndServe [ T ]( rdd : RDD [ T ]) : Array [ Any ] collectAndServe ...FIXME collectAndServeWithJobGroup \u00b6 collectAndServeWithJobGroup [ T ]( rdd : RDD [ T ], groupId : String , description : String , interruptOnCancel : Boolean ) : Array [ Any ] collectAndServeWithJobGroup ...FIXME serveIterator Utility \u00b6 serveIterator ( items : Iterator [ _ ], threadName : String ) : Array [ Any ] serveIterator serveToStream with a writer function that...FIXME serveIterator is used when: PythonRDD utility is used to runJob , collectAndServe and collectAndServeWithJobGroup Dataset is requested to collectToPython , tailToPython , getRowsToPython serveToStream Utility \u00b6 serveToStream ( threadName : String )( writeFunc : OutputStream => Unit ) : Array [ Any ] serveToStream serveToStream with the authHelper and the input arguments. serveToStream is used when: PythonRDD utility is used to serveIterator Dataset is requested to collectAsArrowToPython SocketAuthHelper \u00b6 PythonRDD uses a SocketAuthHelper .","title":"PythonRDD"},{"location":"PythonRDD/#pythonrdd","text":"PythonRDD is an RDD ( RDD[Array[Byte]] ) that uses PythonRunner (to compute a partition ).","title":"PythonRDD"},{"location":"PythonRDD/#creating-instance","text":"PythonRDD takes the following to be created: Parent RDD PythonFunction preservePartitoning flag isFromBarrier flag (default: false ) PythonRDD is created when...FIXME","title":"Creating Instance"},{"location":"PythonRDD/#runjob","text":"runJob ( sc : SparkContext , rdd : JavaRDD [ Array [ Byte ]], partitions : JArrayList [ Int ]) : Array [ Any ] runJob ...FIXME","title":" runJob"},{"location":"PythonRDD/#collectandserve","text":"collectAndServe [ T ]( rdd : RDD [ T ]) : Array [ Any ] collectAndServe ...FIXME","title":" collectAndServe"},{"location":"PythonRDD/#collectandservewithjobgroup","text":"collectAndServeWithJobGroup [ T ]( rdd : RDD [ T ], groupId : String , description : String , interruptOnCancel : Boolean ) : Array [ Any ] collectAndServeWithJobGroup ...FIXME","title":" collectAndServeWithJobGroup"},{"location":"PythonRDD/#serveiterator-utility","text":"serveIterator ( items : Iterator [ _ ], threadName : String ) : Array [ Any ] serveIterator serveToStream with a writer function that...FIXME serveIterator is used when: PythonRDD utility is used to runJob , collectAndServe and collectAndServeWithJobGroup Dataset is requested to collectToPython , tailToPython , getRowsToPython","title":" serveIterator Utility"},{"location":"PythonRDD/#servetostream-utility","text":"serveToStream ( threadName : String )( writeFunc : OutputStream => Unit ) : Array [ Any ] serveToStream serveToStream with the authHelper and the input arguments. serveToStream is used when: PythonRDD utility is used to serveIterator Dataset is requested to collectAsArrowToPython","title":" serveToStream Utility"},{"location":"PythonRDD/#socketauthhelper","text":"PythonRDD uses a SocketAuthHelper .","title":" SocketAuthHelper"},{"location":"PythonRunner/","text":"PythonRunner \u00b6 PythonRunner is a concrete BasePythonRunner . Creating Instance \u00b6 PythonRunner takes the following to be created: Seq[ChainedPythonFunctions] PythonRunner is created (indirectly using apply factory method) when: PythonRDD is requested to compute a partition PythonForeachWriter is requested for a PythonRunner Creating PythonRunner \u00b6 apply ( func : PythonFunction ) : PythonRunner apply simply creates a PythonRunner for the PythonFunction . apply is used when: PythonRDD is requested to compute a partition PythonForeachWriter is requested for a PythonRunner","title":"PythonRunner"},{"location":"PythonRunner/#pythonrunner","text":"PythonRunner is a concrete BasePythonRunner .","title":"PythonRunner"},{"location":"PythonRunner/#creating-instance","text":"PythonRunner takes the following to be created: Seq[ChainedPythonFunctions] PythonRunner is created (indirectly using apply factory method) when: PythonRDD is requested to compute a partition PythonForeachWriter is requested for a PythonRunner","title":"Creating Instance"},{"location":"PythonRunner/#creating-pythonrunner","text":"apply ( func : PythonFunction ) : PythonRunner apply simply creates a PythonRunner for the PythonFunction . apply is used when: PythonRDD is requested to compute a partition PythonForeachWriter is requested for a PythonRunner","title":" Creating PythonRunner"},{"location":"PythonUDF/","text":"PythonUDF \u00b6 PythonUDF is a Catalyst expression ( Spark SQL ). Creating Instance \u00b6 PythonUDF takes the following to be created: Name PythonFunction DataType ( Spark SQL ) Children Catalyst Expressions ( Spark SQL ) Python Eval Type udfDeterministic flag Result ID ( ExprId ) PythonUDF is created when: UserDefinedPythonFunction is requested to builder Unevaluable \u00b6 PythonUDF is an Unevaluable expression ( Spark SQL ). NonSQLExpression \u00b6 PythonUDF is a NonSQLExpression expression ( Spark SQL ). UserDefinedExpression \u00b6 PythonUDF is a UserDefinedExpression expression ( Spark SQL ).","title":"PythonUDF"},{"location":"PythonUDF/#pythonudf","text":"PythonUDF is a Catalyst expression ( Spark SQL ).","title":"PythonUDF"},{"location":"PythonUDF/#creating-instance","text":"PythonUDF takes the following to be created: Name PythonFunction DataType ( Spark SQL ) Children Catalyst Expressions ( Spark SQL ) Python Eval Type udfDeterministic flag Result ID ( ExprId ) PythonUDF is created when: UserDefinedPythonFunction is requested to builder","title":"Creating Instance"},{"location":"PythonUDF/#unevaluable","text":"PythonUDF is an Unevaluable expression ( Spark SQL ).","title":"Unevaluable"},{"location":"PythonUDF/#nonsqlexpression","text":"PythonUDF is a NonSQLExpression expression ( Spark SQL ).","title":"NonSQLExpression"},{"location":"PythonUDF/#userdefinedexpression","text":"PythonUDF is a UserDefinedExpression expression ( Spark SQL ).","title":"UserDefinedExpression"},{"location":"PythonUDFRunner/","text":"PythonUDFRunner \u00b6 PythonUDFRunner is...FIXME","title":"PythonUDFRunner"},{"location":"PythonUDFRunner/#pythonudfrunner","text":"PythonUDFRunner is...FIXME","title":"PythonUDFRunner"},{"location":"PythonWorkerFactory/","text":"PythonWorkerFactory \u00b6 Creating Instance \u00b6 PythonWorkerFactory takes the following to be created: Python Executable Environment Variables ( Map[String, String] ) PythonWorkerFactory is created when SparkEnv is requested to createPythonWorker (when BasePythonRunner is requested to compute a partition ). useDaemon Flag \u00b6 PythonWorkerFactory uses useDaemon internal flag that is the value of spark.python.use.daemon configuration property to decide whether to use lighter daemon or non-daemon workers. useDaemon flag is used when PythonWorkerFactory requested to create , stop or release a worker and stop a daemon module . Python Daemon Module \u00b6 PythonWorkerFactory uses spark.python.daemon.module configuration property to define the Python Daemon Module . The Python Daemon Module is used when PythonWorkerFactory is requested to create and start a daemon module . Python Worker Module \u00b6 PythonWorkerFactory uses spark.python.worker.module configuration property to specify the Python Worker Module . The Python Worker Module is used when PythonWorkerFactory is requested to create and start a worker . Creating Python Worker \u00b6 create () : Socket create ...FIXME create is used when SparkEnv is requested to createPythonWorker . Creating Daemon Worker \u00b6 createThroughDaemon () : Socket createThroughDaemon ...FIXME createThroughDaemon is used when PythonWorkerFactory is requested to create a Python worker (with useDaemon flag enabled). Starting Python Daemon Process \u00b6 startDaemon () : Unit startDaemon ...FIXME Creating Simple Non-Daemon Worker \u00b6 createSimpleWorker () : Socket createSimpleWorker ...FIXME createSimpleWorker is used when PythonWorkerFactory is requested to create a Python worker (with useDaemon flag disabled). Logging \u00b6 Enable ALL logging level for org.apache.spark.api.python.PythonWorkerFactory logger to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.api.python.PythonWorkerFactory=ALL Refer to Logging .","title":"PythonWorkerFactory"},{"location":"PythonWorkerFactory/#pythonworkerfactory","text":"","title":"PythonWorkerFactory"},{"location":"PythonWorkerFactory/#creating-instance","text":"PythonWorkerFactory takes the following to be created: Python Executable Environment Variables ( Map[String, String] ) PythonWorkerFactory is created when SparkEnv is requested to createPythonWorker (when BasePythonRunner is requested to compute a partition ).","title":"Creating Instance"},{"location":"PythonWorkerFactory/#usedaemon-flag","text":"PythonWorkerFactory uses useDaemon internal flag that is the value of spark.python.use.daemon configuration property to decide whether to use lighter daemon or non-daemon workers. useDaemon flag is used when PythonWorkerFactory requested to create , stop or release a worker and stop a daemon module .","title":" useDaemon Flag"},{"location":"PythonWorkerFactory/#python-daemon-module","text":"PythonWorkerFactory uses spark.python.daemon.module configuration property to define the Python Daemon Module . The Python Daemon Module is used when PythonWorkerFactory is requested to create and start a daemon module .","title":" Python Daemon Module"},{"location":"PythonWorkerFactory/#python-worker-module","text":"PythonWorkerFactory uses spark.python.worker.module configuration property to specify the Python Worker Module . The Python Worker Module is used when PythonWorkerFactory is requested to create and start a worker .","title":" Python Worker Module"},{"location":"PythonWorkerFactory/#creating-python-worker","text":"create () : Socket create ...FIXME create is used when SparkEnv is requested to createPythonWorker .","title":" Creating Python Worker"},{"location":"PythonWorkerFactory/#creating-daemon-worker","text":"createThroughDaemon () : Socket createThroughDaemon ...FIXME createThroughDaemon is used when PythonWorkerFactory is requested to create a Python worker (with useDaemon flag enabled).","title":" Creating Daemon Worker"},{"location":"PythonWorkerFactory/#starting-python-daemon-process","text":"startDaemon () : Unit startDaemon ...FIXME","title":" Starting Python Daemon Process"},{"location":"PythonWorkerFactory/#creating-simple-non-daemon-worker","text":"createSimpleWorker () : Socket createSimpleWorker ...FIXME createSimpleWorker is used when PythonWorkerFactory is requested to create a Python worker (with useDaemon flag disabled).","title":" Creating Simple Non-Daemon Worker"},{"location":"PythonWorkerFactory/#logging","text":"Enable ALL logging level for org.apache.spark.api.python.PythonWorkerFactory logger to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.api.python.PythonWorkerFactory=ALL Refer to Logging .","title":"Logging"},{"location":"Setup/","text":"PySpark Setup \u00b6 Install IPython \u00b6 Follow the steps as described in the official documentation of IPython. pip install ipython Start PySpark \u00b6 export PYSPARK_DRIVER_PYTHON = ipython ./bin/pyspark Python 3.9.1 (default, Feb 3 2021, 07:38:02) Type 'copyright', 'credits' or 'license' for more information IPython 7.20.0 -- An enhanced Interactive Python. Type '?' for help. Setting default log level to \"WARN\". To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel). Welcome to ____ __ / __/__ ___ _____/ /__ _\\ \\/ _ \\/ _ `/ __/ '_/ /__ / .__/\\_,_/_/ /_/\\_\\ version 3.1.1 /_/ Using Python version 3.9.1 (default, Feb 3 2021 07:38:02) Spark context Web UI available at http://192.168.68.101:4040 Spark context available as 'sc' (master = local[*], app id = local-1613571272142). SparkSession available as 'spark'. In [1]: In [1]: spark.version Out[1]: '3.1.1'","title":"Setup"},{"location":"Setup/#pyspark-setup","text":"","title":"PySpark Setup"},{"location":"Setup/#install-ipython","text":"Follow the steps as described in the official documentation of IPython. pip install ipython","title":"Install IPython"},{"location":"Setup/#start-pyspark","text":"export PYSPARK_DRIVER_PYTHON = ipython ./bin/pyspark Python 3.9.1 (default, Feb 3 2021, 07:38:02) Type 'copyright', 'credits' or 'license' for more information IPython 7.20.0 -- An enhanced Interactive Python. Type '?' for help. Setting default log level to \"WARN\". To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel). Welcome to ____ __ / __/__ ___ _____/ /__ _\\ \\/ _ \\/ _ `/ __/ '_/ /__ / .__/\\_,_/_/ /_/\\_\\ version 3.1.1 /_/ Using Python version 3.9.1 (default, Feb 3 2021 07:38:02) Spark context Web UI available at http://192.168.68.101:4040 Spark context available as 'sc' (master = local[*], app id = local-1613571272142). SparkSession available as 'spark'. In [1]: In [1]: spark.version Out[1]: '3.1.1'","title":"Start PySpark"},{"location":"SocketAuthHelper/","text":"SocketAuthHelper \u00b6 SocketAuthHelper is...FIXME","title":"SocketAuthHelper"},{"location":"SocketAuthHelper/#socketauthhelper","text":"SocketAuthHelper is...FIXME","title":"SocketAuthHelper"},{"location":"SocketAuthServer/","text":"SocketAuthServer \u00b6 serveToStream Utility \u00b6 serveToStream ( threadName : String , authHelper : SocketAuthHelper )( writeFunc : OutputStream => Unit ) : Array [ Any ] serveToStream ...FIXME serveToStream is used when: FIXME","title":"SocketAuthServer"},{"location":"SocketAuthServer/#socketauthserver","text":"","title":"SocketAuthServer"},{"location":"SocketAuthServer/#servetostream-utility","text":"serveToStream ( threadName : String , authHelper : SocketAuthHelper )( writeFunc : OutputStream => Unit ) : Array [ Any ] serveToStream ...FIXME serveToStream is used when: FIXME","title":" serveToStream Utility"},{"location":"SocketFuncServer/","text":"SocketFuncServer \u00b6 SocketFuncServer is...FIXME","title":"SocketFuncServer"},{"location":"SocketFuncServer/#socketfuncserver","text":"SocketFuncServer is...FIXME","title":"SocketFuncServer"},{"location":"UserDefinedPythonFunction/","text":"UserDefinedPythonFunction \u00b6 Creating Instance \u00b6 UserDefinedPythonFunction takes the following to be created: Name PythonFunction DataType ( Spark SQL ) Python Eval Type udfDeterministic flag Creating PythonUDF \u00b6 builder ( e : Seq [ Expression ]) : Expression builder creates a PythonUDF for the arguments and the given children expressions. builder is used when: UDFRegistration is requested to register a Python UDF ( Spark SQL ) UserDefinedPythonFunction is requested to apply Applying PythonUDF \u00b6 apply ( exprs : Column* ) : Column apply creates a PythonUDF with the input Column ( Spark SQL ) expressions and creates a new Column . apply is used when: UDFRegistration is requested to register a Python UDF ( Spark SQL ) UserDefinedPythonFunction is requested to apply","title":"UserDefinedPythonFunction"},{"location":"UserDefinedPythonFunction/#userdefinedpythonfunction","text":"","title":"UserDefinedPythonFunction"},{"location":"UserDefinedPythonFunction/#creating-instance","text":"UserDefinedPythonFunction takes the following to be created: Name PythonFunction DataType ( Spark SQL ) Python Eval Type udfDeterministic flag","title":"Creating Instance"},{"location":"UserDefinedPythonFunction/#creating-pythonudf","text":"builder ( e : Seq [ Expression ]) : Expression builder creates a PythonUDF for the arguments and the given children expressions. builder is used when: UDFRegistration is requested to register a Python UDF ( Spark SQL ) UserDefinedPythonFunction is requested to apply","title":" Creating PythonUDF"},{"location":"UserDefinedPythonFunction/#applying-pythonudf","text":"apply ( exprs : Column* ) : Column apply creates a PythonUDF with the input Column ( Spark SQL ) expressions and creates a new Column . apply is used when: UDFRegistration is requested to register a Python UDF ( Spark SQL ) UserDefinedPythonFunction is requested to apply","title":" Applying PythonUDF"},{"location":"configuration-properties/","text":"Configuration Properties \u00b6 spark.python.use.daemon \u00b6 Because forking processes from Java is expensive, we prefer to launch a single Python daemon, pyspark/daemon.py (by default) and tell it to fork new workers for our tasks. This daemon currently only works on UNIX-based systems now because it uses signals for child management, so we can also fall back to launching workers, pyspark/worker.py (by default) directly. Default: true (always disabled on Windows) Used when PythonWorkerFactory is created spark.python.daemon.module \u00b6 Default: pyspark.daemon Used when PythonWorkerFactory is created spark.python.worker.module \u00b6 Default: (undefined) Used when PythonWorkerFactory is created","title":"Configuration Properties"},{"location":"configuration-properties/#configuration-properties","text":"","title":"Configuration Properties"},{"location":"configuration-properties/#sparkpythonusedaemon","text":"Because forking processes from Java is expensive, we prefer to launch a single Python daemon, pyspark/daemon.py (by default) and tell it to fork new workers for our tasks. This daemon currently only works on UNIX-based systems now because it uses signals for child management, so we can also fall back to launching workers, pyspark/worker.py (by default) directly. Default: true (always disabled on Windows) Used when PythonWorkerFactory is created","title":" spark.python.use.daemon"},{"location":"configuration-properties/#sparkpythondaemonmodule","text":"Default: pyspark.daemon Used when PythonWorkerFactory is created","title":" spark.python.daemon.module"},{"location":"configuration-properties/#sparkpythonworkermodule","text":"Default: (undefined) Used when PythonWorkerFactory is created","title":" spark.python.worker.module"},{"location":"overview/","text":"PySpark \u00b6 PySpark is the Python API for Apache Spark. pyspark.sql Package \u00b6 pyspark.sql is a Python package for Spark SQL. from pyspark.sql import * Tip Learn more about Modules and Packages in Python in The Python Tutorial . __init__.py \u00b6 The __init__.py files are required to make Python treat directories containing the file as packages. Per 6.4.1. Importing * From a Package : The import statement uses the following convention: if a package's __init__.py code defines a list named __all__ , it is taken to be the list of module names that should be imported when from package import * is encountered. Per Public and Internal Interfaces in PEP 8 -- Style Guide for Python Code : To better support introspection, modules should explicitly declare the names in their public API using the __all__ attribute. From python/pyspark/sql/__init__.py : __all__ = [ 'SparkSession' , 'SQLContext' , 'HiveContext' , 'UDFRegistration' , 'DataFrame' , 'GroupedData' , 'Column' , 'Catalog' , 'Row' , 'DataFrameNaFunctions' , 'DataFrameStatFunctions' , 'Window' , 'WindowSpec' , 'DataFrameReader' , 'DataFrameWriter' , 'PandasCogroupedOps' ]","title":"Overview"},{"location":"overview/#pyspark","text":"PySpark is the Python API for Apache Spark.","title":"PySpark"},{"location":"overview/#pysparksql-package","text":"pyspark.sql is a Python package for Spark SQL. from pyspark.sql import * Tip Learn more about Modules and Packages in Python in The Python Tutorial .","title":"pyspark.sql Package"},{"location":"overview/#__init__py","text":"The __init__.py files are required to make Python treat directories containing the file as packages. Per 6.4.1. Importing * From a Package : The import statement uses the following convention: if a package's __init__.py code defines a list named __all__ , it is taken to be the list of module names that should be imported when from package import * is encountered. Per Public and Internal Interfaces in PEP 8 -- Style Guide for Python Code : To better support introspection, modules should explicitly declare the names in their public API using the __all__ attribute. From python/pyspark/sql/__init__.py : __all__ = [ 'SparkSession' , 'SQLContext' , 'HiveContext' , 'UDFRegistration' , 'DataFrame' , 'GroupedData' , 'Column' , 'Catalog' , 'Row' , 'DataFrameNaFunctions' , 'DataFrameStatFunctions' , 'Window' , 'WindowSpec' , 'DataFrameReader' , 'DataFrameWriter' , 'PandasCogroupedOps' ]","title":"__init__.py"},{"location":"spark-logging/","text":"Logging \u00b6 Spark uses log4j for logging. Note Learn more on Spark Logging in The Internals of Apache Spark online book.","title":"Logging"},{"location":"spark-logging/#logging","text":"Spark uses log4j for logging. Note Learn more on Spark Logging in The Internals of Apache Spark online book.","title":"Logging"}]}