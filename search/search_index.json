{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"The Internals of PySpark (Apache Spark 3.4.0-rc7)","text":"<p>Welcome to The Internals of PySpark online book! \ud83e\udd19</p> <p>I'm Jacek Laskowski, an IT freelancer specializing in Apache Spark (incl. Spark SQL and Spark Structured Streaming), Delta Lake, Databricks, and Apache Kafka (incl. Kafka Streams and ksqlDB) with brief forays into a wider data engineering space (e.g., Trino, Dask and dbt, mostly during Warsaw Data Engineering meetups).</p> <p>I'm very excited to have you here and hope you will enjoy exploring the internals of PySpark as much as I have.</p> <p>Flannery O'Connor</p> <p>I write to discover what I know.</p> <p>\"The Internals Of\" series</p> <p>I'm also writing other online books in the \"The Internals Of\" series. Please visit \"The Internals Of\" Online Books home page.</p> <p>Expect text and code snippets from a variety of public sources. Attribution follows.</p> <p>Now, let's take a deep dive into PySpark \ud83d\udd25</p> <p>Last update: 2023-04-12</p>"},{"location":"MonitorThread/","title":"MonitorThread","text":"<p><code>MonitorThread</code> is...FIXME</p>"},{"location":"Py4JServer/","title":"Py4JServer","text":"<p><code>Py4JServer</code> is a gateway server between Python and Java Virtual Machine (JVM) using Py4J.</p> <p><code>Py4JServer</code> is a wrapper for a py4j Server.</p>"},{"location":"Py4JServer/#creating-instance","title":"Creating Instance","text":"<p><code>Py4JServer</code> takes the following to be created:</p> <ul> <li> <code>SparkConf</code> (Spark Core) <p><code>Py4JServer</code> is created\u00a0when:</p> <ul> <li>PythonGatewayServer command-line application is started</li> <li>PythonRunner command-line application is started</li> </ul>"},{"location":"Py4JServer/#py4j-server","title":"py4j Server <p><code>Py4JServer</code> creates a <code>ClientServer</code> (py4j) or <code>GatewayServer</code> (py4j) based on PYSPARK_PIN_THREAD environment variable.</p>","text":""},{"location":"Py4JServer/#connection-secret","title":"Connection Secret <pre><code>secret: String\n</code></pre> <p><code>Py4JServer</code> creates a connection secret for a secure communication.</p>","text":""},{"location":"Py4JServer/#start","title":"start <pre><code>start(): Unit\n</code></pre> <p><code>start</code> requests the py4j Server to start.</p>","text":""},{"location":"Py4JServer/#getlisteningport","title":"getListeningPort <pre><code>getListeningPort: Int\n</code></pre> <p><code>getListeningPort</code> requests the py4j Server for the listening port.</p>","text":""},{"location":"PythonAccumulatorV2/","title":"PythonAccumulatorV2","text":"<p><code>PythonAccumulatorV2</code> is...FIXME</p>"},{"location":"PythonBroadcast/","title":"PythonBroadcast","text":"<p><code>PythonBroadcast</code> is...FIXME</p>"},{"location":"PythonForeachWriter/","title":"PythonForeachWriter","text":"<p><code>PythonForeachWriter</code> is...FIXME</p>"},{"location":"PythonFunction/","title":"PythonFunction","text":"<p><code>PythonFunction</code> is a metadata of a Python function to be executed in PythonRunner.</p>"},{"location":"PythonFunction/#creating-instance","title":"Creating Instance","text":"<p><code>PythonFunction</code> takes the following to be created:</p> <ul> <li> Command (<code>Array[Byte]</code>) <li> Environment Variables (<code>Map[String, String]</code>) <li> Python Includes (<code>List[String]</code>) <li> Python Executable <li> Python Version <li> Broadcast Variables with PythonBroadcast]s <li> PythonAccumulatorV2 <p><code>PythonFunction</code> is created when...FIXME</p>"},{"location":"PythonGatewayServer/","title":"PythonGatewayServer","text":"<p><code>PythonGatewayServer</code> is a command-line application (process) that starts a Py4JServer on an ephemeral port.</p> <p><code>PythonGatewayServer</code> is the Python runner for pyspark shell (Spark Core).</p>"},{"location":"PythonGatewayServer/#main","title":"main <p><code>main</code> creates a Py4JServer and requests it to start.</p> <p><code>main</code> requests the <code>Py4JServer</code> for the listening port (boundPort) and prints out the following DEBUG message to the logs:</p> <pre><code>Started PythonGatewayServer on port [boundPort]\n</code></pre> <p> <code>main</code> uses _PYSPARK_DRIVER_CONN_INFO_PATH environment variable for the path of a connection info file (for the associated python process) with the listening port and the secret. <p><code>main</code> pauses (blocks) until the Python driver finishes (by reading from the system input that blocks until input data is available, the end of the stream is detected, or an exception is thrown).</p> <p>In the end, once the Python driver finishes, <code>main</code> prints out the following DEBUG message to the logs:</p> <pre><code>Exiting due to broken pipe from Python driver\n</code></pre> <p><code>main</code> prints out the following ERROR message to the logs and exists when the listening port is <code>-1</code>:</p> <pre><code>[server] failed to bind; exiting\n</code></pre>","text":""},{"location":"PythonGatewayServer/#_pyspark_driver_conn_info_path","title":"_PYSPARK_DRIVER_CONN_INFO_PATH <p><code>PythonGatewayServer</code> uses <code>_PYSPARK_DRIVER_CONN_INFO_PATH</code> environment variable for the path of a connection info file for communication between this and the Python processes.</p> <p><code>_PYSPARK_DRIVER_CONN_INFO_PATH</code> is configured when java_gateway.py module is requested to launch_gateway.</p>","text":""},{"location":"PythonGatewayServer/#logging","title":"Logging <p>Enable <code>ALL</code> logging level for <code>org.apache.spark.api.python.PythonGatewayServer</code> logger to see what happens inside.</p> <p>Add the following line to <code>conf/log4j2.properties</code>:</p> <pre><code>logger.PythonGatewayServer.name = org.apache.spark.api.python.PythonGatewayServer\nlogger.PythonGatewayServer.level = all\n</code></pre> <p>Refer to Logging.</p>","text":""},{"location":"PythonRDD/","title":"PythonRDD","text":"<p><code>PythonRDD</code> is an <code>RDD</code> (<code>RDD[Array[Byte]]</code>) that uses PythonRunner (to compute a partition).</p>"},{"location":"PythonRDD/#creating-instance","title":"Creating Instance","text":"<p><code>PythonRDD</code> takes the following to be created:</p> <ul> <li> Parent <code>RDD</code> <li> PythonFunction <li> <code>preservePartitoning</code> flag <li> <code>isFromBarrier</code> flag (default: <code>false</code>) <p><code>PythonRDD</code> is created when...FIXME</p>"},{"location":"PythonRDD/#runjob","title":"runJob <pre><code>runJob(\n  sc: SparkContext,\n  rdd: JavaRDD[Array[Byte]],\n  partitions: JArrayList[Int]): Array[Any]\n</code></pre> <p><code>runJob</code>...FIXME</p>","text":""},{"location":"PythonRDD/#collectandserve","title":"collectAndServe <pre><code>collectAndServe[T](\n  rdd: RDD[T]): Array[Any]\n</code></pre> <p><code>collectAndServe</code>...FIXME</p>","text":""},{"location":"PythonRDD/#collectandservewithjobgroup","title":"collectAndServeWithJobGroup <pre><code>collectAndServeWithJobGroup[T](\n  rdd: RDD[T],\n  groupId: String,\n  description: String,\n  interruptOnCancel: Boolean): Array[Any]\n</code></pre> <p><code>collectAndServeWithJobGroup</code>...FIXME</p>","text":""},{"location":"PythonRDD/#serveiterator-utility","title":"serveIterator Utility <pre><code>serveIterator(\n  items: Iterator[_],\n  threadName: String): Array[Any]\n</code></pre> <p><code>serveIterator</code> serveToStream with a writer function that...FIXME</p> <p><code>serveIterator</code>\u00a0is used when:</p> <ul> <li><code>PythonRDD</code> utility is used to runJob, collectAndServe and collectAndServeWithJobGroup</li> <li><code>Dataset</code> is requested to <code>collectToPython</code>, <code>tailToPython</code>, <code>getRowsToPython</code></li> </ul>","text":""},{"location":"PythonRDD/#servetostream-utility","title":"serveToStream Utility <pre><code>serveToStream(\n  threadName: String)(\n  writeFunc: OutputStream =&gt; Unit): Array[Any]\n</code></pre> <p><code>serveToStream</code> serveToStream with the authHelper and the input arguments.</p> <p><code>serveToStream</code>\u00a0is used when:</p> <ul> <li><code>PythonRDD</code> utility is used to serveIterator</li> <li><code>Dataset</code> is requested to <code>collectAsArrowToPython</code></li> </ul>","text":""},{"location":"PythonRDD/#socketauthhelper","title":"SocketAuthHelper <p><code>PythonRDD</code> uses a SocketAuthHelper.</p>","text":""},{"location":"PythonRunner/","title":"PythonRunner","text":"<p><code>PythonRunner</code> is a command-line application to launch Python applications.</p> <p><code>PythonRunner</code> is used by spark-submit.</p> <p><code>PythonRunner</code> executes a configured python executable as a subprocess and then has it connect back to the JVM to access system properties, etc.</p>"},{"location":"PythonRunner/#arguments","title":"Arguments","text":"<p><code>PythonRunner</code> requires the following command-line arguments:</p> <ol> <li>Main python file (<code>pythonFile</code>)</li> <li>Extra python files (<code>pyFiles</code>)</li> <li>Application arguments</li> </ol>"},{"location":"PythonRunner/#main","title":"main <p><code>main</code> takes the arguments from command line.</p> <p> <code>main</code> determines what python executable to use based on (in that order): <ol> <li>spark.pyspark.driver.python configuration property</li> <li>spark.pyspark.python configuration property</li> <li><code>PYSPARK_DRIVER_PYTHON</code> environment variable</li> <li><code>PYSPARK_PYTHON</code> environment variable</li> <li><code>python3</code></li> </ol> <p><code>main</code> creates a Py4JServer that is started on a daemon py4j-gateway-init thread.</p> <p><code>main</code> waits until the gateway server has started.</p> <p><code>main</code> launches a Python process using the python executable and the following environment variables.</p>    Environment Variable Value     <code>PYTHONPATH</code>    <code>PYTHONUNBUFFERED</code> YES   PYSPARK_GATEWAY_PORT getListeningPort   PYSPARK_GATEWAY_SECRET secret   <code>PYSPARK_PYTHON</code> spark.pyspark.python if defined   <code>PYTHONHASHSEED</code> <code>PYTHONHASHSEED</code> env var if defined   <code>OMP_NUM_THREADS</code> <code>spark.driver.cores</code> unless defined    <p><code>main</code> waits for the Python process to finish and requests the <code>Py4JServer</code> to shutdown.</p>","text":""},{"location":"PythonRunner/#demo","title":"Demo <pre><code>./bin/spark-class org.apache.spark.deploy.PythonRunner\n</code></pre>","text":""},{"location":"PythonWorkerFactory/","title":"PythonWorkerFactory","text":""},{"location":"PythonWorkerFactory/#creating-instance","title":"Creating Instance","text":"<p><code>PythonWorkerFactory</code> takes the following to be created:</p> <ul> <li> Python Executable <li> Environment Variables (<code>Map[String, String]</code>) <p><code>PythonWorkerFactory</code> is created when <code>SparkEnv</code> is requested to <code>createPythonWorker</code> (when <code>BasePythonRunner</code> is requested to compute a partition).</p>"},{"location":"PythonWorkerFactory/#usedaemon-flag","title":"useDaemon Flag <p><code>PythonWorkerFactory</code> uses <code>useDaemon</code> internal flag that is the value of spark.python.use.daemon configuration property to decide whether to use lighter daemon or non-daemon workers.</p> <p><code>useDaemon</code> flag is used when <code>PythonWorkerFactory</code> requested to create, stop or release a worker and stop a daemon module.</p>","text":""},{"location":"PythonWorkerFactory/#python-daemon-module","title":"Python Daemon Module <p><code>PythonWorkerFactory</code> uses spark.python.daemon.module configuration property to define the Python Daemon Module.</p> <p>The Python Daemon Module is used when <code>PythonWorkerFactory</code> is requested to create and start a daemon module.</p>","text":""},{"location":"PythonWorkerFactory/#python-worker-module","title":"Python Worker Module <p><code>PythonWorkerFactory</code> uses spark.python.worker.module configuration property to specify the Python Worker Module.</p> <p>The Python Worker Module is used when <code>PythonWorkerFactory</code> is requested to create and start a worker.</p>","text":""},{"location":"PythonWorkerFactory/#creating-python-worker","title":"Creating Python Worker <pre><code>create(): Socket\n</code></pre> <p><code>create</code>...FIXME</p> <p><code>create</code> is used when <code>SparkEnv</code> is requested to <code>createPythonWorker</code>.</p>","text":""},{"location":"PythonWorkerFactory/#creating-daemon-worker","title":"Creating Daemon Worker <pre><code>createThroughDaemon(): Socket\n</code></pre> <p><code>createThroughDaemon</code>...FIXME</p> <p><code>createThroughDaemon</code> is used when <code>PythonWorkerFactory</code> is requested to create a Python worker (with useDaemon flag enabled).</p>","text":""},{"location":"PythonWorkerFactory/#starting-python-daemon-process","title":"Starting Python Daemon Process <pre><code>startDaemon(): Unit\n</code></pre> <p><code>startDaemon</code>...FIXME</p>","text":""},{"location":"PythonWorkerFactory/#creating-simple-non-daemon-worker","title":"Creating Simple Non-Daemon Worker <pre><code>createSimpleWorker(): Socket\n</code></pre> <p><code>createSimpleWorker</code>...FIXME</p> <p><code>createSimpleWorker</code> is used when <code>PythonWorkerFactory</code> is requested to create a Python worker (with useDaemon flag disabled).</p>","text":""},{"location":"PythonWorkerFactory/#logging","title":"Logging <p>Enable <code>ALL</code> logging level for <code>org.apache.spark.api.python.PythonWorkerFactory</code> logger to see what happens inside.</p> <p>Add the following line to <code>conf/log4j2.properties</code>:</p> <pre><code>logger.PythonWorkerFactory.name = org.apache.spark.api.python.PythonWorkerFactory\nlogger.PythonWorkerFactory.level = all\n</code></pre> <p>Refer to Logging.</p>","text":""},{"location":"Setup/","title":"PySpark Setup","text":""},{"location":"Setup/#install-ipython","title":"Install IPython","text":"<p>Follow the steps as described in the official documentation of IPython.</p> <pre><code>pip install ipython\n</code></pre>"},{"location":"Setup/#start-pyspark","title":"Start PySpark","text":"<pre><code>export PYSPARK_DRIVER_PYTHON=ipython\n</code></pre> <p>For Java 11, use <code>-Dio.netty.tryReflectionSetAccessible=true</code> (see Downloading in the official documentation of Apache Spark).</p> <pre><code>./bin/pyspark --driver-java-options=-Dio.netty.tryReflectionSetAccessible=true\n</code></pre> <pre><code>Python 3.9.1 (default, Feb  3 2021, 07:38:02)\nType 'copyright', 'credits' or 'license' for more information\nIPython 7.20.0 -- An enhanced Interactive Python. Type '?' for help.\nSetting default log level to \"WARN\".\nTo adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\nWelcome to\n      ____              __\n     / __/__  ___ _____/ /__\n    _\\ \\/ _ \\/ _ `/ __/  '_/\n   /__ / .__/\\_,_/_/ /_/\\_\\   version 3.1.1\n      /_/\n\nUsing Python version 3.9.1 (default, Feb  3 2021 07:38:02)\nSpark context Web UI available at http://192.168.68.101:4040\nSpark context available as 'sc' (master = local[*], app id = local-1613571272142).\nSparkSession available as 'spark'.\n\nIn [1]:\n</code></pre> <pre><code>In [1]: spark.version\nOut[1]: '3.1.1'\n</code></pre>"},{"location":"SocketAuthHelper/","title":"SocketAuthHelper","text":"<p><code>SocketAuthHelper</code> is...FIXME</p>"},{"location":"SocketAuthServer/","title":"SocketAuthServer","text":""},{"location":"SocketAuthServer/#servetostream-utility","title":"serveToStream Utility <pre><code>serveToStream(\n  threadName: String,\n  authHelper: SocketAuthHelper)(writeFunc: OutputStream =&gt; Unit): Array[Any]\n</code></pre> <p><code>serveToStream</code>...FIXME</p> <p><code>serveToStream</code>\u00a0is used when:</p> <ul> <li>FIXME</li> </ul>","text":""},{"location":"SocketFuncServer/","title":"SocketFuncServer","text":"<p><code>SocketFuncServer</code> is...FIXME</p>"},{"location":"SparkConf/","title":"SparkConf","text":"<p><code>SparkConf</code> is a Python class.</p>"},{"location":"SparkConf/#creating-instance","title":"Creating Instance","text":"<p><code>SparkConf</code> takes the following to be created:</p> <ul> <li> <code>loadDefaults</code> flag (default: <code>True</code>) <li> <code>JVMView</code> (py4j) <li> JConf (default: <code>None</code>) <p>While being created, <code>SparkConf</code> uses the JVMView (of the SparkContext) unless the <code>_jconf</code> and <code>_jvm</code> are given.</p>"},{"location":"SparkConf/#demo","title":"Demo","text":"<pre><code>from pyspark import SparkConf\n</code></pre>"},{"location":"SparkContext/","title":"SparkContext","text":""},{"location":"SparkContext/#creating-instance","title":"Creating Instance","text":"<p><code>SparkContext</code> takes the following to be created:</p> <ul> <li> Master URL (default: <code>None</code>) <li> Application Name (default: <code>None</code>) <li> Spark Home (default: <code>None</code>) <li> Py Files (default: <code>None</code>) <li> Environment (default: <code>None</code>) <li> Batch Size (default: <code>0</code>) <li> <code>PickleSerializer</code> <li> <code>SparkConf</code> (default: <code>None</code>) <li> Gateway (default: <code>None</code>) <li> Corresponding <code>SparkContext</code> on JVM (default: <code>None</code>) <li> <code>BasicProfiler</code> <p>While being created, <code>SparkContext</code> _ensure_initialized (with the gateway and the conf) followed by _do_init.</p>"},{"location":"SparkContext/#demo","title":"Demo","text":"<pre><code>from pyspark import SparkContext\n</code></pre>"},{"location":"SparkContext/#javagateway","title":"JavaGateway <p><code>SparkContext</code> defines <code>_gateway</code> property for a <code>JavaGateway</code> that is given or launched when _ensure_initialized.</p>","text":""},{"location":"SparkContext/#jvmview","title":"JVMView <p><code>SparkContext</code> defines <code>_jvm</code> property for a <code>JVMView</code> (py4j) to access to the Java Virtual Machine of the JavaGateway.</p>","text":""},{"location":"SparkContext/#_ensure_initialized","title":"_ensure_initialized <pre><code>_ensure_initialized(\n  cls, instance=None, gateway=None, conf=None)\n</code></pre> <p><code>_ensure_initialized</code> is a <code>@classmethod</code>.</p> <p><code>_ensure_initialized</code> takes the given gateway or launch_gateway.</p> <p><code>_ensure_initialized</code>...FIXME</p> <p><code>_ensure_initialized</code>\u00a0is used when:</p> <ul> <li><code>SparkContext</code> is created and <code>setSystemProperty</code></li> <li>shell.py is launched</li> </ul>","text":""},{"location":"SparkContext/#_do_init","title":"_do_init <pre><code>_do_init(\n  self, master, appName, sparkHome,\n  pyFiles, environment, batchSize, serializer,\n  conf, jsc, profiler_cls)\n</code></pre> <p><code>_do_init</code>...FIXME</p>","text":""},{"location":"building-from-sources/","title":"Building from Sources","text":"<pre><code>$ java -version\nopenjdk version \"11.0.10\" 2021-01-19\nOpenJDK Runtime Environment AdoptOpenJDK (build 11.0.10+9)\nOpenJDK 64-Bit Server VM AdoptOpenJDK (build 11.0.10+9, mixed mode)\n</code></pre> <pre><code>./build/mvn \\\n  -Pyarn,kubernetes,hive,hive-thriftserver,scala-2.12 \\\n  -DskipTests \\\n  clean install\n</code></pre>"},{"location":"building-from-sources/#building-pyspark-related-operators","title":"Building PySpark-Related Operators","text":"<pre><code>./build/mvn -DskipTests -pl :spark-sql_2.12 clean install\n</code></pre> <pre><code>cp sql/core/target/spark-sql_2.12-3.1.1.jar assembly/target/scala-2.12/jars/\n</code></pre>"},{"location":"configuration-properties/","title":"Configuration Properties","text":""},{"location":"configuration-properties/#sparkpysparkdriverpython","title":"spark.pyspark.driver.python <p>Default: (undefined)</p>","text":""},{"location":"configuration-properties/#sparkpysparkpython","title":"spark.pyspark.python <p>Default: (undefined)</p>","text":""},{"location":"configuration-properties/#sparkpythonusedaemon","title":"spark.python.use.daemon <p>Because forking processes from Java is expensive, we prefer to launch a single Python daemon, <code>pyspark/daemon.py</code> (by default) and tell it to fork new workers for our tasks. This daemon currently only works on UNIX-based systems now because it uses signals for child management, so we can also fall back to launching workers, <code>pyspark/worker.py</code> (by default) directly.</p> <p>Default: <code>true</code> (always disabled on Windows)</p> <p>Used when PythonWorkerFactory is created</p>","text":""},{"location":"configuration-properties/#sparkpythondaemonmodule","title":"spark.python.daemon.module <p>Default: <code>pyspark.daemon</code></p> <p>Used when PythonWorkerFactory is created</p>","text":""},{"location":"configuration-properties/#sparkpythonworkermodule","title":"spark.python.worker.module <p>Default: (undefined)</p> <p>Used when PythonWorkerFactory is created</p>","text":""},{"location":"environment-variables/","title":"Environment Variables","text":"<p>PySpark uses environment variables to configure execution environment.</p>"},{"location":"environment-variables/#pyspark_gateway_port","title":"PYSPARK_GATEWAY_PORT","text":""},{"location":"environment-variables/#pyspark_gateway_secret","title":"PYSPARK_GATEWAY_SECRET","text":""},{"location":"environment-variables/#pyspark_pin_thread","title":"PYSPARK_PIN_THREAD <p>Enables pinned thread mode to synchronize PVM threads with JVM threads based on Py4J's ClientServer (<code>true</code>) or GatewayServer (<code>false</code>)</p> <p>Default: <code>false</code></p> <p>Used when:</p> <ul> <li>launch_gateway is executed</li> <li>Py4JServer is created (and initializes the server)</li> </ul>","text":""},{"location":"logging/","title":"Logging","text":"<p>Delta Lake uses the same logging infrastructure as Apache Spark.</p>"},{"location":"overview/","title":"PySpark","text":"<p>PySpark is the Python frontend for Apache Spark.</p>"},{"location":"overview/#shellpy","title":"shell.py","text":"<p>pyspark shell defines PYTHONSTARTUP environment variable to execute shell.py before the first prompt is displayed in Python interactive mode.</p>"},{"location":"overview/#py4j","title":"Py4J","text":"<p>java_gateway uses Py4J - A Bridge between Python and Java:</p> <p>Py4J enables Python programs running in a Python interpreter to dynamically access Java objects in a Java Virtual Machine. Methods are called as if the Java objects resided in the Python interpreter and Java collections can be accessed through standard Python collection methods. Py4J also enables Java programs to call back Python objects.</p>"},{"location":"overview/#pysparksql-package","title":"pyspark.sql Package","text":"<p><code>pyspark.sql</code> is a Python package for Spark SQL.</p> <pre><code>from pyspark.sql import *\n</code></pre> <p>Tip</p> <p>Learn more about Modules and Packages in Python in The Python Tutorial.</p>"},{"location":"overview/#__init__py","title":"__init__.py","text":"<p>The <code>__init__.py</code> files are required to make Python treat directories containing the file as packages.</p> <p>Per 6.4.1. Importing * From a Package:</p> <p>The import statement uses the following convention: if a package's <code>__init__.py</code> code defines a list named <code>__all__</code>, it is taken to be the list of module names that should be imported when <code>from package import *</code> is encountered.</p> <p>Per Public and Internal Interfaces in PEP 8 -- Style Guide for Python Code:</p> <p>To better support introspection, modules should explicitly declare the names in their public API using the <code>__all__</code> attribute.</p> <p>From <code>python/pyspark/sql/__init__.py</code>:</p> <pre><code>__all__ = [\n    'SparkSession', 'SQLContext', 'HiveContext', 'UDFRegistration',\n    'DataFrame', 'GroupedData', 'Column', 'Catalog', 'Row',\n    'DataFrameNaFunctions', 'DataFrameStatFunctions', 'Window', 'WindowSpec',\n    'DataFrameReader', 'DataFrameWriter', 'PandasCogroupedOps'\n]\n</code></pre>"},{"location":"overview/#pandas","title":"pandas","text":"<p>The minimum version of Pandas is <code>0.23.2</code> (and PandasConversionMixin asserts that).</p> <pre><code>import pandas as pd\n</code></pre>"},{"location":"overview/#pyarrow","title":"pyarrow","text":"<p>The minimum version of PyArrow is <code>1.0.0</code> (and PandasConversionMixin asserts that).</p> <pre><code>import pyarrow\n</code></pre>"},{"location":"overview/#python-mixins","title":"Python Mixins","text":"<p>From 8.7. Class definitions:</p> <p>classdef    ::=  [decorators] \"class\" classname [inheritance] \":\" suite</p> <p>The inheritance list usually gives a list of base classes</p> <p>PySpark uses mixins:</p> <ul> <li>PandasConversionMixin</li> <li>PandasMapOpsMixin</li> <li>SparkConversionMixin</li> </ul>"},{"location":"overview/#pandas-user-defined-functions","title":"Pandas User Defined Functions","text":"<p>Pandas User Defined Functions (vectorized user defined functions) are user-defined functions that are executed by Spark using Arrow to transfer data and Pandas to work with the data, which allows vectorized operations.</p> <p>Pandas UDFs are defined using pandas_udf function as a decorator (using <code>@pandas_udf(returnType, functionType)</code> annotation) or to wrap the function, and no additional configuration.</p> <p>A Pandas UDF behaves as a regular PySpark function API in general.</p> <p>The minimum versions supported:</p> <ul> <li>pandas 0.23.2</li> <li>pyarrow 1.0.0</li> </ul> <p>As of Spark 3.0 with Python 3.6+, using Python type hints to specify type hints for the pandas UDF is encouraged (instead of specifying pandas UDF type via <code>functionType</code> argument).</p> <p>The type hint should use <code>pandas.Series</code> in most cases (except <code>pandas.DataFrame</code>).</p>"},{"location":"pyspark/","title":"PySpark Shell","text":"<p><code>pyspark</code> is a Spark shell for Python.</p>"},{"location":"demo/","title":"Demos","text":"<ol> <li>Executing PySpark Applications Using spark-submit</li> <li>Running PySpark Application on minikube</li> </ol>"},{"location":"demo/executing-pyspark-applications-using-spark-submit/","title":"Demo: Executing PySpark Applications Using spark-submit","text":"<p>PySpark applications are executed using <code>spark-submit</code> (Spark Core) command-line application.</p> <pre><code>spark-submit 1.py extra args\n</code></pre> <p>For a PySpark application, <code>spark-submit</code> uses PythonRunner and launches an extra python process:</p> <pre><code>ps -o pid,ppid,command | grep python | grep -v grep\n</code></pre> <pre><code>org.apache.spark.deploy.SparkSubmit 1.py extra args\n</code></pre> <pre><code>Python /usr/local/bin/ipython 1.py extra args\n</code></pre>"},{"location":"demo/executing-pyspark-applications-using-spark-submit/#spark_print_launch_command-environment-variable","title":"SPARK_PRINT_LAUNCH_COMMAND Environment Variable","text":"<p>Use <code>SPARK_PRINT_LAUNCH_COMMAND</code> environment variable to have the complete Spark command printed out to the standard output (cf. spark-submit shell script).</p> <pre><code>SPARK_PRINT_LAUNCH_COMMAND=1 spark-submit 1.py extra args\n</code></pre>"},{"location":"demo/executing-pyspark-applications-using-spark-submit/#verbose-option","title":"verbose Option","text":"<p>Use <code>--verbose</code> option for verbose debugging output.</p> <pre><code>Parsed arguments:\n  ...\n  pyFiles                 null\n  ...\n  primaryResource         file:/Users/jacek/dev/sandbox/python-sandbox/1.py\n  name                    1.py\n  childArgs               [extra args]\n...\nMain class:\norg.apache.spark.deploy.PythonRunner\nArguments:\nfile:/Users/jacek/dev/sandbox/python-sandbox/1.py\nnull\nextra\nargs\nSpark config:\n(spark.app.name,1.py)\n(spark.master,local[*])\n(spark.submit.pyFiles,)\n(spark.submit.deployMode,client)\n</code></pre>"},{"location":"demo/running-pyspark-application-on-minikube/","title":"Demo: Running PySpark Application on minikube","text":"<p>This demo shows how to run a PySpark application on Kubernetes (using minikube).</p> <p>Tip</p> <p>This is a follow-up demo to Demo: Running Spark Application on minikube in the The Internals of Spark on Kubernetes.</p>"},{"location":"ml/","title":"PySpark MLlib","text":"<p>PySpark MLlib is a Python module to work with Spark MLlib for <code>DataFrame</code>-based machine learning pipelines.</p> <pre><code>from pyspark.ml import *\n</code></pre>"},{"location":"ml/Distributor/","title":"Distributor","text":"<p><code>Distributor</code> is the parent (abstract) class of TorchDistributor.</p>"},{"location":"ml/Distributor/#creating-instance","title":"Creating Instance","text":"<p><code>Distributor</code> takes the following to be created:</p> <ul> <li> Number of processes (default: <code>1</code>) <li> <code>local_mode</code> flag (default: <code>True</code>) <li> <code>use_gpu</code> flag (default: <code>True</code>) <p>Abstract Class</p> <p><code>Distributor</code> is not supposed to be created directly.</p>"},{"location":"ml/Distributor/#_get_num_tasks","title":"_get_num_tasks","text":"<pre><code>_get_num_tasks(\n  self) -&gt; int\n</code></pre> <p><code>_get_num_tasks</code>...FIXME</p>"},{"location":"ml/Distributor/#get_gpus_owned","title":"get_gpus_owned","text":"<pre><code>get_gpus_owned(\n  context: Union[SparkContext, BarrierTaskContext]) -&gt; List[str]\n</code></pre> <p><code>get_gpus_owned</code>...FIXME</p> <p><code>get_gpus_owned</code> is used when:</p> <ul> <li><code>TorchDistributor</code> is requested to _run_local_training and _get_spark_task_function</li> </ul>"},{"location":"ml/TorchDistributor/","title":"TorchDistributor","text":"<p><code>TorchDistributor</code> is a Distributor.</p> <pre><code>from pyspark.ml.torch.distributor import TorchDistributor\n\ndistributor = TorchDistributor(\n    num_processes=1,\n    local_mode=True,\n    use_gpu=False)\n\n# accepts a Callable (function) or a path to a training script\n# and variable-length kwargs\ndistributor.run(\n    \"train.py\",    # a training script\n    \"--learning-rate=1e-3\",\n    \"--batch-size=64\",\n    \"--my-key=my-value\")\n\n# Started local training with 1 processes\n# NOTE: Redirects are currently not supported in Windows or MacOs.\n# Finished local training with 1 processes\n</code></pre>"},{"location":"ml/TorchDistributor/#run","title":"Running Distributed Training","text":"<pre><code>run(\n    self,\n    train_object: Union[Callable, str],\n    *args: Any) -&gt; Optional[Any]\n</code></pre> <p><code>run</code>...FIXME</p>"},{"location":"pyspark/java_gateway/","title":"java_gateway.py","text":"<p><code>java_gateway</code> is a Python module that allows launching a gateway process to establish communication channel to Py4JServer.</p>"},{"location":"pyspark/java_gateway/#launch_gateway","title":"launch_gateway <pre><code>launch_gateway(\n  conf=None,\n  popen_kwargs=None)\n</code></pre> <p><code>launch_gateway</code> reads PYSPARK_GATEWAY_PORT and PYSPARK_GATEWAY_SECRET environment variables if defined and assumes that the child Java gateway process has already been started (e.g. PythonGatewayServer).</p> <p> <p>Otherwise, <code>launch_gateway</code> builds the command to start <code>spark-submit</code>:</p> <ol> <li>Finds <code>SPARK_HOME</code> with <code>./bin/spark-submit</code></li> <li>Appends all the configuration properties (from the input <code>conf</code>) using <code>--conf</code></li> <li>Appends <code>PYSPARK_SUBMIT_ARGS</code> environment variable if defined or assumes <code>pyspark-shell</code></li> </ol> <p><code>launch_gateway</code> sets up <code>_PYSPARK_DRIVER_CONN_INFO_PATH</code> environment variable to point at an unique temporary file.</p> <p><code>launch_gateway</code> configures a pipe to stdin for the corresponding Java gateway process to use to monitor the Python process.</p> <p><code>launch_gateway</code> starts <code>bin/spark-submit</code> command and waits for a connection info file to be created at <code>_PYSPARK_DRIVER_CONN_INFO_PATH</code>. <code>launch_gateway</code> reads the port and the secret from the file once available.</p> <p><code>launch_gateway</code> connects to the gateway using py4j's <code>ClientServer</code> or <code>JavaGateway</code> based on PYSPARK_PIN_THREAD environment variable.</p> <p><code>launch_gateway</code> imports Spark packages and classes (using py4j):</p> <ul> <li><code>org.apache.spark.SparkConf</code></li> <li><code>org.apache.spark.api.java.*</code></li> <li><code>org.apache.spark.api.python.*</code></li> <li><code>org.apache.spark.ml.python.*</code></li> <li><code>org.apache.spark.mllib.api.python.*</code></li> <li><code>org.apache.spark.resource.*</code></li> <li><code>org.apache.spark.sql.*</code></li> <li><code>org.apache.spark.sql.api.python.*</code></li> <li><code>org.apache.spark.sql.hive.*</code></li> <li><code>scala.Tuple2</code></li> </ul> <p><code>launch_gateway</code>\u00a0is used when:</p> <ul> <li><code>SparkContext</code> is requested to _ensure_initialized</li> </ul>","text":""},{"location":"pyspark/shell/","title":"shell.py","text":"<p><code>shell.py</code> script is the interactive shell of PySpark.</p> <p><code>shell.py</code> defines the following variables:</p> <ul> <li><code>sc</code> being pyspark.SparkContext</li> <li><code>spark</code> being pyspark.sql.session.SparkSession</li> <li><code>sql</code> being SparkSession.sql</li> <li><code>sqlContext</code> and <code>sqlCtx</code> for compatibility</li> </ul>"},{"location":"pyspark/sql/functions/","title":"functions.py","text":"<p><code>functions.py</code> module belongs to <code>pyspark.sql</code> package.</p>"},{"location":"pyspark/sql/functions/#udf","title":"udf <pre><code>udf(f=None, returnType=StringType())\n</code></pre> <p><code>udf</code>...FIXME</p>","text":""},{"location":"pyspark/sql/udf/","title":"udf.py","text":"<p><code>udf.py</code> module belongs to <code>pyspark.sql</code> package.</p>"},{"location":"pyspark/sql/udf/#public-objects","title":"Public Objects <p><code>udf</code> module defines UDFRegistration as the public object.</p>","text":""},{"location":"pyspark/sql/udf/#_create_udf","title":"_create_udf <pre><code>_create_udf(f, returnType, evalType)\n</code></pre> <p><code>_create_udf</code> creates an UserDefinedFunction (with the name of the object to be the name of function <code>f</code>).</p> <p><code>_create_udf</code>\u00a0is used when:</p> <ul> <li>pandas_udf function is used</li> <li>udf function is used</li> </ul>","text":""},{"location":"pyspark/sql/pandas/functions/","title":"functions.py","text":"<p><code>functions.py</code> defines pandas_udf for pandas user defined functions.</p> <p><code>functions.py</code> is part of <code>pyspark.sql.pandas</code> package.</p> <pre><code>from pyspark.sql.functions import pandas_udf\n</code></pre>"},{"location":"pyspark/sql/pandas/functions/#pandas_udf","title":"pandas_udf <pre><code>pandas_udf(f=None, returnType=None, functionType=None)\n</code></pre> <p><code>pandas_udf</code> creates a pandas user defined functions.</p> <p><code>functionType</code> must be one the values from <code>PandasUDFType</code>:</p> <ul> <li><code>PythonEvalType.SQL_SCALAR_PANDAS_UDF</code></li> <li><code>PythonEvalType.SQL_SCALAR_PANDAS_ITER_UDF</code></li> <li><code>PythonEvalType.SQL_GROUPED_MAP_PANDAS_UDF</code></li> <li><code>PythonEvalType.SQL_GROUPED_AGG_PANDAS_UDF</code></li> <li><code>PythonEvalType.SQL_MAP_PANDAS_ITER_UDF</code></li> <li><code>PythonEvalType.SQL_COGROUPED_MAP_PANDAS_UDF</code></li> </ul>","text":""},{"location":"runners/ArrowPythonRunner/","title":"ArrowPythonRunner","text":"<p><code>ArrowPythonRunner</code> is a pyspark runner with <code>PythonArrowOutput</code> for...FIXME</p>"},{"location":"runners/ArrowPythonRunner/#creating-instance","title":"Creating Instance","text":"<p><code>ArrowPythonRunner</code> takes the following to be created:</p> <ul> <li> <code>Seq[ChainedPythonFunctions]</code> <li> Eval Type <li> Argument Offsets (<code>Array[Array[Int]]</code>) <li> <code>Schema</code> (Spark SQL) <li> TimeZone ID <li> Configuration (<code>Map[String, String]</code>) <p><code>ArrowPythonRunner</code> is created when AggregateInPandasExec, <code>ArrowEvalPythonExec</code>, <code>FlatMapGroupsInPandasExec</code>, <code>MapInPandasExec</code>, <code>WindowInPandasExec</code> physical operators are executed.</p>"},{"location":"runners/BasePythonRunner/","title":"BasePythonRunner","text":"<p><code>BasePythonRunner</code> is an abstraction of Python Runners.</p>"},{"location":"runners/BasePythonRunner/#contract","title":"Contract","text":""},{"location":"runners/BasePythonRunner/#newreaderiterator","title":"newReaderIterator <pre><code>newReaderIterator(\n  stream: DataInputStream,\n  writerThread: WriterThread,\n  startTime: Long,\n  env: SparkEnv,\n  worker: Socket,\n  releasedOrClosed: AtomicBoolean,\n  context: TaskContext): Iterator[OUT]\n</code></pre> <p>Used when <code>BasePythonRunner</code> is requested to compute</p>","text":""},{"location":"runners/BasePythonRunner/#newwriterthread","title":"newWriterThread <pre><code>newWriterThread(\n  env: SparkEnv,\n  worker: Socket,\n  inputIterator: Iterator[IN],\n  partitionIndex: Int,\n  context: TaskContext): WriterThread\n</code></pre> <p>Used when <code>BasePythonRunner</code> is requested to compute</p>","text":""},{"location":"runners/BasePythonRunner/#implementations","title":"Implementations","text":"<ul> <li>ArrowPythonRunner</li> <li>CoGroupedArrowPythonRunner</li> <li>PythonRunner</li> <li>PythonUDFRunner</li> </ul>"},{"location":"runners/BasePythonRunner/#scala-definition","title":"Scala Definition","text":"<p><code>BasePythonRunner</code> is a type constructor in Scala (generic class in Java) with the following definition:</p> <pre><code>abstract class BasePythonRunner[IN, OUT](...) {\n// ...\n}\n</code></pre> <p><code>BasePythonRunner</code> uses <code>IN</code> and <code>OUT</code> as the name of the types for the input and output values.</p>"},{"location":"runners/BasePythonRunner/#creating-instance","title":"Creating Instance","text":"<p><code>BasePythonRunner</code> takes the following to be created:</p> <ul> <li> <code>ChainedPythonFunctions</code> <li> Eval Type <li> Argument Offsets <p><code>BasePythonRunner</code> requires that the number of ChainedPythonFunctions and Argument Offsets are the same.</p> <p>Abstract Class</p> <p><code>BasePythonRunner</code> is an abstract class and cannot be created directly. It is created indirectly for the concrete BasePythonRunners.</p>"},{"location":"runners/BasePythonRunner/#computing-result","title":"Computing Result <pre><code>compute(\n  inputIterator: Iterator[IN],\n  partitionIndex: Int,\n  context: TaskContext): Iterator[OUT]\n</code></pre> <p><code>compute</code> makes sure that <code>spark.executorEnv.OMP_NUM_THREADS</code> configuration option is set or defaults to <code>spark.executor.cores</code> property.</p> <p><code>compute</code> defines the following environment variables:</p> <ul> <li><code>SPARK_LOCAL_DIRS</code> to be the local directories of the local <code>DiskBlockManager</code></li> <li><code>SPARK_BUFFER_SIZE</code> to be the value of <code>spark.buffer.size</code> configuration property (default: <code>65536</code>)</li> </ul> <p><code>compute</code> can optionally define environment variables:</p> <ul> <li><code>SPARK_REUSE_WORKER</code> to be <code>1</code> based on <code>spark.python.worker.reuse</code> configuration property (default: <code>true</code>)</li> <li><code>PYSPARK_EXECUTOR_MEMORY_MB</code> to be the value of <code>spark.executor.pyspark.memory</code> configuration property if defined</li> </ul> <p><code>compute</code> requests the executor's <code>SparkEnv</code> to <code>createPythonWorker</code> (for a <code>pythonExec</code> and the environment variables) that requests PythonWorkerFactory to create a Python worker (and give a <code>java.net.Socket</code>).</p>  <p>FIXME</p> <p>Describe <code>pythonExec</code>.</p>  <p><code>compute</code> [newWriterThread] with the Python worker and the input arguments.</p> <p><code>compute</code> creates and starts a MonitorThread to watch the Python worker.</p> <p><code>compute</code> creates a new reader iterator to read lines from the Python worker's stdout.</p> <p><code>compute</code> is used when:</p> <ul> <li><code>PythonRDD</code> is requested to <code>compute</code></li> <li>AggregateInPandasExec, <code>ArrowEvalPythonExec</code>, <code>BatchEvalPythonExec</code>, <code>FlatMapCoGroupsInPandasExec</code>, <code>FlatMapGroupsInPandasExec</code> <code>MapInPandasExec</code>, <code>WindowInPandasExec</code> physical operators are executed</li> </ul>","text":""},{"location":"runners/PythonRunner/","title":"PythonRunner","text":"<p><code>PythonRunner</code> is a concrete BasePythonRunner.</p>"},{"location":"runners/PythonRunner/#creating-instance","title":"Creating Instance","text":"<p><code>PythonRunner</code> takes the following to be created:</p> <ul> <li> <code>Seq[ChainedPythonFunctions]</code> <p><code>PythonRunner</code> is created (indirectly using apply factory method) when:</p> <ul> <li><code>PythonRDD</code> is requested to compute a partition</li> <li><code>PythonForeachWriter</code> is requested for a PythonRunner</li> </ul>"},{"location":"runners/PythonRunner/#creating-pythonrunner","title":"Creating PythonRunner <pre><code>apply(\n  func: PythonFunction): PythonRunner\n</code></pre> <p><code>apply</code> simply creates a PythonRunner for the PythonFunction.</p> <p><code>apply</code> is used when:</p> <ul> <li><code>PythonRDD</code> is requested to compute a partition</li> <li><code>PythonForeachWriter</code> is requested for a PythonRunner</li> </ul>","text":""},{"location":"runners/PythonUDFRunner/","title":"PythonUDFRunner","text":"<p><code>PythonUDFRunner</code> is...FIXME</p>"},{"location":"sql/","title":"PySpark SQL","text":"<p>PySpark SQL is a Python module to work with Spark SQL.</p> <pre><code>from pyspark.sql import *\n</code></pre>"},{"location":"sql/AggregateInPandasExec/","title":"AggregateInPandasExec Physical Operator","text":"<p><code>AggregateInPandasExec</code> is a unary physical operator (Spark SQL).</p>"},{"location":"sql/AggregateInPandasExec/#creating-instance","title":"Creating Instance","text":"<p><code>AggregateInPandasExec</code> takes the following to be created:</p> <ul> <li> Grouping Expressions (Spark SQL) (<code>Seq[NamedExpression]</code>) <li> PythonUDFs <li> Result Named Expressions (Spark SQL) (<code>Seq[NamedExpression]</code>) <li> Child Physical Operator (Spark SQL) <p><code>AggregateInPandasExec</code> is created when <code>Aggregation</code> execution planning strategy (Spark SQL) is executed for <code>Aggregate</code> logical operators (Spark SQL) with PythonUDF aggregate expressions only.</p>"},{"location":"sql/AggregateInPandasExec/#executing-operator","title":"Executing Operator <pre><code>doExecute(): RDD[InternalRow]\n</code></pre> <p><code>doExecute</code> uses ArrowPythonRunner (one per partition) to execute PythonUDFs.</p> <p><code>doExecute</code> is part of the <code>SparkPlan</code> (Spark SQL) abstraction.</p>","text":""},{"location":"sql/DataFrame/","title":"DataFrame","text":"<p><code>DataFrame</code> is a Python class with PandasMapOpsMixin and PandasConversionMixin mixins.</p> <p><code>DataFrame</code> lives in <code>pyspark.sql.dataframe</code> module (together with <code>DataFrameNaFunctions</code> and <code>DataFrameStatFunctions</code>).</p> <pre><code>from pyspark.sql.dataframe import DataFrame\n</code></pre>"},{"location":"sql/DataFrame/#creating-instance","title":"Creating Instance","text":"<p><code>DataFrame</code> takes the following to be created:</p> <ul> <li> jdf <li> SQLContext"},{"location":"sql/DataFrame/#groupby","title":"groupBy <pre><code>groupBy(self, *cols)\n</code></pre> <p><code>groupBy</code> requests the _jdf to <code>groupBy</code> and creates a GroupedData with it.</p>","text":""},{"location":"sql/FlatMapGroupsInPandas/","title":"FlatMapGroupsInPandas Logical Operator","text":"<p><code>FlatMapGroupsInPandas</code> is a unary logical operator (Spark SQL).</p> <p><code>FlatMapGroupsInPandas</code> is planned as a FlatMapGroupsInPandasExec physical operator.</p>"},{"location":"sql/FlatMapGroupsInPandas/#creating-instance","title":"Creating Instance","text":"<p><code>FlatMapGroupsInPandas</code> takes the following to be created:</p> <ul> <li> Grouping Attributes (Spark SQL) <li> Function Expression (Spark SQL) <li> Output Attributes (Spark SQL) <li> Child Logical Operator (Spark SQL) <p><code>FlatMapGroupsInPandas</code> is created\u00a0when:</p> <ul> <li><code>RelationalGroupedDataset</code> is requested to flatMapGroupsInPandas (with a PythonUDF)</li> </ul>"},{"location":"sql/FlatMapGroupsInPandasExec/","title":"FlatMapGroupsInPandasExec Physical Operator","text":"<p><code>FlatMapGroupsInPandasExec</code> is a unary physical operator (Spark SQL) to execute a PythonUDF.</p> <p><code>FlatMapGroupsInPandasExec</code> represents a FlatMapGroupsInPandas logical operator at execution time.</p>"},{"location":"sql/FlatMapGroupsInPandasExec/#creating-instance","title":"Creating Instance","text":"<p><code>FlatMapGroupsInPandasExec</code> takes the following to be created:</p> <ul> <li> Grouping Attributes (Spark SQL) <li> Function Expression (Spark SQL) <li> Output Attributes (Spark SQL) <li> Child Physical Operator (Spark SQL) <p><code>FlatMapGroupsInPandasExec</code> is created\u00a0when:</p> <ul> <li><code>BasicOperators</code> (Spark SQL) execution planning strategy is executed (on a logical query plan with FlatMapGroupsInPandas logical operators)</li> </ul>"},{"location":"sql/GroupedData/","title":"GroupedData","text":"<p><code>GroupedData</code> is created by DataFrame.groupBy.</p> <p><code>GroupedData</code> is a Python class with PandasGroupedOpsMixin mixin.</p> <p><code>GroupedData</code> is defined in <code>pyspark.sql.group</code> module.</p> <pre><code>from pyspark.sql.group import GroupedData\n</code></pre>"},{"location":"sql/GroupedData/#creating-instance","title":"Creating Instance","text":"<p><code>GroupedData</code> takes the following to be created:</p> <ul> <li> RelationalGroupedDataset <li> DataFrame <p><code>GroupedData</code> is created\u00a0when:</p> <ul> <li>FIXME</li> </ul>"},{"location":"sql/PandasCogroupedOps/","title":"PandasCogroupedOps","text":"<p><code>PandasCogroupedOps</code> is a logical grouping created by GroupedData.cogroup.</p> <pre><code>from pyspark.sql.pandas.group_ops import PandasCogroupedOps\n</code></pre> <p><code>PandasCogroupedOps</code> is included in <code>__all__</code> of <code>pyspark.sql</code> module (via <code>__init__.py</code>).</p>"},{"location":"sql/PandasCogroupedOps/#creating-instance","title":"Creating Instance","text":"<p><code>PandasCogroupedOps</code> takes the following to be created:</p> <ul> <li> GroupedData <li> GroupedData <p><code>PandasCogroupedOps</code> is created\u00a0when:</p> <ul> <li><code>PandasGroupedOpsMixin</code> is requested to cogroup</li> </ul>"},{"location":"sql/PandasCogroupedOps/#applyinpandas","title":"applyInPandas <pre><code>applyInPandas(self, func, schema)\n</code></pre> <p><code>applyInPandas</code> creates a DataFrame with the result of flatMapCoGroupsInPandas with a pandas user defined function of <code>SQL_COGROUPED_MAP_PANDAS_UDF</code> type.</p>  <p><code>applyInPandas</code> creates a pandas user defined function for the given <code>func</code> and the return type by the given <code>schema</code>. The pandas UDF is of <code>SQL_COGROUPED_MAP_PANDAS_UDF</code> type.</p> <p><code>applyInPandas</code> applies the pandas UDF on all the columns of the two GroupedDatas (that creates a <code>Column</code> expression).</p> <p><code>applyInPandas</code> requests the GroupedData for the associated RelationalGroupedDataset that is in turn requested to flatMapCoGroupsInPandas.</p>","text":""},{"location":"sql/PandasConversionMixin/","title":"PandasConversionMixin","text":"<p><code>PandasConversionMixin</code> is a Python mixin for DataFrame class.</p>"},{"location":"sql/PandasConversionMixin/#topandas","title":"toPandas <pre><code>toPandas(self)\n</code></pre> <p><code>toPandas</code> can only be used with <code>DataFrame</code> (from <code>pyspark.sql.dataframe</code>).</p> <p><code>toPandas</code>...FIXME</p>","text":""},{"location":"sql/PandasGroupedOpsMixin/","title":"PandasGroupedOpsMixin","text":"<p><code>PandasGroupedOpsMixin</code> is a Python mixin for GroupedData class.</p>"},{"location":"sql/PandasGroupedOpsMixin/#applyinpandas","title":"applyInPandas <pre><code>applyInPandas(self, func, schema)\n</code></pre> <p><code>applyInPandas</code> creates a <code>DataFrame</code> with flatMapGroupsInPandas.</p>","text":""},{"location":"sql/PandasGroupedOpsMixin/#example","title":"Example <pre><code>df1 = spark.createDataFrame(\n    [(20000101, 1, 1.0), (20000101, 2, 2.0), (20000102, 1, 3.0), (20000102, 2, 4.0)],\n    (\"time\", \"id\", \"v1\"))\ndf2 = spark.createDataFrame(\n    [(20000101, 1, \"x\"), (20000101, 2, \"y\")],\n    (\"time\", \"id\", \"v2\"))\n</code></pre> <pre><code>import pandas as pd\ndef asof_join(k, l, r):\n  if k == (1,):\n    return pd.merge_asof(l, r, on=\"time\", by=\"id\")\n  else:\n    return pd.DataFrame(columns=['time', 'id', 'v1', 'v2'])\n</code></pre> <pre><code>gd1 = df1.groupby(\"id\")\ngd2 = df2.groupby(\"id\")\n\ngd1.cogroup(gd2).applyInPandas(\n  asof_join, \"time int, id int, v1 double, v2 string\").show()\n</code></pre>","text":""},{"location":"sql/PandasGroupedOpsMixin/#cogroup","title":"cogroup <pre><code>cogroup(self, other)\n</code></pre> <p><code>cogroup</code> creates a PandasCogroupedOps.</p>","text":""},{"location":"sql/PandasMapOpsMixin/","title":"PandasMapOpsMixin","text":"<p><code>PandasMapOpsMixin</code> is a Python mixin for DataFrame class.</p>"},{"location":"sql/PythonUDF/","title":"PythonUDF","text":"<p><code>PythonUDF</code> is a Catalyst expression (Spark SQL).</p>"},{"location":"sql/PythonUDF/#creating-instance","title":"Creating Instance","text":"<p><code>PythonUDF</code> takes the following to be created:</p> <ul> <li> Name <li> PythonFunction <li> <code>DataType</code> (Spark SQL) <li> Children Catalyst Expressions (Spark SQL) <li> Python Eval Type <li> <code>udfDeterministic</code> flag <li> Result ID (<code>ExprId</code>) <p><code>PythonUDF</code> is created\u00a0when:</p> <ul> <li><code>UserDefinedPythonFunction</code> is requested to builder</li> </ul>"},{"location":"sql/PythonUDF/#unevaluable","title":"Unevaluable","text":"<p><code>PythonUDF</code> is an <code>Unevaluable</code> expression (Spark SQL).</p>"},{"location":"sql/PythonUDF/#nonsqlexpression","title":"NonSQLExpression","text":"<p><code>PythonUDF</code> is a <code>NonSQLExpression</code> expression (Spark SQL).</p>"},{"location":"sql/PythonUDF/#userdefinedexpression","title":"UserDefinedExpression","text":"<p><code>PythonUDF</code> is a <code>UserDefinedExpression</code> expression (Spark SQL).</p>"},{"location":"sql/RelationalGroupedDataset/","title":"RelationalGroupedDataset","text":"<p><code>RelationalGroupedDataset</code> is a result of executing high-level grouping operators.</p> <p>This is a stub</p> <p>This page is a stub to describe PySpark-related methods only. Learn more about RelationalGroupedDataset in The Internals of Spark SQL.</p>"},{"location":"sql/RelationalGroupedDataset/#flatmapcogroupsinpandas","title":"flatMapCoGroupsInPandas <pre><code>flatMapCoGroupsInPandas(\n  r: RelationalGroupedDataset,\n  expr: PythonUDF): DataFrame\n</code></pre> <p><code>flatMapCoGroupsInPandas</code>...FIXME</p> <p><code>flatMapCoGroupsInPandas</code>\u00a0is used when:</p> <ul> <li><code>PandasCogroupedOps</code> is requested to applyInPandas</li> </ul>","text":""},{"location":"sql/RelationalGroupedDataset/#flatmapgroupsinpandas","title":"flatMapGroupsInPandas <pre><code>flatMapGroupsInPandas(\n  expr: PythonUDF): DataFrame\n</code></pre> <p><code>flatMapGroupsInPandas</code> creates a <code>DataFrame</code> with a FlatMapGroupsInPandas logical operator.</p> <p><code>flatMapGroupsInPandas</code> asserts that the input PythonUDF is a grouped map udf (the evalType is <code>PythonEvalType.SQL_GROUPED_MAP_PANDAS_UDF</code>).</p> <p><code>flatMapGroupsInPandas</code> asserts that the return type of the input PythonUDF is <code>StructType</code>.</p> <p><code>flatMapGroupsInPandas</code>\u00a0is used when:</p> <ul> <li><code>PandasGroupedOpsMixin</code> is requested to applyInPandas</li> </ul>","text":""},{"location":"sql/SQLContext/","title":"SQLContext","text":"<p><code>SQLContext</code> is...FIXME</p>"},{"location":"sql/SparkConversionMixin/","title":"SparkConversionMixin","text":"<p><code>SparkConversionMixin</code> is a Python mixin for SparkSession class.</p>"},{"location":"sql/SparkSession/","title":"SparkSession","text":""},{"location":"sql/SparkSession/#_create_shell_session","title":"_create_shell_session <pre><code>_create_shell_session()\n</code></pre> <p><code>_create_shell_session</code> is a <code>@staticmethod</code>.</p> <p><code>_create_shell_session</code>...FIXME</p> <p><code>_create_shell_session</code>\u00a0is used when:</p> <ul> <li>FIXME</li> </ul>","text":""},{"location":"sql/SparkSession/#sql","title":"sql <pre><code>sql(\n  self, sqlQuery)\n</code></pre> <p><code>sql</code> creates a <code>DataFrame</code> with the <code>sqlQuery</code> query executed.</p>","text":""},{"location":"sql/UDFRegistration/","title":"UDFRegistration","text":"<p><code>UDFRegistration</code> is...FIXME</p>"},{"location":"sql/UserDefinedFunction/","title":"UserDefinedFunction","text":"<p><code>UserDefinedFunction</code> is...FIXME</p>"},{"location":"sql/UserDefinedPythonFunction/","title":"UserDefinedPythonFunction","text":""},{"location":"sql/UserDefinedPythonFunction/#creating-instance","title":"Creating Instance","text":"<p><code>UserDefinedPythonFunction</code> takes the following to be created:</p> <ul> <li> Name <li> <code>PythonFunction</code> <li> <code>DataType</code> (Spark SQL) <li> Python Eval Type <li> <code>udfDeterministic</code> flag"},{"location":"sql/UserDefinedPythonFunction/#creating-pythonudf","title":"Creating PythonUDF <pre><code>builder(\n  e: Seq[Expression]): Expression\n</code></pre> <p><code>builder</code> creates a PythonUDF for the arguments and the given children expressions.</p> <p><code>builder</code>\u00a0is used when:</p> <ul> <li><code>UDFRegistration</code> is requested to register a Python UDF (Spark SQL)</li> <li><code>UserDefinedPythonFunction</code> is requested to apply</li> </ul>","text":""},{"location":"sql/UserDefinedPythonFunction/#applying-pythonudf","title":"Applying PythonUDF <pre><code>apply(\n  exprs: Column*): Column\n</code></pre> <p><code>apply</code> creates a PythonUDF with the input <code>Column</code> (Spark SQL) expressions and creates a new <code>Column</code>.</p> <p><code>apply</code>\u00a0is used when:</p> <ul> <li><code>UDFRegistration</code> is requested to register a Python UDF (Spark SQL)</li> <li><code>UserDefinedPythonFunction</code> is requested to apply</li> </ul>","text":""}]}